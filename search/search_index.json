{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>TLDR:  A Game theoretical approach for calculating the contribution of each element of a system (here network models of the brain) to a system-wide description of the system. The classic neuroscience example: How much each brain region is causally relevant to an arbitrary cognitive function. </p>"},{"location":"index.html#motivation-such","title":"Motivation &amp; such:","text":"<p>MSA is developed by Keinan and colleagues back in 2004, the motivation for them was to have a causal picture of the system by lesioning its elements. The method itself is not new, if not the first, it was among one of the earliest ones used by neuroscientists to understand the brain. The reasoning is quite simple, let us study broken systems to see what's missing both from the brain and the behavior (or cognition) and assume that region was causally necessary for the emergence of that cognitive/behavioral state. What MSA does is to see this necessity as contribution. If the brain region is indeed the seed of this cognitive function (whatever this means) then its contribution should be very high while other regions will have near zero contribution. Having this in mind then we can see the whole scenario as a cooperative game in which a coalition of players work together and obtain some divisible outcome, then the question is quite the same. How to divide the outcome to the players in a \"fair\" way such that the most \"important\" player gets the biggest chunk. Shapley value is then that chunk! It is the result of a mathematically rigorous and axiomatic procedure that derives who should get how much from all possible combinations of coalitions and all ordering in which players can enter the game. Translating it to neuroscience, it derives a ranking of contributions from a dataset of all possible combinations of lesions. This means 2<sup>N</sup> lesions (assuming lesions are binary, either perturbed or not), which N is the number of brain regions. </p> <p>As you probably noticed this won't be feasible to calclulate as for example, it means a total number of 4,503,599,627,370,496 lesion combinations, assuming the brain is organized as Broadmann said, i.e., with 52 regions. So we estimate! For a more detailed description visit:</p> <p>Keinan, Alon, Claus C. Hilgetag, Isaac Meilijson, and Eytan Ruppin. 2004. \u201cCausal Localization of Neural Function: The Shapley Value Method.\u201d Neurocomputing 58-60 (June): 215\u201322. </p> <p>Keinan, Alon, Ben Sandbank, Claus C. Hilgetag, Isaac Meilijson, and Eytan Ruppin. 2006. \u201cAxiomatic Scalable Neurocontroller Analysis via the Shapley Value.\u201d Artificial Life 12 (3): 333\u201352.</p> <p>And our own recent work Fakhar K, Hilgetag CC. Systematic perturbation of an artificial neural network: A step towards quantifying causal contributions in the brain. PLoS Comput Biol. 2022;18: e1010250. doi:10.1371/journal.pcbi.1010250</p>"},{"location":"index.html#installation","title":"Installation:","text":"<p>The easiest way is to <code>pip install msapy</code>, This package is tested on Python <code>3.9</code> to Python <code>3.11</code>. Other versions might not work.</p>"},{"location":"index.html#how-it-works","title":"How it works:","text":"<p>Here you can see a schematic representation of how the algorithm works (interested in math instead? check the papers above). Briefly, all MSA needs from you is a list of players and a game function. The players can be your nodes, for example, brain regions or indices in a connectivity matrix, or links between them as tuples. It then shuffles them to produce N orderings in which they can join the game. This can end with repeating permutations if the set is small but that's fine don't worry! MSA then produces a \"combination space\" in which it produces all the combinations the player should form coalitions. Then it uses your game function and fills the contributions of those coalitions. The last step is to perform a Shapley integration and isolate each player's contribution in that given permutation. Repeating this for all permutations produces a contribution table (shapley table) and you'll get your shapley values by averaging over permutations so the end result is a value per element/player. To get a better grasp of how this works in code, check the minimal example in the examples folder.</p> <p> </p>"},{"location":"index.html#how-it-works-in-python","title":"How it works in Python:","text":"<p>I tried to make the package compact and easy-to-use but still there are a few things to keep in mind. Please take a look at the examples but just to give a flavor let's start working with the set ABCD as we have in the above picture.</p> <p>Importing will be just:  <pre><code>from msapy import msa, utils as ut\n</code></pre> Then we define some elements and generate the permutation space: <pre><code>nodes = ['A', 'B', 'C', 'D']\npermutation_space = msa.make_permutation_space(n_permutations=1000, elements=nodes)\n</code></pre> This results in a list of tuples, our permutation space that has 1000 permutations in it, here are the top 5 ones: <pre><code>[('D', 'C', 'A', 'B'),\n ('A', 'D', 'C', 'B'),\n ('D', 'A', 'B', 'C'),\n ('D', 'B', 'C', 'A'),\n ('A', 'D', 'C', 'B')]\n</code></pre> Then we use this to produce our combination space: <pre><code>combination_space = msa.make_combination_space(permutation_space=permutation_space)\n</code></pre> And a quick look of what's inside: <pre><code>[frozenset({'D'}),\n frozenset(),\n frozenset({'C', 'D'}),\n frozenset({'A', 'C', 'D'}),\n frozenset({'A', 'B', 'C', 'D'}),\n frozenset({'A'}),\n frozenset({'A', 'D'}),\n frozenset({'A', 'B', 'D'}),\n frozenset({'B', 'D'}),\n frozenset({'B', 'C', 'D'}),\n frozenset({'C'}),\n frozenset({'A', 'B'}),\n frozenset({'A', 'B', 'C'}),\n frozenset({'A', 'C'}),\n frozenset({'B', 'C'}),\n frozenset({'B'})]\n</code></pre> As you can see eventhough the permutation space has 1000 permutations, the combination space is exhausted because the total number of possible combinations is 2<sup>4</sup> or 16. Now here's the trick, we need to assign values to these combinations (coalitions) by keeping them intact while every other element is perturbed. In other words, the contribution of coalition <code>{'B', 'C'}</code> is isolated if we lesion <code>{'A', 'D'}</code> before playing the game. So what we do (and is not in the figure above) is to produce the \"complement space\" of the combination space: <pre><code>complement_space = msa.make_complement_space(combination_space=combination_space, elements=nodes)\n</code></pre> that is the difference of what's in the combination space in that coalition and what is not:</p> <p><pre><code>[('C', 'B', 'A'),\n ('C', 'D', 'B', 'A'),\n ('B', 'A'),\n ('B',),\n (),\n ('C', 'D', 'B'),\n ('C', 'B'),\n ('C',),\n ('C', 'A'),\n ('A',),\n ('D', 'B', 'A'),\n ('C', 'D'),\n ('D',),\n ('D', 'B'),\n ('D', 'A'),\n ('C', 'D', 'A')]\n</code></pre> As you can see, for example when combination is <code>{'D'}</code> the corresponding complement is <code>('C', 'B', 'A')</code>. Note the difference in types, combination space is an <code>OrderedSet</code> of <code>frozenset</code>s so the Shapley value calculations are quicker while complement space is an <code>OrderedSet</code> of <code>Tuples</code> So handling it in your objective function is easier. Speaking of, let's make the worst objective function that just produces random values regardless of what's what (see the example <code>on ground-truth models.ipynb</code> for a more elaborate version.)(see the example <code>on ground-truth models.ipynb</code> for a more elaborate version.) <pre><code>def rnd(complements):\n    return np.random.randint(1, 10)\n</code></pre> We'll next play the games and aquire the contributions as follows: <pre><code>contributions, lesion_effects = msa.take_contributions(elements=nodes,\n                                        combination_space=combination_space,\n                                        complement_space=complement_space,\n                                        objective_function=rnd)\n</code></pre> Both <code>contributions</code> and <code>lesion_effects</code> are the same values just addressed differently. For example, if the contribution of coalition <code>{'B', 'C'}</code> is 5 points then you can also say the effect of lesioning coalition <code>{'A', 'D'}</code> is 5 points. This by itself is not that informative but if you know the contribution of the grand coalition (intact system) then you can claim that the effect of lesioning <code>{'A', 'D'}</code> is a drop of some performance from x to 5.</p> <p>Lastly, you can calculate Shapley values like:</p> <p><pre><code>import msa\n\nshapley_table = msa.get_shapley_table(contributions=contributions, permutation_space=permutation_space)\n</code></pre> Which gives you a <code>ShapleyTable</code> data structure which is a wrapper around <code>pd.DataFrame</code> to work with.</p>"},{"location":"index.html#the-interface","title":"The Interface:","text":"<p>To make things easier, msa comes with an interface function:</p> <p><pre><code>shapley_table, contributions, lesion_effects = msa.interface(multiprocessing_method='joblib',\n                                                             elements=regions,\n                                                             n_permutations=1000,\n                                                             objective_function=rnd,\n                                                             n_parallel_games=-1,\n                                                             random_seed=1)\n</code></pre> For this one, all you have to do is to provide your elements, the objective function, and specify some parameters. For example, you can choose between two different multiprocessing toolboxes <code>joblib</code> and <code>ray</code> to distribute <code>msa.take_contributions</code> over <code>n_parallel_games</code>. Specifying a <code>random_seed</code> is encouraged for reproducibility but the default is <code>None</code>.</p>"},{"location":"index.html#todo-interested-in-contributing","title":"TODO (Interested in Contributing?):","text":"<ul> <li>More estimation methods, for example see: amiratag/neuronshapley.</li> <li>GPU and HPC compatibilty</li> <li>Providing built-in objective functions for common use-cases.</li> <li>Improved documentation</li> <li>More Tests</li> </ul>"},{"location":"index.html#cite","title":"Cite:","text":"<pre><code>@misc{MSA,\n  author = {Kayson Fakhar and Shrey Dixit},\n  title = {MSA: A compact Python package for Multiperturbation Shapley value Analysis.},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/kuffmode/msa}},\n}\n</code></pre>"},{"location":"index.html#acknowledgement","title":"Acknowledgement:","text":"<p>I thank my good friend and Python mentor Fabrizio Damicelli  whom I learned a lot from. Without him this package would be a disaster to look at.</p>"},{"location":"Getting%20Started.html","title":"Getting Started","text":"<p>To start the code examples, we load the library</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\nfrom msapy import msa\n</pre> import numpy as np  from msapy import msa <p>Lets assume for this example that we have a system of four elements: \"A\", \"B\", \"C\", \"D\". The system produces some activity that is equal to the sum of contributions of the individual elements. For the sake of this example, we take the contribution of each element to be 50, 100, 200 and -30 with some added noise. We can write:</p> <ul> <li>$ A \\sim \\mathcal{N}(50,\\,1)\\, $</li> <li>$ B \\sim \\mathcal{N}(100,\\,1)\\, $</li> <li>$ C \\sim \\mathcal{N}(200,\\,1)\\, $</li> <li>$ D \\sim \\mathcal{N}(-30,\\,1)\\, $</li> </ul> In\u00a0[2]: Copied! <pre>nodes = ['A', 'B', 'C', 'D']\n</pre> nodes = ['A', 'B', 'C', 'D'] <p>Now you need an objective function. An objective function returns the value that you want to know how it's affected by different elements of your system. For this example, it's the total activity of our system. The objective function should take an argument called <code>complements</code> which specifies the elements that are leasioned or removed.</p> In\u00a0[3]: Copied! <pre>def objective_function(complements):\n    contributions = {\"A\": 50, \"B\": 100, \"C\": 200, \"D\": -30}\n    activity = 0.0\n    for k, v in contributions.items():\n        if k not in complements:\n            activity += np.random.normal(v, 1)\n    return activity\n</pre> def objective_function(complements):     contributions = {\"A\": 50, \"B\": 100, \"C\": 200, \"D\": -30}     activity = 0.0     for k, v in contributions.items():         if k not in complements:             activity += np.random.normal(v, 1)     return activity <p>Now that we have all the things to run MSA, we can just call a single function</p> In\u00a0[4]: Copied! <pre>shapley_table = msa.interface(multiprocessing_method='joblib',\n                              elements=nodes,\n                              n_permutations=1000,\n                              objective_function=objective_function,\n                              n_parallel_games=-1,\n                              random_seed=1)\n</pre> shapley_table = msa.interface(multiprocessing_method='joblib',                               elements=nodes,                               n_permutations=1000,                               objective_function=objective_function,                               n_parallel_games=-1,                               random_seed=1) <p>The <code>shapley_table</code> returned from <code>msa.interface</code> is a Pandas DataFrame with columns as the elements and the rows as permutations. We can take the mean of the rows to calculate the shapley values i.e. the contribution of each element.</p> In\u00a0[5]: Copied! <pre>shapley_table.head()\n</pre> shapley_table.head() Out[5]: A B C D 0 50.839483 100.641381 200.096570 -29.600767 1 51.688566 99.761323 200.650561 -30.748772 2 53.423007 99.635949 198.951589 -34.134876 3 49.359333 101.446440 198.679690 -27.510115 4 51.727647 100.589809 201.427518 -28.892053 In\u00a0[6]: Copied! <pre>shapley_table.shapley_values\n</pre> shapley_table.shapley_values Out[6]: <pre>A     50.451432\nB     99.917185\nC    199.603503\nD    -30.127528\ndtype: float64</pre> <p>The contributions from the MSA are very close to the real contributions.</p> <p>This was just a very simple example for how to use MSA.</p>"},{"location":"Getting%20Started.html#getting-started","title":"Getting Started\u00b6","text":"<p>MSA (Multi-perturbation Shapley value Analysis) is a Game theoretical approach for calculating the contribution of each element of a system (here network models of the brain) to a system-wide description of the system. The classic neuroscience example: How much each brain region is causally relevant to an arbitrary cognitive function.</p> <p>The following examples show some of msapy's capabilities and give you an idea of the API.</p> <p>For more details on the API, see API Docs.</p> <p>For specific purposes, refer to the following examples:</p> <ol> <li>PyTorch and MSA</li> <li>Time Series MSA</li> <li>MSA 2D</li> </ol>"},{"location":"API/estimate_causal_influences.html","title":"Estimate causal influences","text":"<p>Estimates the causal contribution (Shapley values) of each node on the rest of the network. Basically, this function performs MSA iteratively on each node and tracks the changes in the objective_function of the target node. For example we have a chain A -&gt; B -&gt; C, and we want to know how much A and B are contributing to C. We first need to define a metric for C (objective_function) which here let's say is the average activity of C. MSA then performs a multi-site lesioning analysis of A and B so for each we will end up with a number indicating their contributions to the average activity of C.</p> <p>VERY IMPORTANT NOTES:</p> <pre><code>1. The resulting causal contribution matrix does not necessarily reflect the connectome. In the example above\nthere's no actual connection A -&gt; C but there might be one in the causal contribution matrix since A is causally\ninfluencing C via B.\n2. Think twice (even three times) about your objective function. The same everything will result in different\ncausal contribution matrices depending on what are you tracking and how accurate it's capturing the effect of\nlesions. Also don't forget the edge-cases. There will be weird behaviors in your system, for example, what it\ndoes if every node is perturbed?\n3. The metric you track is preferred to be non-negative and bounded (at least practically!)\n4. Obviously this will take N times longer than a normal MSA with N is the number of nodes. So make sure your\nprocess is as fast as it can be for example use Numba and stuff, but you don't need to implement any parallel\nprocesses since it's already implemented here. Going below 1000 permutations might be an option depending on\nyour specific case but based on experience, it's not a good idea \n5. Shapley values sum up (or will be close) to the value of the intact coalition. So for example if the\nmean activity of node C here is 50 then causal_contribution_matrix.sum(axis=0) = 50 or close to 50. If not it\nmeans:\n    1. the number of permutations are not enough\n    2. there is randomness somewhere in the process\n    3. your objective function is not suitable\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>list</code> <p>List of the players (elements). Can be strings (names), integers (indicies), and tuples.</p> required <code>objective_function</code> <code>Callable</code> <p>The game (in-silico experiment). It should get the complement set and return one numeric value either int or float. This function is just calling it as: objective_function(complement, **objective_function_params)</p> <p>An example using networkx with some tips:</p> <p>def lesion_me_senpai(complements, network, index):     # note \"index\", your function should be able to track the effects on the target and the keyword for       that is \"index\"</p> <pre><code>if len(complements) == len(A)-1:  # -1 since the target node is active\n    return 0\n\nlesioned_network = deepcopy(network)\nfor target in complements:\n    lesioned_network[target] = 0  # setting all connections of the targets to 0\n\nactivity = network.run(lesioned_network) # or really, whatever you want!\nreturn float(activity[index].mean())\n</code></pre> <p>(you sometimes need to specify what should happen during edge-cases like an all-lesioned network)</p> required <code>objective_function_params</code> <code>Optional[Dict]</code> <p>Kwargs for the objective_function. A dictionary pair of {'index': index} will be added to this during the process so your function can track the lesion effect.</p> <code>None</code> <code>target_elements</code> <code>Optional[list]</code> <p>list of elements that you want to calculate the causal influence of.</p> <code>None</code> <code>multiprocessing_method</code> <code>str = 'joblib'</code> <p>So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib. If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their documentations to see how to go for it.</p> <code>'joblib'</code> <code>n_cores</code> <code>int = -1</code> <p>Number of parallel games. Default is -1, which means all cores so it can make the system freeze for a short period, if that happened then maybe go for -2, which means one msapy is left out. Or really just specify the number of threads you want to use!</p> <code>-1</code> <code>n_permutations</code> <code>int = 1000</code> <p>Number of permutations per node. Didn't check it systematically yet but just based on random explorations I'd say something around 1000 is enough.</p> <code>1000</code> <code>permutation_seed</code> <code>Optional[int] = None</code> <p>Sets the random seed of the sampling process. Default is None so if nothing is given every call results in a different orderings.</p> <code>None</code> <code>parallelize_over_games</code> <code>bool = False</code> <p>Whether to parallelize over games or parallelize over elements. Parallelizing over the elements is generally faster. Defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>causal_influences (pd.DataFrame)</p> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef estimate_causal_influences(elements: list,\n                               objective_function: Callable,\n                               objective_function_params: Optional[dict] = None,\n                               target_elements: Optional[list] = None,\n                               multiprocessing_method: str = 'joblib',\n                               n_cores: int = -1,\n                               n_permutations: int = 1000,\n                               permutation_seed: Optional[int] = None,\n                               parallelize_over_games=False,\n                               lazy=True\n                               ) -&gt; pd.DataFrame:\n    \"\"\"\n    Estimates the causal contribution (Shapley values) of each node on the rest of the network. Basically, this function\n    performs MSA iteratively on each node and tracks the changes in the objective_function of the target node.\n    For example we have a chain A -&gt; B -&gt; C, and we want to know how much A and B are contributing to C. We first need to\n    define a metric for C (objective_function) which here let's say is the average activity of C. MSA then performs a\n    multi-site lesioning analysis of A and B so for each we will end up with a number indicating their contributions to\n    the average activity of C.\n\n    VERY IMPORTANT NOTES:\n\n        1. The resulting causal contribution matrix does not necessarily reflect the connectome. In the example above\n        there's no actual connection A -&gt; C but there might be one in the causal contribution matrix since A is causally\n        influencing C via B.\n        2. Think twice (even three times) about your objective function. The same everything will result in different\n        causal contribution matrices depending on what are you tracking and how accurate it's capturing the effect of\n        lesions. Also don't forget the edge-cases. There will be weird behaviors in your system, for example, what it\n        does if every node is perturbed?\n        3. The metric you track is preferred to be non-negative and bounded (at least practically!)\n        4. Obviously this will take N times longer than a normal MSA with N is the number of nodes. So make sure your\n        process is as fast as it can be for example use Numba and stuff, but you don't need to implement any parallel\n        processes since it's already implemented here. Going below 1000 permutations might be an option depending on\n        your specific case but based on experience, it's not a good idea \n        5. Shapley values sum up (or will be close) to the value of the intact coalition. So for example if the\n        mean activity of node C here is 50 then causal_contribution_matrix.sum(axis=0) = 50 or close to 50. If not it\n        means:\n            1. the number of permutations are not enough\n            2. there is randomness somewhere in the process\n            3. your objective function is not suitable\n\n\n    Args:\n        elements (list):\n            List of the players (elements). Can be strings (names), integers (indicies), and tuples.\n\n        objective_function (Callable):\n            The game (in-silico experiment). It should get the complement set and return one numeric value\n            either int or float.\n            This function is just calling it as: objective_function(complement, **objective_function_params)\n\n            An example using networkx with some tips:\n\n            def lesion_me_senpai(complements, network, index):\n                # note \"index\", your function should be able to track the effects on the target and the keyword for\n                  that is \"index\"\n\n                if len(complements) == len(A)-1:  # -1 since the target node is active\n                    return 0\n\n                lesioned_network = deepcopy(network)\n                for target in complements:\n                    lesioned_network[target] = 0  # setting all connections of the targets to 0\n\n                activity = network.run(lesioned_network) # or really, whatever you want!\n                return float(activity[index].mean())\n\n            (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)\n\n\n        objective_function_params (Optional[Dict]):\n            Kwargs for the objective_function. A dictionary pair of {'index': index} will be added to this during\n            the process so your function can track the lesion effect.\n\n        target_elements (Optional[list]): list of elements that you want to calculate the causal influence of.\n\n        multiprocessing_method (str = 'joblib'):\n            So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib.\n            If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their\n            documentations to see how to go for it.\n\n        n_cores (int = -1):\n            Number of parallel games. Default is -1, which means all cores so it can make the system\n            freeze for a short period, if that happened then maybe go for -2, which means one msapy is\n            left out. Or really just specify the number of threads you want to use!\n\n        n_permutations (int = 1000):\n            Number of permutations per node.\n            Didn't check it systematically yet but just based on random explorations\n            I'd say something around 1000 is enough.\n\n        permutation_seed (Optional[int] = None):\n            Sets the random seed of the sampling process. Default is None so if nothing is given every call results in\n            a different orderings.\n\n        parallelize_over_games (bool = False): Whether to parallelize over games or parallelize over elements. Parallelizing\n            over the elements is generally faster. Defaults to False\n\n    Returns:\n        causal_influences (pd.DataFrame)\n\n    \"\"\"\n    target_elements = target_elements if target_elements else elements\n    objective_function_params = objective_function_params if objective_function_params else {}\n\n    if parallelize_over_games:\n        # run causal_influence_single_element for all target elements.\n        mbar = master_bar(enumerate(target_elements),\n                          total=len(target_elements))\n        results = [causal_influence_single_element(elements, objective_function,\n                                                   objective_function_params, n_permutations,\n                                                   n_cores, multiprocessing_method,\n                                                   permutation_seed, index, element, lazy, mbar) for index, element in mbar]\n\n    elif multiprocessing_method == 'ray':\n        if importlib.util.find_spec(\"ray\") is None:\n            raise ImportError(\n                \"The ray package is required to run this algorithm. Install and use at your own risk.\")\n\n        import ray\n        if n_cores &lt;= 0:\n            warnings.warn(\"A zero or a negative n_cores was passed and ray doesn't like so \"\n                          \"to fix that ray.init() will get no arguments, \"\n                          \"which means use all cores as n_cores = -1 does for joblib.\", stacklevel=2)\n            ray.init()\n        else:\n            ray.init(num_cpus=n_cores)\n\n        result_ids = [ray.remote(causal_influence_single_element).remote(elements, objective_function,\n                                                                         objective_function_params, n_permutations,\n                                                                         1, 'joblib',\n                                                                         permutation_seed, index, element, lazy, None) for index, element in enumerate(target_elements)]\n\n        for _ in tqdm(ut.ray_iterator(result_ids), total=len(result_ids)):\n            pass\n\n        results = ray.get(result_ids)\n        ray.shutdown()\n\n    else:\n        with tqdm_joblib(desc=\"Doing Nodes: \", total=len(target_elements)) as pb:\n            results = (Parallel(n_jobs=n_cores)(delayed(causal_influence_single_element)(elements, objective_function,\n                                                                                         objective_function_params, n_permutations,\n                                                                                         1, 'joblib',\n                                                                                         permutation_seed, index, element, lazy) for index, element in enumerate(target_elements)))\n\n    _, contribution_type = results[0]\n    shapley_values = [r[0] for r in results]\n\n    causal_influences = pd.DataFrame(\n        shapley_values, columns=elements) if contribution_type == \"scaler\" else pd.concat(shapley_values, keys=elements)\n\n    if contribution_type == \"scaler\":\n        return causal_influences\n    return causal_influences[causal_influences.index.levels[0]]\n</code></pre>"},{"location":"API/get_shapley_table.html","title":"Get shapley table","text":"<p>Calculates Shapley values based on the filled contribution_space. Briefly, for a permutation (A,B,C) it will be:</p> <p>(A,B,C) - (B,C) = Contribution of A to the coalition (B,C). (B,C) - (C) = Contribution of B to the coalition formed with (C). (C) = Contribution of C alone.</p> <p>This will repeat over all permutations. and the result is a distribution of Shapley values for each element, note that the estimation method we're using here is an \"unbiased estimator\" so the variance is fairly large.</p> <p>Parameters:</p> Name Type Description Default <code>contributions</code> <code>Dict</code> <p>Filled Dictionary of coalition:result</p> <code>None</code> <code>permutation_space</code> <code>list</code> <p>Should be the same passed to make_combination_space.</p> required <code>lesioned</code> <code>Optional[any]</code> <p>leseioned element that will not be present in any combination</p> <code>None</code> <code>objective_function</code> <code>Callable</code> <p>The game (in-silico experiment). It should get the complement set and return one numeric value either int or float. This function is just calling it as: objective_function(complement, **objective_function_params)</p> <p>An example using networkx with some tips: (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)</p> <p>def local_efficiency(complements, graph):     if len(complements) &lt; 0:         # the network is intact so:         return nx.local_efficiency(graph)</p> <pre><code>elif len(complements) == len(graph):\n    # the network is fully lesioned so:\n    return 0.0\n\nelse:\n    # lesion the system, calculate things\n    lesioned = graph.copy()\n    lesioned.remove_nodes_from(complements)\n    return nx.local_efficiency(lesioned)\n</code></pre> <code>None</code> <code>objective_function_params</code> <code>Dict</code> <p>Kwargs for the objective_function.</p> <code>None</code> <code>lesioned</code> <code>Optional[any]</code> <p>leseioned element that will not be present in any combination</p> <code>None</code> <code>multiprocessing_method</code> <code>str</code> <p>So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib. If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!) and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around, I realized joblib is faster for tasks that are small themselves. Remedies are here: https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html</p> <p>Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:</p> <pre><code>- no parallel: 1020 sec\n- joblib: 63 sec\n- ray: 65 sec\n</code></pre> <p>That makes sense since I have 16 cores and 1000/16 is around 62.</p> required <code>rng</code> <code>Optional[Generator]</code> <p>Numpy random generator object used for reproducable results. Default is None.</p> required <code>random_seed</code> <code>Optional[int]</code> <p>sets the random seed of the sampling process. Only used when <code>rng</code> is None. Default is None.</p> required <code>n_parallel_games</code> <code>int</code> <p>Number of parallel jobs (number of to-be-occupied cores), -1 means all CPU cores and 1 means a serial process. I suggest using 1 for debugging since things get messy in parallel!</p> required <code>lazy</code> <code>bool</code> <p>if set to True, objective function will be called lazily instead of calling it all at once and storing the outputs in a dict. Setting it to True saves a lot of memory and might even be faster in certain cases.</p> <code>False</code> <code>save_permutations</code> <code>bool</code> <p>If set to True, the shapley values are calculated by calculating the running mean of the permutations instead of storing the permutations. This parameter is ignored in case the objective function returns a scaler.</p> <code>False</code> <code>dual_progress_bar</code> <code>bool</code> <p>If set to true, you will have two progress bars. One parent that will track the permutations, other child that will track the elements. Its ignored in case the mbar is provided</p> required <code>mbar</code> <code>MasterBar</code> <p>A Fastprogress MasterBar. Use it in case you're calling the interface multiple times to have a nester progress bar.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Shapley table or a dict of Shapely tables, columns will be </p> <code>DataFrame</code> <p>elements and indices will be samples (permutations). </p> <code>DataFrame</code> <p>It will be a Multi-Index DataFrame if the contributions are a timeseries.</p> <code>DataFrame</code> <p>The index at <code>level=1</code> will be the timestamps</p> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef get_shapley_table(*,\n                      permutation_space: list,\n                      contributions: Optional[Dict] = None,\n                      lesioned: Optional[any] = None,\n                      objective_function: Optional[Callable] = None,\n                      objective_function_params: Optional[Dict] = None,\n                      lazy=False,\n                      save_permutations: bool = False,\n                      dual_progress_bars: bool = True,\n                      mbar: Optional[MasterBar] = None,) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculates Shapley values based on the filled contribution_space.\n    Briefly, for a permutation (A,B,C) it will be:\n\n    (A,B,C) - (B,C) = Contribution of A to the coalition (B,C).\n    (B,C) - (C) = Contribution of B to the coalition formed with (C).\n    (C) = Contribution of C alone.\n\n    This will repeat over all permutations. and the result is a distribution of Shapley values for each element,\n    note that the estimation method we're using here is an \"unbiased estimator\" so the variance is fairly large.\n\n    Args:\n        contributions (Dict):\n            Filled Dictionary of coalition:result\n\n        permutation_space (list):\n            Should be the same passed to make_combination_space.\n\n        lesioned (Optional[any]):\n            leseioned element that will not be present in any combination\n\n        objective_function (Callable):\n            The game (in-silico experiment). It should get the complement set and return one numeric value\n            either int or float.\n            This function is just calling it as: objective_function(complement, **objective_function_params)\n\n            An example using networkx with some tips:\n            (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)\n\n            def local_efficiency(complements, graph):\n                if len(complements) &lt; 0:\n                    # the network is intact so:\n                    return nx.local_efficiency(graph)\n\n                elif len(complements) == len(graph):\n                    # the network is fully lesioned so:\n                    return 0.0\n\n                else:\n                    # lesion the system, calculate things\n                    lesioned = graph.copy()\n                    lesioned.remove_nodes_from(complements)\n                    return nx.local_efficiency(lesioned)\n\n        objective_function_params (Dict):\n            Kwargs for the objective_function.\n\n        lesioned (Optional[any]):\n            leseioned element that will not be present in any combination\n\n        multiprocessing_method (str):\n            So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib.\n            If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their\n            documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!)\n            and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around,\n            I realized joblib is faster for tasks that are small themselves. Remedies are here:\n            https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html\n\n            Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself\n            takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a\n            second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:\n\n                - no parallel: 1020 sec\n                - joblib: 63 sec\n                - ray: 65 sec\n\n            That makes sense since I have 16 cores and 1000/16 is around 62.\n\n        rng (Optional[np.random.Generator]): Numpy random generator object used for reproducable results. Default is None.\n\n        random_seed (Optional[int]):\n            sets the random seed of the sampling process. Only used when `rng` is None. Default is None.\n\n        n_parallel_games (int):\n            Number of parallel jobs (number of to-be-occupied cores),\n            -1 means all CPU cores and 1 means a serial process.\n            I suggest using 1 for debugging since things get messy in parallel!\n\n        lazy (bool): if set to True, objective function will be called lazily instead of calling it all at once and storing the outputs in a dict.\n            Setting it to True saves a lot of memory and might even be faster in certain cases.\n\n        save_permutations (bool): If set to True, the shapley values are calculated by calculating the running mean of the permutations instead of\n            storing the permutations. This parameter is ignored in case the objective function returns a scaler.\n\n        dual_progress_bar (bool): If set to true, you will have two progress bars. One parent that will track the permutations, other child that\n            will track the elements. Its ignored in case the mbar is provided\n\n        mbar (MasterBar): A Fastprogress MasterBar. Use it in case you're calling the interface multiple times to have a nester progress bar.\n\n    Returns:\n        pd.DataFrame: Shapley table or a dict of Shapely tables, columns will be \n        elements and indices will be samples (permutations). \n        It will be a Multi-Index DataFrame if the contributions are a timeseries.\n        The index at `level=1` will be the timestamps\n    \"\"\"\n    _check_get_shapley_table_args(contributions, objective_function, lazy)\n    _check_valid_permutation_space(permutation_space)\n\n    lesioned = {lesioned} if lesioned else set()\n    contributions = {tuple(lesioned): objective_function(tuple(lesioned), **objective_function_params)} if lazy else contributions\n\n    contribution_type, intact_contributions_in_case_lazy = _get_contribution_type(contributions)\n    contrib_shape = intact_contributions_in_case_lazy.shape if contribution_type == \"nd\" else []\n\n    sorted_elements = sorted(permutation_space[0])\n    permutation_space = set(permutation_space)\n\n    if not lazy:\n        parent_bar = enumerate(permutation_space)\n    elif (not dual_progress_bars) or mbar:\n        parent_bar = progress_bar(enumerate(permutation_space), total=len(\n            permutation_space), leave=False, parent=mbar)\n    elif lazy:\n        parent_bar = master_bar(\n            enumerate(permutation_space), total=len(permutation_space))\n\n    shapley_table = 0 if (contribution_type == 'nd' and not save_permutations) else np.zeros((len(permutation_space), len(sorted_elements), *contrib_shape), dtype=float)\n\n    for i, permutation in parent_bar:\n        isolated_contributions = np.zeros((len(permutation), *intact_contributions_in_case_lazy.shape), dtype=float) if contribution_type==\"nd\" else ([None] * len(permutation))  # got to be a better way!\n        child_bar = enumerate(permutation) if not (dual_progress_bars and lazy) else progress_bar(\n            enumerate(permutation), total=len(permutation), leave=False, parent=parent_bar)\n        # iterate over all elements in the permutation to calculate their isolated contributions\n\n        contributions_including = intact_contributions_in_case_lazy\n        for index, element in child_bar:\n            including = frozenset(permutation[:index + 1])\n            excluding = frozenset(permutation[:index])\n\n            # the isolated contribution of an element is the difference of contribution with that element and without that element\n            if lazy:\n                contributions_excluding = objective_function(tuple(including.union(lesioned)), **objective_function_params)\n                isolated_contributions[sorted_elements.index(element)] = contributions_including - contributions_excluding\n                contributions_including = contributions_excluding\n            else:\n                isolated_contributions[sorted_elements.index(element)] =  contributions[including - lesioned] - contributions[excluding - lesioned]\n\n        if contribution_type == 'nd' and not save_permutations:\n            shapley_table += (isolated_contributions - shapley_table) / (i + 1)\n        else:\n            shapley_table[i] = np.array(isolated_contributions)\n\n    # post processing of shapley values based on what type of contribution it is. The format of output will vary based on if the\n    # values are multi-scores, timeseries, etc.\n    if contribution_type == 'nd' and not save_permutations:\n        shapley_table = shapley_table.reshape(shapley_table.shape[0], -1).T\n        shapley_table = pd.DataFrame(\n            shapley_table, columns=sorted_elements)\n        return ShapleyModeND(shapley_table, intact_contributions_in_case_lazy.shape)\n\n    if contribution_type == \"scaler\":\n        return ShapleyTable(pd.DataFrame(shapley_table, columns=sorted_elements))\n\n    return ShapleyTableND.from_ndarray(shapley_table, columns=sorted_elements)\n</code></pre>"},{"location":"API/interface.html","title":"Interface","text":"<p>A wrapper function to call other related functions internally and produces an easy-to-use pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>n_permutations</code> <code>int</code> <p>Number of permutations (samples) per element.</p> required <code>elements</code> <code>list</code> <p>List of the players (elements). Can be strings (names), integers (indicies), and tuples.</p> required <code>objective_function</code> <code>Callable</code> <p>The game (in-silico experiment). It should get the complement set and return one numeric value either int or float. This function is just calling it as: objective_function(complement, **objective_function_params)</p> <p>An example using networkx with some tips: (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)</p> <p>def local_efficiency(complements, graph):     if len(complements) &lt; 0:         # the network is intact so:         return nx.local_efficiency(graph)</p> <pre><code>elif len(complements) == len(graph):\n    # the network is fully lesioned so:\n    return 0.0\n\nelse:\n    # lesion the system, calculate things\n    lesioned = graph.copy()\n    lesioned.remove_nodes_from(complements)\n    return nx.local_efficiency(lesioned)\n</code></pre> required <code>objective_function_params</code> <code>Dict</code> <p>Kwargs for the objective_function.</p> <code>{}</code> <code>permutation_space</code> <code>Optional[list]</code> <p>Already generated permutation space, in case you want to be more reproducible or something and use the same lesion combinations for many metrics.</p> <code>None</code> <code>pair</code> <code>Optional[Tuple]</code> <p>pair of elements that will always be together in every combination</p> <code>None</code> <code>lesioned</code> <code>Optional[any]</code> <p>leseioned element that will not be present in any combination</p> <code>None</code> <code>multiprocessing_method</code> <code>str</code> <p>So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib. If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!) and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around, I realized joblib is faster for tasks that are small themselves. Remedies are here: https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html</p> <p>Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:</p> <pre><code>- no parallel: 1020 sec\n- joblib: 63 sec\n- ray: 65 sec\n</code></pre> <p>That makes sense since I have 16 cores and 1000/16 is around 62.</p> <code>'joblib'</code> <code>rng</code> <code>Optional[Generator]</code> <p>Numpy random generator object used for reproducable results. Default is None.</p> <code>None</code> <code>random_seed</code> <code>Optional[int]</code> <p>sets the random seed of the sampling process. Only used when <code>rng</code> is None. Default is None.</p> <code>None</code> <code>n_parallel_games</code> <code>int</code> <p>Number of parallel jobs (number of to-be-occupied cores), -1 means all CPU cores and 1 means a serial process. I suggest using 1 for debugging since things get messy in parallel!</p> <code>-1</code> <code>lazy</code> <code>bool</code> <p>if set to True, objective function will be called lazily instead of calling it all at once and storing the outputs in a dict. Setting it to True saves a lot of memory and might even be faster in certain cases.</p> <code>True</code> <code>save_permutations</code> <code>bool</code> <p>If set to True, the shapley values are calculated by calculating the running mean of the permutations instead of storing the permutations. This parameter is ignored in case the objective function returns a scaler.</p> <code>False</code> <code>dual_progress_bar</code> <code>bool</code> <p>If set to true, you will have two progress bars. One parent that will track the permutations, other child that will track the elements. Its ignored in case the mbar is provided</p> required <code>mbar</code> <code>MasterBar</code> <p>A Fastprogress MasterBar. Use it in case you're calling the interface multiple times to have a nester progress bar.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Tuple[pd.DataFrame, Dict, Dict]: shapley_table, contributions, lesion_effects</p> <p>Note that contributions and lesion_effects are the same values, addressed differently. For example: If from a set of ABCD removing AC ends with some value x, you can say the contribution of BD=x and the effect of removing AC=x. So the same values are addressed differently in the two returned Dicts. Of course, it makes more sense to compare the lesion effects with the intact system but who am I to judge.</p> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef interface(*,\n              n_permutations: int,\n              elements: list,\n              objective_function: Callable,\n              objective_function_params: Dict = {},\n              permutation_space: Optional[list] = None,\n              pair: Optional[Tuple] = None,\n              lesioned: Optional[any] = None,\n              multiprocessing_method: str = 'joblib',\n              rng: Optional[np.random.Generator] = None,\n              random_seed: Optional[int] = None,\n              n_parallel_games: int = -1,\n              lazy: bool = True,\n              save_permutations: bool = False,\n              dual_progress_bars: bool = True,\n              mbar: Optional[MasterBar] = None\n              ) -&gt; pd.DataFrame:\n    \"\"\"\n    A wrapper function to call other related functions internally and produces an easy-to-use pipeline.\n\n    Args:\n        n_permutations (int):\n            Number of permutations (samples) per element.\n\n        elements (list):\n            List of the players (elements). Can be strings (names), integers (indicies), and tuples.\n\n        objective_function (Callable):\n            The game (in-silico experiment). It should get the complement set and return one numeric value\n            either int or float.\n            This function is just calling it as: objective_function(complement, **objective_function_params)\n\n            An example using networkx with some tips:\n            (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)\n\n            def local_efficiency(complements, graph):\n                if len(complements) &lt; 0:\n                    # the network is intact so:\n                    return nx.local_efficiency(graph)\n\n                elif len(complements) == len(graph):\n                    # the network is fully lesioned so:\n                    return 0.0\n\n                else:\n                    # lesion the system, calculate things\n                    lesioned = graph.copy()\n                    lesioned.remove_nodes_from(complements)\n                    return nx.local_efficiency(lesioned)\n\n        objective_function_params (Dict):\n            Kwargs for the objective_function.\n\n        permutation_space (Optional[list]):\n            Already generated permutation space, in case you want to be more reproducible or something and use the same\n            lesion combinations for many metrics.\n\n        pair (Optional[Tuple]):\n            pair of elements that will always be together in every combination\n\n        lesioned (Optional[any]):\n            leseioned element that will not be present in any combination\n\n        multiprocessing_method (str):\n            So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib.\n            If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their\n            documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!)\n            and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around,\n            I realized joblib is faster for tasks that are small themselves. Remedies are here:\n            https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html\n\n            Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself\n            takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a\n            second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:\n\n                - no parallel: 1020 sec\n                - joblib: 63 sec\n                - ray: 65 sec\n\n            That makes sense since I have 16 cores and 1000/16 is around 62.\n\n        rng (Optional[np.random.Generator]): Numpy random generator object used for reproducable results. Default is None.\n\n        random_seed (Optional[int]):\n            sets the random seed of the sampling process. Only used when `rng` is None. Default is None.\n\n        n_parallel_games (int):\n            Number of parallel jobs (number of to-be-occupied cores),\n            -1 means all CPU cores and 1 means a serial process.\n            I suggest using 1 for debugging since things get messy in parallel!\n\n        lazy (bool): if set to True, objective function will be called lazily instead of calling it all at once and storing the outputs in a dict.\n            Setting it to True saves a lot of memory and might even be faster in certain cases.\n\n        save_permutations (bool): If set to True, the shapley values are calculated by calculating the running mean of the permutations instead of\n            storing the permutations. This parameter is ignored in case the objective function returns a scaler.\n\n        dual_progress_bar (bool): If set to true, you will have two progress bars. One parent that will track the permutations, other child that\n            will track the elements. Its ignored in case the mbar is provided\n\n        mbar (MasterBar): A Fastprogress MasterBar. Use it in case you're calling the interface multiple times to have a nester progress bar.\n\n\n    Returns:\n        Tuple[pd.DataFrame, Dict, Dict]: shapley_table, contributions, lesion_effects\n\n    Note that contributions and lesion_effects are the same values, addressed differently. For example:\n    If from a set of ABCD removing AC ends with some value x, you can say the contribution of BD=x and the\n    effect of removing AC=x. So the same values are addressed differently in the two returned Dicts.\n    Of course, it makes more sense to compare the lesion effects with the intact system but who am I to judge.\n    \"\"\"\n\n    # create a numpy random number generator if one is not passed\n    if not rng:\n        rng = np.random.default_rng(\n            random_seed) if random_seed else np.random.default_rng()\n\n    # create a permutation_space if one is not passed\n    if not permutation_space:\n        permutation_space = make_permutation_space(elements=elements,\n                                                   n_permutations=n_permutations,\n                                                   pair=pair,\n                                                   rng=rng)\n    else:\n        warnings.warn(\"A Permutation space is given so n_permutations will fall back to what's specified there.\",\n                      stacklevel=2)\n\n    if lazy:\n        shapley_table = get_shapley_table(permutation_space=permutation_space,\n                                          lesioned=lesioned,\n                                          lazy=True,\n                                          objective_function=objective_function,\n                                          objective_function_params=objective_function_params,\n                                          dual_progress_bars=dual_progress_bars,\n                                          save_permutations=save_permutations,\n                                          mbar=mbar)[elements]\n        return shapley_table\n\n    combination_space = make_combination_space(permutation_space=permutation_space,\n                                               pair=pair,\n                                               lesioned=lesioned)\n    complement_space = make_complement_space(combination_space=combination_space,\n                                             elements=elements,\n                                             lesioned=lesioned)\n\n    if n_parallel_games == 1:\n        contributions, _ = take_contributions(elements=elements,\n                                              complement_space=complement_space,\n                                              combination_space=combination_space,\n                                              objective_function=objective_function,\n                                              objective_function_params=objective_function_params,\n                                              mbar=mbar)\n    else:\n        contributions, _ = ut.parallelized_take_contributions(\n            multiprocessing_method=multiprocessing_method,\n            n_cores=n_parallel_games,\n            complement_space=complement_space,\n            combination_space=combination_space,\n            objective_function=objective_function,\n            objective_function_params=objective_function_params,\n            mbar=mbar)\n\n    shapley_table = get_shapley_table(contributions=contributions,\n                                      permutation_space=permutation_space,\n                                      dual_progress_bars=dual_progress_bars,\n                                      save_permutations=save_permutations,\n                                      lesioned=lesioned, mbar=mbar)[elements]\n    return shapley_table\n</code></pre>"},{"location":"API/make_combination_space.html","title":"Make combination space","text":"<p>Generates a dataset (OrderedSet) of coalitions from the permutation_space. In principle, this could be directly filled and passed to the make_shapley_values function but then the function wouldn't be pure so this will be just an empty template. Don't mix up this and the later-filled combination space. Briefly, the combination space of one permutation of (A,B,C) is something like this:</p> <p>(A,B,C) (A,B) (B,C) (A,C) (C) (B) (A) ()</p> <p>This will happen for every permutation of the permutation space so either there will be a large number of combinations here or if the set is small enough, it will be exhausted.</p> <p>Parameters:</p> Name Type Description Default <code>permutation_space</code> <code>list</code> <p>A list of players to be shuffled n times.</p> required <code>pair</code> <code>Optional[Tuple]</code> <p>pair of elements that will always be together in every combination</p> <code>None</code> <code>lesioned</code> <code>Optional[any]</code> <p>leseioned element that will not be present in any combination</p> <code>None</code> <p>Returns:</p> Type Description <code>OrderedSet</code> <p>Combination space as an OrderedSet of frozensets.</p> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef make_combination_space(*, permutation_space: list, pair: Optional[Tuple] = None, lesioned: Optional[any] = None) -&gt; OrderedSet:\n    \"\"\"\n    Generates a dataset (OrderedSet) of coalitions from the permutation_space.\n    In principle, this could be directly filled and passed to the make_shapley_values function\n    but then the function wouldn't be pure so **this will be just an empty template**.\n    Don't mix up this and the later-filled combination space.\n    Briefly, the combination space of **one permutation of** (A,B,C) is something like this:\n\n    (A,B,C)\n    (A,B)\n    (B,C)\n    (A,C)\n    (C)\n    (B)\n    (A)\n    ()\n\n    This will happen for every permutation of the permutation space so either there will be a large number\n    of combinations here or if the set is small enough, it will be exhausted.\n\n    Args:\n        permutation_space (list):\n            A list of players to be shuffled n times.\n\n        pair (Optional[Tuple]):\n            pair of elements that will always be together in every combination\n\n        lesioned (Optional[any]):\n            leseioned element that will not be present in any combination\n\n    Returns:\n        (OrderedSet): Combination space as an OrderedSet of frozensets.\n    \"\"\"\n\n    _check_valid_permutation_space(permutation_space)\n\n    # if we have an element that needs to be lesioned in every combination, then we store it in a set so that taking a difference becomes easier and efficient\n    lesioned = {lesioned} if lesioned else set()\n\n    combination_space = OrderedSet()\n\n    # iterate over all permutations and generate including and excluding combinations\n    for permutation in permutation_space:\n        # we need this parameter if we have a pair so that the pair of elements are always together\n        skip_next = False\n        for index, element in enumerate(permutation):\n            # logic to skip the next element if we encounter a pair element\n            if skip_next:\n                skip_next = False\n                continue\n            if pair and element == pair[0]:\n                index += 1\n                skip_next = True\n\n            # forming the coalition with the target element\n            including = frozenset(permutation[:index + 1]) - lesioned\n            # forming it without the target element\n            excluding = frozenset(permutation[:index]) - lesioned\n\n            combination_space.add(including)\n            combination_space.add(excluding)\n\n    return combination_space\n</code></pre>"},{"location":"API/make_complement_space.html","title":"Make complement space","text":"<p>Produces the complement space of the combination space, useful for debugging and the multiprocessing function. Args:     combination_space (OrderedSet):         ordered set of target combinations (coalitions).     elements (list):         list of players.     lesioned (Optional[any]):         leseioned element that will not be present in any combination but every complement</p> <p>Returns:</p> Type Description <code>OrderedSet</code> <p>complements to be passed for lesioning.</p> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef make_complement_space(*,\n                          combination_space: OrderedSet,\n                          elements: list,\n                          lesioned: Optional[any] = None) -&gt; OrderedSet:\n    \"\"\"\n    Produces the complement space of the combination space, useful for debugging\n    and the multiprocessing function.\n    Args:\n        combination_space (OrderedSet):\n            ordered set of target combinations (coalitions).\n        elements (list):\n            list of players.\n        lesioned (Optional[any]):\n            leseioned element that will not be present in any combination but every complement\n\n    Returns:\n        (OrderedSet): complements to be passed for lesioning.\n    \"\"\"\n    _check_valid_elements(elements)\n    elements = frozenset(elements)\n    _check_valid_combination_space(combination_space, elements, lesioned)\n\n    complement_space = OrderedSet()\n\n    # iterate over all combinations and take their difference from set elements to find complements\n    for combination in combination_space:\n        complement_space.add(tuple(elements.difference(combination)))\n    return complement_space\n</code></pre>"},{"location":"API/make_permutation_space.html","title":"Make permutation space","text":"<p>Generates a list of tuples containing n_permutations of the given elements. This will be used later in make_combination_space so you can have the same permutation and combination spaces for different games played by the same set. Probably makes things more reproducible! The elements themselves can be anything I guess, I tried str (names/labels) and integers (indexes), and tuples (edges, from-to style). Briefly, the permutation space of (A,B,C) is something like this:</p> <p>(A,B,C) (B,C,A) (C,B,A) (A,C,B) (C,A,B) (B,A,C) (C,A,B) . . . As you might have seen, there might be repetitions for small set of players and that's fine.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>list</code> <p>A list of players to be shuffled n times.</p> required <code>n_permutations</code> <code>int</code> <p>Number of permutations, Didn't check it systematically yet but just based on random explorations I'd say something around 1_000 is enough.</p> required <code>pair</code> <code>Optional[Tuple]</code> <p>pair of elements that will always be together in every permutation</p> <code>None</code> <code>rng</code> <code>Optional[Generator]</code> <p>Numpy random generator object used for reproducable results. Default is None.</p> <code>None</code> <code>random_seed</code> <code>Optional[int]</code> <p>sets the random seed of the sampling process. Only used when <code>rng</code> is None. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>Permutation space as a list of lists with shape (n_permutations, len(elements))</p> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef make_permutation_space(*,\n                           elements: list,\n                           n_permutations: int,\n                           pair: Optional[Tuple] = None,\n                           rng: Optional[np.random.Generator] = None,\n                           random_seed: Optional[int] = None) -&gt; list:\n    \"\"\"\n    Generates a list of tuples containing n_permutations of the given elements.\n    This will be used later in make_combination_space so you can have the same permutation and combination spaces for\n    different games played by the same set. Probably makes things more reproducible!\n    The elements themselves can be anything I guess, I tried str (names/labels) and integers (indexes),\n    and tuples (edges, from-to style).\n    Briefly, the permutation space of (A,B,C) is something like this:\n\n    (A,B,C)\n    (B,C,A)\n    (C,B,A)\n    (A,C,B)\n    (C,A,B)\n    (B,A,C)\n    (C,A,B)\n    .\n    .\n    .\n    As you might have seen, there might be repetitions for small set of players and that's fine.\n\n    Args:\n        elements (list):\n            A list of players to be shuffled n times.\n\n        n_permutations (int):\n            Number of permutations, Didn't check it systematically yet but just based on random explorations I'd say\n            something around 1_000 is enough.\n\n        pair (Optional[Tuple]):\n            pair of elements that will always be together in every permutation\n\n        rng (Optional[np.random.Generator]): Numpy random generator object used for reproducable results. Default is None.\n\n        random_seed (Optional[int]):\n            sets the random seed of the sampling process. Only used when `rng` is None. Default is None.\n\n    Returns:\n        (list[tuple]): Permutation space as a list of lists with shape (n_permutations, len(elements))\n    \"\"\"\n\n    # ------------------------------#\n    _check_valid_elements(elements)\n    _check_valid_n_permutations(n_permutations)\n    # ------------------------------#\n\n    type_of_elements = type(elements[0])\n\n    # create a numpy random number generator if one is not passed\n    if not rng:\n        rng = np.random.default_rng(random_seed) if random_seed \\\n            else np.random.default_rng()\n\n    # return n_permutations random permutations if pair argument is not passed\n    if not pair:\n        permutation_space = [tuple(type_of_elements(element)\n                                   for element in rng.permutation(elements))\n                             for _ in range(n_permutations)]\n\n        return permutation_space\n\n    # if the pair argument is passed, then all permutations will have those two elements together using the following logic\n    elements = [e for e in elements if e != pair[0]]\n    permutation_space = []\n\n    for _ in range(n_permutations):\n        permutation = list(rng.permutation(elements))\n        permutation.insert(permutation.index(pair[1]), pair[0])\n        permutation_space.append(tuple(permutation))\n\n    return permutation_space\n</code></pre>"},{"location":"API/network_interaction_2d.html","title":"Network interaction 2d","text":"<p>Performs Two dimensional MSA as explain in section 2.3 of [1]     for every possible pair of elements and returns a symmetric matrix of     interactions between the elements.</p> <pre><code>Args:\n    n_permutations (int): Number of permutations (samplescontributions_excluding) per element.\n\n    elements (list): List of the players (elements). Can be strings (names), integers (indicies), and tuples.\n\n    pairs (Optional[list]): List of pairs of elements that you want to analyze the interaction between. \n        Defaults to None which means all possible pairs\n\n    objective_function (Callable):\n        The game (in-silico experiment). It should get the complement set and return one numeric value\n        either int or float.\n        This function is just calling it as: objective_function(complement, **objective_function_params)\n\n        An example using networkx with some tips:\n        (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)\n\n        def local_efficiency(complements, graph):\n            if len(complements) &lt; 0:\n                # the network is intact so:\n                return nx.local_efficiency(graph)\n\n            elif len(complements) == len(graph):\n                # the network is fully lesioned so:\n                return 0.0\n\n            else:\n                # lesion the system, calculate things\n                lesioned = graph.copy()\n                lesioned.remove_nodes_from(complements)\n                return nx.local_efficiency(lesioned)\n</code></pre> <p>contributions_excluding         objective_function_params (Dict, optional): Kwargs for the objective_function. Defaults to {}.</p> <pre><code>    multiprocessing_method (str, optional): \n        So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib.\n        If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their\n        documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!)\n        and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around,\n        I realized joblib is faster for tasks that are small themselves. Remedies are here:\n        https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html\n\n        Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself\n        takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a\n        second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:\n\n            - no parallel: 1020 seccontributions_excluding\n            - joblib: 63 sec\n            - ray: 65 sec\n\n        That makes sense since I have 16 cores and 1000/16 is around 62. \n        Defaults to 'joblib'.\n\n    rng (Optional[np.random.Generator], optional): Numpy random generator object used for reproducable results. Default is None. Defaults to None.\n\n    random_seed (Optional[int], optional): \n        sets the random seed of the sampling process. Only used when `rng` is None. Default is None. Defaults to None.\n\n    n_parallel_games (int):\n        Number of parallel jobs (number of to-be-occupied cores),\n        -1 means all CPU cores and 1 means a serial process.\n        I suggest using 1 for debugging since things get messy in parallel!\n\n\nRaises:\n    NotImplementedError: Raises this error in case the contribution is a timeseries or there are\n        multiple contributions\n\nReturns:\n    np.ndarray: the interaction matrix\n</code></pre> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef network_interaction_2d(*,\n                           n_permutations: int,\n                           elements: list,\n                           pairs: Optional[list] = None,\n                           objective_function: Callable,\n                           objective_function_params: Dict = {},\n                           multiprocessing_method: str = 'joblib',\n                           rng: Optional[np.random.Generator] = None,\n                           random_seed: Optional[int] = None,\n                           n_parallel_games: int = -1,\n                           lazy: bool = False\n                           ) -&gt; np.ndarray:\n    \"\"\"Performs Two dimensional MSA as explain in section 2.3 of [1]\n    for every possible pair of elements and returns a symmetric matrix of\n    interactions between the elements.\n\n    Args:\n        n_permutations (int): Number of permutations (samplescontributions_excluding) per element.\n\n        elements (list): List of the players (elements). Can be strings (names), integers (indicies), and tuples.\n\n        pairs (Optional[list]): List of pairs of elements that you want to analyze the interaction between. \n            Defaults to None which means all possible pairs\n\n        objective_function (Callable):\n            The game (in-silico experiment). It should get the complement set and return one numeric value\n            either int or float.\n            This function is just calling it as: objective_function(complement, **objective_function_params)\n\n            An example using networkx with some tips:\n            (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)\n\n            def local_efficiency(complements, graph):\n                if len(complements) &lt; 0:\n                    # the network is intact so:\n                    return nx.local_efficiency(graph)\n\n                elif len(complements) == len(graph):\n                    # the network is fully lesioned so:\n                    return 0.0\n\n                else:\n                    # lesion the system, calculate things\n                    lesioned = graph.copy()\n                    lesioned.remove_nodes_from(complements)\n                    return nx.local_efficiency(lesioned)\ncontributions_excluding\n        objective_function_params (Dict, optional): Kwargs for the objective_function. Defaults to {}.\n\n        multiprocessing_method (str, optional): \n            So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib.\n            If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their\n            documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!)\n            and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around,\n            I realized joblib is faster for tasks that are small themselves. Remedies are here:\n            https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html\n\n            Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself\n            takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a\n            second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:\n\n                - no parallel: 1020 seccontributions_excluding\n                - joblib: 63 sec\n                - ray: 65 sec\n\n            That makes sense since I have 16 cores and 1000/16 is around 62. \n            Defaults to 'joblib'.\n\n        rng (Optional[np.random.Generator], optional): Numpy random generator object used for reproducable results. Default is None. Defaults to None.\n\n        random_seed (Optional[int], optional): \n            sets the random seed of the sampling process. Only used when `rng` is None. Default is None. Defaults to None.\n\n        n_parallel_games (int):\n            Number of parallel jobs (number of to-be-occupied cores),\n            -1 means all CPU cores and 1 means a serial process.\n            I suggest using 1 for debugging since things get messy in parallel!\n\n\n    Raises:\n        NotImplementedError: Raises this error in case the contribution is a timeseries or there are\n            multiple contributions\n\n    Returns:\n        np.ndarray: the interaction matrix\n    \"\"\"\n    elements_idx = list(range(len(elements)))\n\n    # create a list of pairs for wich we'll calculate the 2d interaction. By default, all possible pairs are considered unless specified otherwise\n    all_pairs = [(elements.index(x), elements.index(y))\n                 for x, y in pairs] if pairs else combinations(elements_idx, 2)\n\n    interface_args = {\"elements\": elements,\n                      \"n_permutations\": n_permutations,\n                      \"objective_function_params\": objective_function_params,\n                      \"objective_function\": objective_function,\n                      \"multiprocessing_method\": multiprocessing_method,\n                      \"rng\": rng,\n                      \"random_seed\": random_seed,\n                      \"n_parallel_games\": n_parallel_games,\n                      \"lazy\": lazy}\n\n    interactions = np.zeros((len(elements), len(elements)))\n\n    # iterate over all the pairs to run interaction_2d\n    for x, y in tqdm(all_pairs, desc=\"Running interface 2d for all pair of nodes:\"):\n        gammaAB, gammaA, gammaB = interaction_2d(pair=(elements[x], elements[y]),\n                                                 **interface_args)\n        if not _is_number(gammaAB):\n            raise NotImplementedError(\"`network_interaction_2d` does not work with\"\n                                      \" timeseries or multiscore contributions yet.\")\n        interactions[x, y] = interactions[y, x] = gammaAB - gammaA - gammaB\n\n    return interactions\n</code></pre>"},{"location":"API/take_contributions.html","title":"Take contributions","text":"<p>This function fills up the combination_space with the game you define (objective function). There is an important point to keep in mind, Shapley values are the added contributions of elements while in MSA we calculate them by perturbation so although it's intuitive to think the combination in combination space is the element that will be lesioned, it is not the case, it will be everything else but the coalition, i.e., the target coalition are the only intact elements. This function takes care of this by passing the complement of each coalition to the game while assigning the results to the target coalition, just keep the logic in mind.</p> <p>A second point is that this function returns a filled combination_space, it is not filling it in-place for the sake of purity.</p> <p>Note on returns:     Contributions and lesion effects are virtually the same thing it's just about how you're looking at them.     For example, you might want to use lesion effects by conditioning elements' length and see the effect of     single lesions, dual, triple,... so, for contributions we have a value contributed by the intact coalition,     the same result can be compared to the intact system to see how big was the impact of lesioning the complements.     \"Same same, but different, but still same!\" - James Franco</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>list</code> <p>List of the players. Obviously, should be the same passed to make_permutation.</p> required <code>complement_space</code> <code>OrderedSet</code> <p>The actual targets for lesioning. Shapley values are the added contributions of elements while in MSA we calculate them by perturbation so although it's intuitive to think the combination in combination space is the element that will be lesioned, it is not the case, it will be everything else but the coalition, i.e., the target coalition are the only intact elements.</p> required <code>combination_space</code> <code>OrderedSet</code> <p>The template, will be copied, filled by the objective_function, and returned.</p> required <code>objective_function</code> <code>Callable</code> <p>The game, it should get the complement set and return one numeric value either int or float. This function is just calling it as: objective_function(complement, **objective_function_params) so design accordingly.</p> <p>An example using networkx with some tips: (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)</p> <p>def local_efficiency(complements, graph):     if len(complements) &lt; 0:         # the network is intact so:         return nx.local_efficiency(graph)</p> <pre><code>elif len(complements) == len(graph):\n    # the network is fully lesioned so:\n    return 0.0\n\nelse:\n    # lesion the system, calculate things\n    lesioned = graph.copy()\n    lesioned.remove_nodes_from(complements)\n    return nx.local_efficiency(lesioned)\n</code></pre> required <code>objective_function_params</code> <code>Optional[Dict]</code> <p>Kwargs for the objective_function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>A dictionary of combinations:results</p> Source code in <code>msapy/msa.py</code> <pre><code>@typechecked\ndef take_contributions(*,\n                       elements: list,\n                       complement_space: OrderedSet,\n                       combination_space: OrderedSet,\n                       objective_function: Callable,\n                       objective_function_params: Optional[Dict] = None,\n                       mbar: Optional[MasterBar] = None) -&gt; Tuple[Dict, Dict]:\n    \"\"\"\n    This function fills up the combination_space with the game you define (objective function). There is an important\n    point to keep in mind, Shapley values are the added contributions of elements while in MSA we calculate them by\n    perturbation so although it's intuitive to think the combination in combination space is the element that will be\n    lesioned, it is not the case, it will be everything else but the coalition, i.e., the target coalition are the\n    only intact elements. This function takes care of this by passing the complement of each coalition to the\n    game while assigning the results to the target coalition, just keep the logic in mind.\n\n    A second point is that this function returns a filled combination_space, it is not filling it in-place for the\n    sake of purity.\n\n    ---------------\n    Note on returns:\n        Contributions and lesion effects are virtually the same thing it's just about how you're looking at them.\n        For example, you might want to use lesion effects by conditioning elements' length and see the effect of\n        single lesions, dual, triple,... so, for contributions we have a value contributed by the intact coalition,\n        the same result can be compared to the intact system to see how big was the impact of lesioning the complements.\n        \"Same same, but different, but still same!\" - James Franco\n\n    Args:\n        elements (list):\n            List of the players. Obviously, should be the same passed to make_permutation.\n\n        complement_space (OrderedSet):\n            The actual targets for lesioning. Shapley values are the added contributions of elements\n            while in MSA we calculate them by perturbation so although it's intuitive to think the combination\n            in combination space is the element that will be lesioned, it is not the case,\n            it will be everything else but the coalition, i.e., the target coalition are the only intact elements.\n\n        combination_space (OrderedSet):\n            The template, will be copied, filled by the objective_function, and returned.\n\n        objective_function (Callable):\n            The game, it should get the complement set and return one numeric value either int or float.\n            This function is just calling it as: objective_function(complement, **objective_function_params)\n            so design accordingly.\n\n            An example using networkx with some tips:\n            (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)\n\n            def local_efficiency(complements, graph):\n                if len(complements) &lt; 0:\n                    # the network is intact so:\n                    return nx.local_efficiency(graph)\n\n                elif len(complements) == len(graph):\n                    # the network is fully lesioned so:\n                    return 0.0\n\n                else:\n                    # lesion the system, calculate things\n                    lesioned = graph.copy()\n                    lesioned.remove_nodes_from(complements)\n                    return nx.local_efficiency(lesioned)\n\n        objective_function_params (Optional[Dict]):\n            Kwargs for the objective_function.\n\n    Returns:\n        (Dict): A dictionary of combinations:results\n    \"\"\"\n\n    elements = frozenset(elements)\n    contributions = dict.fromkeys(combination_space)\n    lesion_effects = dict.fromkeys(complement_space)\n    objective_function_params = objective_function_params if objective_function_params else {}\n\n    # ------------------------------#\n    if len(complement_space.items[1]) == 0:\n        warnings.warn(\"Are you sure you're not mistaking complement and combination spaces?\"\n                      \"Length of the first element in complement space (really, complement_space[1]) is 0. \"\n                      \"It should be equal to the number of elements.\",\n                      stacklevel=2)\n    # ------------------------------#\n\n    # run the objective function over all complement space\n    for combination, complement in progress_bar(zip(combination_space, complement_space), parent=mbar, total=len(combination_space), leave=False):\n        result = objective_function(complement, **objective_function_params)\n\n        contributions[combination] = result\n        lesion_effects[complement] = result\n    return contributions, lesion_effects\n</code></pre>"},{"location":"Utils/bootstrap_hypothesis_testing.html","title":"Bootstrap hypothesis testing","text":"<p>Performs a bootstrap hypothesis testing on the given dataset to find which elements have significant contributions. Null hypothesis is: Elements have on average no contributions, unless a reference mean is given. This can be used for both a dataset of Shapley values (Shapley table) or a dataset of lesions if there are many samples for each element, e.g., if lesioning an element significantly impacted some feature of the system. For more information, watch this brilliant tutorial: https://www.youtube.com/watch?v=isEcgoCmlO0&amp;t=893s</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>Common case use is Shapley table but anything else works too, as long as it follows the same structure/format and you know what you're doing.</p> required <code>p_value</code> <code>float</code> <p>Default is 0.05 please first correct for multiple-comparisons easiest and most conservative way: Bonferroni correction, p_value/n_elements.</p> <code>0.05</code> <code>bootstrap_samples</code> <code>int</code> <p>Number of bootstraps, default is 10_000 and I seems like it's a common practice to use 10_000.</p> <code>10000</code> <code>reference_mean</code> <code>Optional[int]</code> <p>In case the means should not be compared with a zero-mean distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Distributions that are significantly different from H0.</p>"},{"location":"Utils/bootstrap_hypothesis_testing.html#msapy.utils.bootstrap_hypothesis_testing--todo-really-needs-some-performance-optimization-probably-with-numba-and-a-change-in-the-algorithm","title":"TODO: really needs some performance optimization, probably with Numba and a change in the algorithm!","text":"Source code in <code>msapy/utils.py</code> <pre><code>@typechecked\ndef bootstrap_hypothesis_testing(*,\n                                 dataset: pd.DataFrame,\n                                 p_value: float = 0.05,\n                                 bootstrap_samples: int = 10_000,\n                                 reference_mean: Optional[int] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs a bootstrap hypothesis testing on the given dataset to find which elements have significant contributions.\n    Null hypothesis is: Elements have on average no contributions, unless a reference mean is given. This can be used\n    for both a dataset of Shapley values (Shapley table) or a dataset of lesions if there are many samples for each\n    element, e.g., if lesioning an element significantly impacted some feature of the system.\n    For more information, watch this brilliant tutorial:\n    https://www.youtube.com/watch?v=isEcgoCmlO0&amp;t=893s\n\n    Args:\n        dataset (pd.DataFrame):\n            Common case use is Shapley table but anything else works too,\n            as long as it follows the same structure/format and you know what you're doing.\n\n        p_value (float):\n            Default is 0.05 **please first correct for multiple-comparisons** easiest and most conservative way:\n            Bonferroni correction, p_value/n_elements.\n\n        bootstrap_samples (int):\n            Number of bootstraps, default is 10_000 and I seems like it's a common practice to use 10_000.\n\n        reference_mean (Optional[int]):\n            In case the means should not be compared with a zero-mean distribution.\n\n    Returns:\n        pd.DataFrame: Distributions that are significantly different from H0.\n\n    #TODO: really needs some performance optimization, probably with Numba and a change in the algorithm!\n    \"\"\"\n    mean_adjusted_distributions = pd.DataFrame()\n    bootstrapped_distributions = pd.DataFrame()\n    significants = pd.DataFrame()\n    percentile = (1 - p_value) * 100\n\n    if p_value &lt;= 0.:\n        raise ValueError(\"A zero/negative value for p_value? What?\")\n\n    if bootstrap_samples &lt;= 0.:\n        raise ValueError(\"A zero/negative value for bootstrap_samples? What?\")\n\n    elif 1 &lt; bootstrap_samples &lt; 1_000:\n        warnings.warn(\"Bootstrap sample size is small, please go above 1000.\", stacklevel=2)\n\n    for distribution in tqdm(dataset.columns, total=len(dataset.columns), desc='Bootstrapping: '):\n        if reference_mean:  # adjusting the distributions to have the same mean as the reference.\n            mean_adjusted_distributions[distribution] = \\\n                dataset[distribution] - dataset[distribution].mean() + reference_mean\n\n        else:  # adjusting the distributions to center around zero.\n            mean_adjusted_distributions[distribution] = \\\n                dataset[distribution] - dataset[distribution].mean()\n\n        resampled_means = []\n        for sample in range(bootstrap_samples):  # resampling (with replacement) from the mean-adjusted distribution\n            resampled_means.append(np.mean((np.random.choice(\n                list(mean_adjusted_distributions[distribution]),\n                len(mean_adjusted_distributions[distribution].values),\n                replace=True))))\n\n        bootstrapped_distributions[distribution] = resampled_means\n\n    # checking if the means are significantly different.\n    for bootstrapped_distribution in bootstrapped_distributions.columns:\n        percentiles = np.percentile(bootstrapped_distributions[bootstrapped_distribution], [0, percentile])\n        if not percentiles[0] &lt;= dataset[bootstrapped_distribution].mean() &lt;= percentiles[1]:\n            significants[bootstrapped_distribution] = dataset[bootstrapped_distribution]\n\n    significants = sorter(significants)\n    return significants\n</code></pre>"},{"location":"Utils/distribution_of_processing.html","title":"Distribution of processing","text":"<p>Calculates how much the function is distributed accross the system, with values close to 0 means more localized functions and values near 1 means most elements are fairly involved in producing the outcome. Remember, this value will be low if many units have near zero shapley values while a few has either positive or negative contributions. So, negative contributions still count as involvment in the process.</p> read more here <p>Aharonov, R., Segev, L., Meilijson, I., &amp; Ruppin, E. 2003. Localization of function via lesion analysis. Neural Computation.</p> and here <p>Saggie-Wexler, Keren, Alon Keinan, and Eytan Ruppin. 2006. Neural Processing of Counting in Evolved Spiking and McCulloch-Pitts Agents. Artificial Life.</p> <p>Parameters:</p> Name Type Description Default <code>shapley_vector</code> <code>DataFrame</code> <p>Shapley values of the system, not the shapley table tho, shapley values themselves, i.e., averaged over samples so each element has one shapley value.</p> required <p>Returns:</p> Type Description <code>float64</code> <p>np.float64: distribution of processing!</p> Source code in <code>msapy/utils.py</code> <pre><code>@typechecked\ndef distribution_of_processing(*, shapley_vector: pd.Series) -&gt; np.float64:\n    \"\"\"\n    Calculates how much the function is distributed accross the system, with values close to 0 means more localized\n    functions and values near 1 means most elements are fairly involved in producing the outcome. Remember, this value\n    will be low if many units have near zero shapley values while a few has either positive or negative contributions.\n    So, negative contributions still count as involvment in the process.\n\n    read more here:\n        Aharonov, R., Segev, L., Meilijson, I., &amp; Ruppin, E. 2003.\n        Localization of function via lesion analysis.\n        Neural Computation.\n\n    and here:\n        Saggie-Wexler, Keren, Alon Keinan, and Eytan Ruppin. 2006.\n        Neural Processing of Counting in Evolved Spiking and McCulloch-Pitts Agents.\n        Artificial Life.\n\n    Args:\n        shapley_vector (pd.DataFrame):\n            Shapley values of the system, not the shapley table tho, shapley values themselves, i.e., averaged over\n            samples so each element has one shapley value.\n\n    returns:\n        np.float64: distribution of processing!\n    \"\"\"\n    normalized = shapley_vector / shapley_vector.abs().sum()  # L1 norm\n    d = 1 - normalized.std(ddof=0) / np.sqrt((len(normalized) - 1) / len(normalized) ** 2)\n    return d\n</code></pre>"},{"location":"Utils/parallelized_take_contributions.html","title":"Parallelized take contributions","text":"<p>Same as the take_contribution function but parallelized over CPU cores to boost performance. I'd first try the single msapy version on a toy example to make sure everything makes sense then go for this because debugging parallel jobs is a disaster. Also, you don't need this if your game is happening on GPU. For HPC systems, I guess either dask or ray will be better options.</p> <p>Note on returns:     Contributions and lesion effects are virtually the same thing it's just about how you're looking at them.     For example, you might want to use lesion effects by conditioning elements' length and see the effect of     single lesions, dual, triple,... so, for contributions we have a value contributed by the intact coalition,     the same result can be compared to the intact system to see how big was the impact of lesioning the complements.     \"Same same, but different, but still same!\" - James Franco</p> <p>Parameters:</p> Name Type Description Default <code>multiprocessing_method</code> <code>str</code> <p>So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib. If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!) and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around, I realized joblib is faster for tasks that are small themselves. Remedies are here: https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html</p> <p>Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:</p> <pre><code>- no parallel: 1020 sec\n- joblib: 63 sec\n- ray: 65 sec\n</code></pre> <p>That makes sense since I have 16 cores and 1000/16 is around 62. TODO: allow more flexibility in ray method. Scaling up to a cluster?</p> <code>'joblib'</code> <code>n_cores</code> <code>int</code> <p>Number of parallel games. Default is -1, which means all cores so it can make the system freeze for a short period, if that happened then maybe go for -2, which means one msapy is left out. Or really just specify the number of threads you want to use!</p> <code>-1</code> <code>complement_space</code> <code>OrderedSet</code> <p>The actual targets for lesioning. Shapley values are the added contributions of elements while in MSA we calculate them by perturbation so although it's intuitive to think the combination in combination space is the element that will be lesioned, it is not the case, it will be everything else but the coalition, i.e., the target coalition are the only intact elements.</p> required <code>combination_space</code> <code>OrderedSet</code> <p>The template, will be copied, filled by the objective_function, and returned.</p> required <code>objective_function</code> <code>Callable</code> <p>The game, it should get the complement set and return one numeric value either int or float. This function is just calling it as: objective_function(complement, **objective_function_params) so design accordingly.</p> <p>An example using networkx with some tips: (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)</p> <pre><code>def local_efficiency(complements, graph):\n    if len(complements) &lt; 0:\n       # the network is intact so:\n       return nx.local_efficiency(graph)\n\n    elif len(complements) == len(graph):\n       # the network is fully lesioned so:\n       return 0.0\n\n    else:\n       # lesion the system, calculate things\n       lesioned = graph.copy()\n       lesioned.remove_nodes_from(complements)\n       return nx.local_efficiency(lesioned)\n</code></pre> required <code>objective_function_params</code> <code>Optional[Dict]</code> <p>Kwargs for the objective_function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <ul> <li>contributions: A dictionary of coalitions:results</li> <li>lesion_effects: A dictionary of lesions:results</li> </ul> Source code in <code>msapy/utils.py</code> <pre><code>@typechecked\ndef parallelized_take_contributions(*,\n                                    multiprocessing_method: str = 'joblib',\n                                    n_cores: int = -1,\n                                    complement_space: OrderedSet,\n                                    combination_space: OrderedSet,\n                                    objective_function: Callable,\n                                    objective_function_params: Optional[Dict] = None,\n                                    mbar=None) -&gt; Tuple[Dict, Dict]:\n    \"\"\"\n    Same as the take_contribution function but parallelized over CPU cores to boost performance.\n    I'd first try the single msapy version on a toy example to make sure everything makes sense then\n    go for this because debugging parallel jobs is a disaster. Also, you don't need this if your game\n    is happening on GPU. For HPC systems, I guess either dask or ray will be better options.\n    ---------------\n    Note on returns:\n        Contributions and lesion effects are virtually the same thing it's just about how you're looking at them.\n        For example, you might want to use lesion effects by conditioning elements' length and see the effect of\n        single lesions, dual, triple,... so, for contributions we have a value contributed by the intact coalition,\n        the same result can be compared to the intact system to see how big was the impact of lesioning the complements.\n        \"Same same, but different, but still same!\" - James Franco\n\n    Args:\n        multiprocessing_method (str):\n            So far, two methods of parallelization is implemented, 'joblib' and 'ray' and the default method is joblib.\n            If using ray tho, you need to decorate your objective function with @ray.remote decorator. Visit their\n            documentations to see how to go for it. I guess ray works better on HPC clusters (if they support it tho!)\n            and probably doesn't suffer from the sneaky \"memory leakage\" of joblib. But just by playing around,\n            I realized joblib is faster for tasks that are small themselves. Remedies are here:\n            https://docs.ray.io/en/latest/auto_examples/tips-for-first-time.html\n\n            Note: Generally, multiprocessing isn't always faster as explained above. Use it when the function itself\n            takes some like each game takes longer than 0.5 seconds or so. For example, a function that sleeps for a\n            second on a set of 10 elements with 1000 permutations each (1024 games) performs as follows:\n\n                - no parallel: 1020 sec\n                - joblib: 63 sec\n                - ray: 65 sec\n\n            That makes sense since I have 16 cores and 1000/16 is around 62.\n            TODO: allow more flexibility in ray method. Scaling up to a cluster?\n\n        n_cores (int):\n            Number of parallel games. Default is -1, which means all cores so it can make the system\n            freeze for a short period, if that happened then maybe go for -2, which means one msapy is\n            left out. Or really just specify the number of threads you want to use!\n\n        complement_space (OrderedSet):\n            The actual targets for lesioning. Shapley values are the added contributions of elements\n            while in MSA we calculate them by perturbation so although it's intuitive to think the combination\n            in combination space is the element that will be lesioned, it is not the case,\n            it will be everything else but the coalition, i.e., the target coalition are the only intact elements.\n\n        combination_space (OrderedSet):\n            The template, will be copied, filled by the objective_function, and returned.\n\n        objective_function (Callable):\n            The game, it should get the complement set and return one numeric value either int or float.\n            This function is just calling it as: objective_function(complement, **objective_function_params)\n            so design accordingly.\n\n            An example using networkx with some tips:\n            (you sometimes need to specify what should happen during edge-cases like an all-lesioned network)\n\n            &gt;&gt;&gt;     def local_efficiency(complements, graph):\n            &gt;&gt;&gt;         if len(complements) &lt; 0:\n            &gt;&gt;&gt;            # the network is intact so:\n            &gt;&gt;&gt;            return nx.local_efficiency(graph)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt;         elif len(complements) == len(graph):\n            &gt;&gt;&gt;            # the network is fully lesioned so:\n            &gt;&gt;&gt;            return 0.0\n            &gt;&gt;&gt;\n            &gt;&gt;&gt;         else:\n            &gt;&gt;&gt;            # lesion the system, calculate things\n            &gt;&gt;&gt;            lesioned = graph.copy()\n            &gt;&gt;&gt;            lesioned.remove_nodes_from(complements)\n            &gt;&gt;&gt;            return nx.local_efficiency(lesioned)\n\n        objective_function_params (Optional[Dict]):\n            Kwargs for the objective_function.\n\n    Returns:\n        (Tuple[Dict, Dict]): \n            - contributions: A dictionary of coalitions:results\n            - lesion_effects: A dictionary of lesions:results\n    \"\"\"\n    objective_function_params = objective_function_params if objective_function_params else {}\n    cbar = progress_bar(complement_space, total=len(complement_space), parent=mbar, leave=False)\n\n    if len(complement_space.items[0]) == 1:\n        warnings.warn(\"Are you sure you're not mistaking complement and combination spaces?\"\n                      \"Length of the first element in complement space is 1, that is usually n_elements-1\",\n                      stacklevel=2)\n    if multiprocessing_method == 'ray':\n        if importlib.util.find_spec(\"ray\") is None:\n            raise ImportError(\"The ray package is required to run this algorithm\")\n\n        import ray\n        if type(objective_function) is not ray.remote_function.RemoteFunction:\n            raise ValueError(\"Objective function is not decorated with ray. You probably forgot @ray.remote\")\n\n        if n_cores &lt;= 0:\n            warnings.warn(\"A zero or a negative n_cores was passed and ray doesn't like so \"\n                          \"to fix that ray.init() will get no arguments, \"\n                          \"which means use all cores as n_cores = -1 does for joblib.\", stacklevel=2)\n            ray.init()\n        else:\n            ray.init(num_cpus=n_cores)\n\n        result_ids = [objective_function.remote(complement, **objective_function_params) for complement in complement_space]\n        for _ in tqdm(ray_iterator(result_ids), total=len(result_ids)):\n            pass\n\n        results = ray.get(result_ids)\n        ray.shutdown()\n\n    elif multiprocessing_method == 'joblib':\n        results = (Parallel(n_jobs=n_cores)(delayed(objective_function)(\n            complement, **objective_function_params) for complement in cbar))\n    else:\n        raise NotImplemented(\"Available multiprocessing backends are 'ray' and 'joblib'\")\n\n    contributions = dict(zip(combination_space, results))\n    lesion_effects = dict(zip(complement_space, results))\n\n    gc.collect()\n    get_reusable_executor().shutdown(wait=True)\n\n    return contributions, lesion_effects\n</code></pre>"},{"location":"Utils/sorter.html","title":"Sorter","text":"Sorts the elements based on their average shapley values or in ascending order by calling <p><code>df.sort_index(axis=1)</code></p> <p>Args:     ascending (bool):         I noticed although in the DataFrame itself the Shapley values are at their right places, but the order of         elements are shuffled (probably during the calculation). This causes headache and is potentially dangerous         if you're using a list of indices as elements that you'll translate to np or normal lists within your game.         so assuming the elements were in ascending order like np.arange or range, this will save you from the pain.</p> <pre><code>shapley_table (pd.DataFrame):\n    Unsorted shapley table.\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>sorted shapley table.</p> Source code in <code>msapy/utils.py</code> <pre><code>@typechecked\ndef sorter(shapley_table: pd.DataFrame, ascending: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Sorts the elements based on their average shapley values or in ascending order by calling:\n        `df.sort_index(axis=1)`\n    Args:\n        ascending (bool):\n            I noticed although in the DataFrame itself the Shapley values are at their right places, but the order of\n            elements are shuffled (probably during the calculation). This causes headache and is potentially dangerous\n            if you're using a list of indices as elements that you'll translate to np or normal lists within your game.\n            so assuming the elements were in ascending order like np.arange or range, this will save you from the pain.\n\n        shapley_table (pd.DataFrame):\n            Unsorted shapley table.\n\n    Returns:\n         (pd.DataFrame): sorted shapley table.\n    \"\"\"\n    if ascending:\n        return shapley_table.sort_index(axis=1)\n    else:\n        return shapley_table.reindex(shapley_table.mean().sort_values().index, axis=1)\n</code></pre>"},{"location":"examples/MSA%20TimeSeries.html","title":"MSA TimeSeries","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># Imports\nimport matplotlib.pyplot as plt\nimport numpy as np\n# ---------\nfrom msapy import msa\n# ---------\nfrom itertools import product\n\nSEED = 42\nRNG = np.random.default_rng(SEED)\nmy_colors = ['#006685', '#3FA5C4', '#FFFFFF', '#E84653', '#BF003F']\nFIG_PATH = \"figures/timeseries/\"\n</pre> # Imports import matplotlib.pyplot as plt import numpy as np # --------- from msapy import msa # --------- from itertools import product  SEED = 42 RNG = np.random.default_rng(SEED) my_colors = ['#006685', '#3FA5C4', '#FFFFFF', '#E84653', '#BF003F'] FIG_PATH = \"figures/timeseries/\" <pre>/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n</pre> In\u00a0[3]: Copied! <pre>sampling_rate = 200\nsampling_interval = 1/sampling_rate\ntimestamps = np.arange(0, 1, sampling_interval)\n\nfrequencies = np.arange(1, 10, 1.5)\namplitudes = np.arange(0.2, 2, 0.4)\n\namp_freq_pairs = np.array(list(map(list, product(amplitudes, frequencies))))\n</pre> sampling_rate = 200 sampling_interval = 1/sampling_rate timestamps = np.arange(0, 1, sampling_interval)  frequencies = np.arange(1, 10, 1.5) amplitudes = np.arange(0.2, 2, 0.4)  amp_freq_pairs = np.array(list(map(list, product(amplitudes, frequencies)))) In\u00a0[4]: Copied! <pre>def generate_wave_data(amp_freq_pairs, timestamps, sampling_rate):\n    frequencies = amp_freq_pairs[:, 1, None]\n    amplitudes = amp_freq_pairs[:, 0, None]\n    timestamps = np.broadcast_to(\n        timestamps, (amplitudes.shape[0], sampling_rate))\n    data = np.sin(2 * np.pi * timestamps * frequencies) * amplitudes\n    return data\n</pre> def generate_wave_data(amp_freq_pairs, timestamps, sampling_rate):     frequencies = amp_freq_pairs[:, 1, None]     amplitudes = amp_freq_pairs[:, 0, None]     timestamps = np.broadcast_to(         timestamps, (amplitudes.shape[0], sampling_rate))     data = np.sin(2 * np.pi * timestamps * frequencies) * amplitudes     return data In\u00a0[5]: Copied! <pre>data = generate_wave_data(amp_freq_pairs, timestamps, sampling_rate)\nelements = list(range(len(data)))\n</pre> data = generate_wave_data(amp_freq_pairs, timestamps, sampling_rate) elements = list(range(len(data))) In\u00a0[6]: Copied! <pre>plt.figure(dpi=200)\nplt.plot(data[5], label=\"node 5\",c=\"#EAAC8B\",lw=2,alpha=0.9)\nplt.plot(data[10], label=\"node 10\",c=\"#E56B6F\",lw=2,alpha=0.9)\nplt.plot(data[25], label=\"node 25\",c=\"#B56576\",lw=2,alpha=0.9)\nplt.title(\"Example Activities\")\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Amplitude\")\n#plt.legend()\nplt.savefig(f\"{FIG_PATH}example_activities.pdf\",dpi=300,bbox_inches='tight')\n</pre> plt.figure(dpi=200) plt.plot(data[5], label=\"node 5\",c=\"#EAAC8B\",lw=2,alpha=0.9) plt.plot(data[10], label=\"node 10\",c=\"#E56B6F\",lw=2,alpha=0.9) plt.plot(data[25], label=\"node 25\",c=\"#B56576\",lw=2,alpha=0.9) plt.title(\"Example Activities\") plt.xlabel(\"Time steps\") plt.ylabel(\"Amplitude\") #plt.legend() plt.savefig(f\"{FIG_PATH}example_activities.pdf\",dpi=300,bbox_inches='tight')  <p>For our first example, let's assume that the total activity is a linear function of the activity of each node. Therefore, let's say that the total activity of the network is the sum of activity of each node. And when we lesion some nodes, we remove the sum of their activites from the total sum.</p> In\u00a0[7]: Copied! <pre>def linear_score_function(complements):\n    return data.sum(0) - data[complements, :].sum(0)\n</pre> def linear_score_function(complements):     return data.sum(0) - data[complements, :].sum(0) In\u00a0[8]: Copied! <pre>plt.figure(dpi=200)\nplt.plot(linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9)\nplt.title(\"Combined Activity\")\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Amplitude\")\nplt.savefig(f\"{FIG_PATH}combined_activity.pdf\",dpi=300,bbox_inches='tight');\n</pre> plt.figure(dpi=200) plt.plot(linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9) plt.title(\"Combined Activity\") plt.xlabel(\"Time steps\") plt.ylabel(\"Amplitude\") plt.savefig(f\"{FIG_PATH}combined_activity.pdf\",dpi=300,bbox_inches='tight');  In\u00a0[9]: Copied! <pre>shapley_table = msa.interface(\n    elements=elements,\n    n_permutations=1_000,\n    objective_function=linear_score_function,\n    n_parallel_games=1, #parallelized over all CPU cores\n    save_permutations=True,\n    rng=RNG)\n</pre> shapley_table = msa.interface(     elements=elements,     n_permutations=1_000,     objective_function=linear_score_function,     n_parallel_games=1, #parallelized over all CPU cores     save_permutations=True,     rng=RNG)        0.50% [5/1000 00:00&lt;00:06]             10.00% [3/30 00:00&lt;00:00]      <p>After running the MSA algorithm for 5,000 permutations and 120658 trials, let's analyse the contributions of each node at each timestamp. The Shapley Table we got after running the MSA algorithm is a MultiIndex Pandas Dataframe. It has the contribution of each node for every permutation and for every timestamp.</p> In\u00a0[10]: Copied! <pre>shapley_table\n</pre> shapley_table Out[10]: 0 1 2 3 4 5 6 7 8 9 ... 20 21 22 23 24 25 26 27 28 29 ND 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1 0.006282 0.015692 0.025067 0.034386 0.043629 0.052775 0.018846 0.047075 0.075200 0.103157 ... 0.175467 0.240701 0.305401 0.369422 0.056539 0.141226 0.225600 0.309472 0.392658 0.474971 2 0.012558 0.031287 0.049738 0.067748 0.085156 0.101808 0.037674 0.093861 0.149214 0.203243 ... 0.348166 0.474233 0.596091 0.712658 0.113023 0.281582 0.447642 0.609728 0.766403 0.916275 3 0.018822 0.046689 0.073625 0.099092 0.122581 0.143625 0.056465 0.140067 0.220875 0.297275 ... 0.515374 0.693642 0.858070 1.005377 0.169395 0.420202 0.662624 0.891826 1.103233 1.292627 4 0.025067 0.061803 0.096351 0.127485 0.154103 0.175261 0.075200 0.185410 0.289052 0.382454 ... 0.674455 0.892394 1.078719 1.226829 0.225600 0.556231 0.867157 1.147363 1.386924 1.577352 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 999 195 -0.031287 0.076537 -0.117557 0.152081 -0.178201 0.194474 -0.093861 0.229610 -0.352671 0.456244 ... -0.822899 1.064568 -1.247409 1.361318 -0.281582 0.688830 -1.058013 1.368731 -1.603812 1.750266 196 -0.025067 0.061803 -0.096351 0.127485 -0.154103 0.175261 -0.075200 0.185410 -0.289052 0.382454 ... -0.674455 0.892394 -1.078719 1.226829 -0.225600 0.556231 -0.867157 1.147363 -1.386924 1.577352 197 -0.018822 0.046689 -0.073625 0.099092 -0.122581 0.143625 -0.056465 0.140067 -0.220875 0.297275 ... -0.515374 0.693642 -0.858070 1.005377 -0.169395 0.420202 -0.662624 0.891826 -1.103233 1.292627 198 -0.012558 0.031287 -0.049738 0.067748 -0.085156 0.101808 -0.037674 0.093861 -0.149214 0.203243 ... -0.348166 0.474233 -0.596091 0.712658 -0.113023 0.281582 -0.447642 0.609728 -0.766403 0.916275 199 -0.006282 0.015692 -0.025067 0.034386 -0.043629 0.052775 -0.018846 0.047075 -0.075200 0.103157 ... -0.175467 0.240701 -0.305401 0.369422 -0.056539 0.141226 -0.225600 0.309472 -0.392658 0.474971 <p>200000 rows \u00d7 30 columns</p> <p>We take the average of all permutations to calculate the shapley values for each node for each timestamp which we term as shapley modes</p> In\u00a0[11]: Copied! <pre>shapley_table.shapley_modes\n</pre> shapley_table.shapley_modes Out[11]: 0 1 2 3 4 5 6 7 8 9 ... 20 21 22 23 24 25 26 27 28 29 ND 0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1 0.006282 0.015692 0.025067 0.034386 0.043629 0.052775 0.018846 0.047075 0.075200 0.103157 ... 0.175467 0.240701 0.305401 0.369422 0.056539 0.141226 0.225600 0.309472 0.392658 0.474971 2 0.012558 0.031287 0.049738 0.067748 0.085156 0.101808 0.037674 0.093861 0.149214 0.203243 ... 0.348166 0.474233 0.596091 0.712658 0.113023 0.281582 0.447642 0.609728 0.766403 0.916275 3 0.018822 0.046689 0.073625 0.099092 0.122581 0.143625 0.056465 0.140067 0.220875 0.297275 ... 0.515374 0.693642 0.858070 1.005377 0.169395 0.420202 0.662624 0.891826 1.103233 1.292627 4 0.025067 0.061803 0.096351 0.127485 0.154103 0.175261 0.075200 0.185410 0.289052 0.382454 ... 0.674455 0.892394 1.078719 1.226829 0.225600 0.556231 0.867157 1.147363 1.386924 1.577352 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 195 -0.031287 0.076537 -0.117557 0.152081 -0.178201 0.194474 -0.093861 0.229610 -0.352671 0.456244 ... -0.822899 1.064568 -1.247409 1.361318 -0.281582 0.688830 -1.058013 1.368731 -1.603812 1.750266 196 -0.025067 0.061803 -0.096351 0.127485 -0.154103 0.175261 -0.075200 0.185410 -0.289052 0.382454 ... -0.674455 0.892394 -1.078719 1.226829 -0.225600 0.556231 -0.867157 1.147363 -1.386924 1.577352 197 -0.018822 0.046689 -0.073625 0.099092 -0.122581 0.143625 -0.056465 0.140067 -0.220875 0.297275 ... -0.515374 0.693642 -0.858070 1.005377 -0.169395 0.420202 -0.662624 0.891826 -1.103233 1.292627 198 -0.012558 0.031287 -0.049738 0.067748 -0.085156 0.101808 -0.037674 0.093861 -0.149214 0.203243 ... -0.348166 0.474233 -0.596091 0.712658 -0.113023 0.281582 -0.447642 0.609728 -0.766403 0.916275 199 -0.006282 0.015692 -0.025067 0.034386 -0.043629 0.052775 -0.018846 0.047075 -0.075200 0.103157 ... -0.175467 0.240701 -0.305401 0.369422 -0.056539 0.141226 -0.225600 0.309472 -0.392658 0.474971 <p>200 rows \u00d7 30 columns</p> <p>Now we plot the contrbution we got after running MSA vs the activity of the node</p> In\u00a0[12]: Copied! <pre>plt.figure(dpi=200)\nplt.plot(data[5], label=\"node 5\",c=\"#EAAC8B\",lw=2,alpha=0.9)\nplt.plot(data[10], label=\"node 10\",c=\"#E56B6F\",lw=2,alpha=0.9)\nplt.plot(data[25], label=\"node 25\",c=\"#B56576\",lw=2,alpha=0.9)\n\nplt.plot(shapley_table.shapley_modes[5], label=\"Contribution #5\",c=\"#EAAC8B\",lw=4.5,alpha=0.4)\nplt.plot(shapley_table.shapley_modes[10], label=\"Contribution #10\",c=\"#E56B6F\",lw=4.5,alpha=0.4)\nplt.plot(shapley_table.shapley_modes[25], label=\"Contribution #25\",c=\"#B56576\",lw=4.5,alpha=0.4)\nplt.title(\"Activity vs Contribution\")\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Amplitude\")\nplt.savefig(f\"{FIG_PATH}activity_vs_contribution.pdf\",dpi=300,bbox_inches='tight')\n</pre> plt.figure(dpi=200) plt.plot(data[5], label=\"node 5\",c=\"#EAAC8B\",lw=2,alpha=0.9) plt.plot(data[10], label=\"node 10\",c=\"#E56B6F\",lw=2,alpha=0.9) plt.plot(data[25], label=\"node 25\",c=\"#B56576\",lw=2,alpha=0.9)  plt.plot(shapley_table.shapley_modes[5], label=\"Contribution #5\",c=\"#EAAC8B\",lw=4.5,alpha=0.4) plt.plot(shapley_table.shapley_modes[10], label=\"Contribution #10\",c=\"#E56B6F\",lw=4.5,alpha=0.4) plt.plot(shapley_table.shapley_modes[25], label=\"Contribution #25\",c=\"#B56576\",lw=4.5,alpha=0.4) plt.title(\"Activity vs Contribution\") plt.xlabel(\"Time steps\") plt.ylabel(\"Amplitude\") plt.savefig(f\"{FIG_PATH}activity_vs_contribution.pdf\",dpi=300,bbox_inches='tight') <p>The reason why we only see one wave is because both he activity of that node and the shapley value of that node is exactly the same for each timestamp and that's why they're overlapping in the plot. This is because the total activity of the nodes is a linear function of the activities of individual node. We can confirm that both the activities and shapley values are exactly the same by running the following code</p> In\u00a0[13]: Copied! <pre>plt.figure(dpi=200)\nplt.plot(linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9)\nplt.plot(shapley_table.shapley_modes.values.sum(1),c=my_colors[0],lw=4.5,alpha=0.4)\nplt.title(\"Reconstructing the Combined Activities\")\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Amplitude\")\nplt.savefig(f\"{FIG_PATH}reconstructing_easy.pdf\",dpi=300,bbox_inches='tight')\n</pre> plt.figure(dpi=200) plt.plot(linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9) plt.plot(shapley_table.shapley_modes.values.sum(1),c=my_colors[0],lw=4.5,alpha=0.4) plt.title(\"Reconstructing the Combined Activities\") plt.xlabel(\"Time steps\") plt.ylabel(\"Amplitude\") plt.savefig(f\"{FIG_PATH}reconstructing_easy.pdf\",dpi=300,bbox_inches='tight') In\u00a0[14]: Copied! <pre>np.allclose(shapley_table.shapley_modes, data.T)\n</pre> np.allclose(shapley_table.shapley_modes, data.T) Out[14]: <pre>True</pre> <p>Okay, this is cool. But what will happen when the total activity of the network is not a linear function of the activities of individual neuron. To check this, we add a non-linearity of a <code>TanH</code> function and run the MSA again</p> In\u00a0[15]: Copied! <pre>non_linear_score_function = lambda x: np.tanh(linear_score_function(x))\n</pre> non_linear_score_function = lambda x: np.tanh(linear_score_function(x)) In\u00a0[16]: Copied! <pre>plt.figure(dpi=200)\nplt.plot(non_linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9)\nplt.title(\"tanh(Combined Activity)\")\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Amplitude\")\nplt.savefig(f\"{FIG_PATH}combined_activity_tanh.pdf\",dpi=300,bbox_inches='tight');\n</pre> plt.figure(dpi=200) plt.plot(non_linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9) plt.title(\"tanh(Combined Activity)\") plt.xlabel(\"Time steps\") plt.ylabel(\"Amplitude\") plt.savefig(f\"{FIG_PATH}combined_activity_tanh.pdf\",dpi=300,bbox_inches='tight'); In\u00a0[17]: Copied! <pre>shapley_table = msa.interface(\n    elements=elements,\n    n_permutations=10_000,\n    objective_function=non_linear_score_function,\n    n_parallel_games=-1, #parallelized over all CPU cores\n    save_permutations=True,\n    rng=RNG)\n</pre> shapley_table = msa.interface(     elements=elements,     n_permutations=10_000,     objective_function=non_linear_score_function,     n_parallel_games=-1, #parallelized over all CPU cores     save_permutations=True,     rng=RNG) In\u00a0[18]: Copied! <pre># node = 5\n# plt.plot(timestamps, shapley_values[node], label = f\"Shapley Value for node {node}\")\n# plt.plot(timestamps, data[node], label = f\"activity of node {node}\")\n# plt.legend();\n\nplt.figure(dpi=200)\n\nplt.plot(data[25], label=\"Activity\",c=\"#B56576\",lw=2,alpha=0.9)\n\nplt.plot(shapley_table.shapley_modes[25], label=\"Contribution\",c=\"k\",lw=2,alpha=0.9)\nplt.title(\"Activity vs Contribution\")\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Amplitude\")\nplt.legend(loc=\"lower left\")\nplt.savefig(f\"{FIG_PATH}activity_vs_contribution_tanh.pdf\",dpi=300,bbox_inches='tight')\n</pre> # node = 5 # plt.plot(timestamps, shapley_values[node], label = f\"Shapley Value for node {node}\") # plt.plot(timestamps, data[node], label = f\"activity of node {node}\") # plt.legend();  plt.figure(dpi=200)  plt.plot(data[25], label=\"Activity\",c=\"#B56576\",lw=2,alpha=0.9)  plt.plot(shapley_table.shapley_modes[25], label=\"Contribution\",c=\"k\",lw=2,alpha=0.9) plt.title(\"Activity vs Contribution\") plt.xlabel(\"Time steps\") plt.ylabel(\"Amplitude\") plt.legend(loc=\"lower left\") plt.savefig(f\"{FIG_PATH}activity_vs_contribution_tanh.pdf\",dpi=300,bbox_inches='tight') <p>As we can see, in case of a non-linear function, the activities of the nodes is not exactly equal to their shapley values. But since the non-linearity was monotonic, we can observe that the activities and the shapley values of the nodes are very correlated to each other. Now, let's try to plot the sum of shapley values of each node and the total activity of the network</p> In\u00a0[20]: Copied! <pre>plt.figure(dpi=200)\nplt.plot(non_linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9)\nplt.plot(shapley_table.shapley_modes.values.sum(1),c=my_colors[0],lw=4.5,alpha=0.4)\nplt.title(\"Reconstructing the Combined Activities\")\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Amplitude\")\nplt.savefig(f\"{FIG_PATH}reconstructing_hard.pdf\",dpi=300,bbox_inches='tight')\n</pre> plt.figure(dpi=200) plt.plot(non_linear_score_function([]),c=my_colors[0],lw=2,alpha=0.9) plt.plot(shapley_table.shapley_modes.values.sum(1),c=my_colors[0],lw=4.5,alpha=0.4) plt.title(\"Reconstructing the Combined Activities\") plt.xlabel(\"Time steps\") plt.ylabel(\"Amplitude\") plt.savefig(f\"{FIG_PATH}reconstructing_hard.pdf\",dpi=300,bbox_inches='tight') <p>The Combined Activity and the Sum of Shapley Values for all neurons are exactly the same and we can see them perfectly overlapping with each other. This is how we know that the MSA algorithm is working perfectly</p>"},{"location":"examples/MSA%20TimeSeries.html#msa-on-time-series-data","title":"MSA on Time Series Data\u00b6","text":"<p>This is an example of how we can perform \"Multiperturbation Shapley value Analysis\" on a time series dataset to analyse the contribution of each node at each timestamp in some network.</p>"},{"location":"examples/MSA%20TimeSeries.html#data-simulation","title":"Data Simulation\u00b6","text":"<p>We use sine waves to simulate the activity of each node. The activity of each node is wave of a certain frequency and amplitute</p>"},{"location":"examples/MSA%20TimeSeries.html#msa","title":"MSA\u00b6","text":""},{"location":"examples/Shapley%20GAN.html","title":"Work In Progress","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># Imports\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nimport seaborn as sns\n# ---------\nfrom msapy import msa, utils as ut, plottings as pl\n# ---------\nfrom functools import partial\nfrom typing import Union, Optional, List\nfrom itertools import product\nfrom collections import defaultdict\n\nfrom IPython.display import HTML\n\nCM = 1 / 2.54\nSEED = 42\nRNG = np.random.default_rng(SEED)\nFIGPATH = \"figures/ShpleyGAN/\"\n</pre> # Imports import matplotlib.pyplot as plt import matplotlib.animation as animation import numpy as np import seaborn as sns # --------- from msapy import msa, utils as ut, plottings as pl # --------- from functools import partial from typing import Union, Optional, List from itertools import product from collections import defaultdict  from IPython.display import HTML  CM = 1 / 2.54 SEED = 42 RNG = np.random.default_rng(SEED) FIGPATH = \"figures/ShpleyGAN/\" In\u00a0[3]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\nimport torchvision.datasets as dset\nimport torchvision.utils as vutils\n\n\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n</pre> import torch import torch.nn as nn import torch.optim as optim import torchvision.transforms as T import torchvision.datasets as dset import torchvision.utils as vutils   from torch.utils.data import DataLoader, TensorDataset from torchvision import datasets from sklearn.metrics import accuracy_score, confusion_matrix In\u00a0[4]: Copied! <pre># Root directory for dataset\ndataroot = \"~/Desktop/celebA\"\n\n# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 128\n\n# Spatial size of training images. All images will be resized to this\n#   size using a transformer.\nimage_size = 128\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 10\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n</pre> # Root directory for dataset dataroot = \"~/Desktop/celebA\"  # Number of workers for dataloader workers = 2  # Batch size during training batch_size = 128  # Spatial size of training images. All images will be resized to this #   size using a transformer. image_size = 128  # Number of channels in the training images. For color images this is 3 nc = 3  # Size of z latent vector (i.e. size of generator input) nz = 100  # Size of feature maps in generator ngf = 64  # Size of feature maps in discriminator ndf = 64  # Number of training epochs num_epochs = 10  # Learning rate for optimizers lr = 0.0002  # Beta1 hyperparam for Adam optimizers beta1 = 0.5  # Number of GPUs available. Use 0 for CPU mode. ngpu = 1 In\u00a0[5]: Copied! <pre>dataset = dset.ImageFolder(root=dataroot,\n                           transform=T.Compose([\n                               T.Resize(image_size),\n                               T.CenterCrop(image_size),\n                               T.ToTensor(),\n                               T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu &gt; 0) else \"cpu\")\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n</pre> dataset = dset.ImageFolder(root=dataroot,                            transform=T.Compose([                                T.Resize(image_size),                                T.CenterCrop(image_size),                                T.ToTensor(),                                T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),                            ])) # Create the dataloader dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,                                          shuffle=True, num_workers=workers)  # Decide which device we want to run on device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu &gt; 0) else \"cpu\")  # Plot some training images real_batch = next(iter(dataloader)) plt.figure(figsize=(8,8)) plt.axis(\"off\") plt.title(\"Training Images\") plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) Out[5]: <pre>&lt;matplotlib.image.AxesImage at 0x7f457808dfa0&gt;</pre> In\u00a0[6]: Copied! <pre># custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n</pre> # custom weights initialization called on netG and netD def weights_init(m):     classname = m.__class__.__name__     if classname.find('Conv') != -1:         nn.init.normal_(m.weight.data, 0.0, 0.02)     elif classname.find('BatchNorm') != -1:         nn.init.normal_(m.weight.data, 1.0, 0.02)         nn.init.constant_(m.bias.data, 0) In\u00a0[7]: Copied! <pre># Generator Code\nclass ConvTranspose2dLayer(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size = 4, stride = 1, padding = 0, output_padding = 0, groups: int = 1, bias: bool = True):\n        super(ConvTranspose2dLayer, self).__init__()\n        self.seq = nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n    \n    def forward(self, input):\n        return self.seq(input)\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n\n        first_layer = ConvTranspose2dLayer(nz, ngf * 16, 4, 1, 0, bias=False)\n        middle_layers = []\n        for i in range(4, 0, -1):\n            middle_layers.append(ConvTranspose2dLayer(ngf * 2**(i), ngf * 2**(i-1), 4, 2, 1, bias=False))\n        last_layer = nn.Sequential(nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False), nn.Tanh())\n        self.model = nn.Sequential(first_layer, *middle_layers, last_layer)\n\n    def forward(self, input, lesion_dict = None):\n        lesion_dict = lesion_dict if lesion_dict else defaultdict(list)\n        x = input\n        for i, layer in enumerate(self.model):\n            x = layer(x)\n            if i in lesion_dict:\n                x[:, lesion_dict[i]] = 0\n        \n        return x\n\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n</pre> # Generator Code class ConvTranspose2dLayer(nn.Module):     def __init__(self, in_channels: int, out_channels: int, kernel_size = 4, stride = 1, padding = 0, output_padding = 0, groups: int = 1, bias: bool = True):         super(ConvTranspose2dLayer, self).__init__()         self.seq = nn.Sequential(             nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias),             nn.BatchNorm2d(out_channels),             nn.ReLU(True),         )          def forward(self, input):         return self.seq(input)  class Generator(nn.Module):     def __init__(self, ngpu):         super(Generator, self).__init__()         self.ngpu = ngpu          first_layer = ConvTranspose2dLayer(nz, ngf * 16, 4, 1, 0, bias=False)         middle_layers = []         for i in range(4, 0, -1):             middle_layers.append(ConvTranspose2dLayer(ngf * 2**(i), ngf * 2**(i-1), 4, 2, 1, bias=False))         last_layer = nn.Sequential(nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False), nn.Tanh())         self.model = nn.Sequential(first_layer, *middle_layers, last_layer)      def forward(self, input, lesion_dict = None):         lesion_dict = lesion_dict if lesion_dict else defaultdict(list)         x = input         for i, layer in enumerate(self.model):             x = layer(x)             if i in lesion_dict:                 x[:, lesion_dict[i]] = 0                  return x  class Discriminator(nn.Module):     def __init__(self, ngpu):         super(Discriminator, self).__init__()         self.ngpu = ngpu         self.main = nn.Sequential(             # input is (nc) x 64 x 64             nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),             nn.LeakyReLU(0.2, inplace=True),                          nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf) x 32 x 32             nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf * 2),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf*2) x 16 x 16             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf * 4),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf*4) x 8 x 8             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf * 8),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf*8) x 4 x 4             nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),             nn.Sigmoid()         )      def forward(self, input):         return self.main(input)  In\u00a0[8]: Copied! <pre>netG = Generator(ngpu).to(device)\nnetD = Discriminator(ngpu).to(device)\n</pre> netG = Generator(ngpu).to(device) netD = Discriminator(ngpu).to(device) In\u00a0[9]: Copied! <pre># Initialize BCELoss function\ncriterion = nn.BCELoss()\n\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(32, nz, 1, 1, device=device)\n\n# Establish convention for real and fake labels during training\nreal_label = 1.\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n</pre> # Initialize BCELoss function criterion = nn.BCELoss()  # Create batch of latent vectors that we will use to visualize #  the progression of the generator fixed_noise = torch.randn(32, nz, 1, 1, device=device)  # Establish convention for real and fake labels during training real_label = 1. fake_label = 0.  # Setup Adam optimizers for both G and D optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)) In\u00a0[10]: Copied! <pre># Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Compute error of D as sum over the fake and the real batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1\n</pre> # Training Loop  # Lists to keep track of progress img_list = [] G_losses = [] D_losses = [] iters = 0  print(\"Starting Training Loop...\") # For each epoch for epoch in range(num_epochs):     # For each batch in the dataloader     for i, data in enumerate(dataloader, 0):          ############################         # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))         ###########################         ## Train with all-real batch         netD.zero_grad()         # Format batch         real_cpu = data[0].to(device)         b_size = real_cpu.size(0)         label = torch.full((b_size,), real_label, dtype=torch.float, device=device)         # Forward pass real batch through D         output = netD(real_cpu).view(-1)         # Calculate loss on all-real batch         errD_real = criterion(output, label)         # Calculate gradients for D in backward pass         errD_real.backward()         D_x = output.mean().item()          ## Train with all-fake batch         # Generate batch of latent vectors         noise = torch.randn(b_size, nz, 1, 1, device=device)         # Generate fake image batch with G         fake = netG(noise)         label.fill_(fake_label)         # Classify all fake batch with D         output = netD(fake.detach()).view(-1)         # Calculate D's loss on the all-fake batch         errD_fake = criterion(output, label)         # Calculate the gradients for this batch, accumulated (summed) with previous gradients         errD_fake.backward()         D_G_z1 = output.mean().item()         # Compute error of D as sum over the fake and the real batches         errD = errD_real + errD_fake         # Update D         optimizerD.step()          ############################         # (2) Update G network: maximize log(D(G(z)))         ###########################         netG.zero_grad()         label.fill_(real_label)  # fake labels are real for generator cost         # Since we just updated D, perform another forward pass of all-fake batch through D         output = netD(fake).view(-1)         # Calculate G's loss based on this output         errG = criterion(output, label)         # Calculate gradients for G         errG.backward()         D_G_z2 = output.mean().item()         # Update G         optimizerG.step()          # Output training stats         if i % 50 == 0:             print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'                   % (epoch, num_epochs, i, len(dataloader),                      errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))          # Save Losses for plotting later         G_losses.append(errG.item())         D_losses.append(errD.item())          # Check how the generator is doing by saving G's output on fixed_noise         if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):             with torch.no_grad():                 fake = netG(fixed_noise).detach().cpu()             img_list.append(vutils.make_grid(fake, padding=2, normalize=True))          iters += 1 <pre>Starting Training Loop...\n[0/10][0/1583]\tLoss_D: 1.4624\tLoss_G: 2.7180\tD(x): 0.5107\tD(G(z)): 0.5332 / 0.0697\n[0/10][50/1583]\tLoss_D: 0.1972\tLoss_G: 5.6375\tD(x): 0.8873\tD(G(z)): 0.0409 / 0.0066\n[0/10][100/1583]\tLoss_D: 0.0549\tLoss_G: 5.0424\tD(x): 0.9628\tD(G(z)): 0.0123 / 0.0080\n[0/10][150/1583]\tLoss_D: 1.3439\tLoss_G: 2.6715\tD(x): 0.7229\tD(G(z)): 0.6022 / 0.0815\n[0/10][200/1583]\tLoss_D: 1.2696\tLoss_G: 3.0152\tD(x): 0.8828\tD(G(z)): 0.6689 / 0.0527\n[0/10][250/1583]\tLoss_D: 0.8095\tLoss_G: 2.6289\tD(x): 0.7701\tD(G(z)): 0.4055 / 0.0840\n[0/10][300/1583]\tLoss_D: 1.0757\tLoss_G: 1.9925\tD(x): 0.6634\tD(G(z)): 0.4536 / 0.1592\n[0/10][350/1583]\tLoss_D: 0.7433\tLoss_G: 1.3303\tD(x): 0.6426\tD(G(z)): 0.2354 / 0.2815\n[0/10][400/1583]\tLoss_D: 1.0007\tLoss_G: 1.4088\tD(x): 0.6214\tD(G(z)): 0.3786 / 0.2641\n[0/10][450/1583]\tLoss_D: 1.2477\tLoss_G: 1.8634\tD(x): 0.7313\tD(G(z)): 0.5867 / 0.1687\n[0/10][500/1583]\tLoss_D: 0.8448\tLoss_G: 2.0525\tD(x): 0.7214\tD(G(z)): 0.3760 / 0.1486\n[0/10][550/1583]\tLoss_D: 1.4869\tLoss_G: 1.3303\tD(x): 0.5822\tD(G(z)): 0.5928 / 0.2792\n[0/10][600/1583]\tLoss_D: 1.0737\tLoss_G: 1.6774\tD(x): 0.5550\tD(G(z)): 0.3442 / 0.2081\n[0/10][650/1583]\tLoss_D: 0.9202\tLoss_G: 1.7250\tD(x): 0.6598\tD(G(z)): 0.3754 / 0.1975\n[0/10][700/1583]\tLoss_D: 1.2879\tLoss_G: 1.4974\tD(x): 0.3792\tD(G(z)): 0.2009 / 0.2505\n[0/10][750/1583]\tLoss_D: 0.9193\tLoss_G: 1.4820\tD(x): 0.5873\tD(G(z)): 0.2824 / 0.2451\n[0/10][800/1583]\tLoss_D: 1.3325\tLoss_G: 3.5556\tD(x): 0.7569\tD(G(z)): 0.6297 / 0.0370\n[0/10][850/1583]\tLoss_D: 1.1232\tLoss_G: 2.2912\tD(x): 0.7127\tD(G(z)): 0.5121 / 0.1180\n[0/10][900/1583]\tLoss_D: 1.0059\tLoss_G: 1.8699\tD(x): 0.6119\tD(G(z)): 0.3686 / 0.1709\n[0/10][950/1583]\tLoss_D: 1.1302\tLoss_G: 1.4309\tD(x): 0.5996\tD(G(z)): 0.4382 / 0.2611\n[0/10][1000/1583]\tLoss_D: 1.4429\tLoss_G: 3.8645\tD(x): 0.7573\tD(G(z)): 0.6587 / 0.0250\n[0/10][1050/1583]\tLoss_D: 0.9013\tLoss_G: 2.0680\tD(x): 0.6645\tD(G(z)): 0.3574 / 0.1395\n[0/10][1100/1583]\tLoss_D: 1.1543\tLoss_G: 2.3717\tD(x): 0.7772\tD(G(z)): 0.5739 / 0.1076\n[0/10][1150/1583]\tLoss_D: 2.1200\tLoss_G: 1.0341\tD(x): 0.1559\tD(G(z)): 0.0595 / 0.3794\n[0/10][1200/1583]\tLoss_D: 1.2080\tLoss_G: 1.1139\tD(x): 0.4724\tD(G(z)): 0.3244 / 0.3425\n[0/10][1250/1583]\tLoss_D: 1.2974\tLoss_G: 1.0462\tD(x): 0.4248\tD(G(z)): 0.2817 / 0.3718\n[0/10][1300/1583]\tLoss_D: 0.9998\tLoss_G: 1.9929\tD(x): 0.7559\tD(G(z)): 0.4953 / 0.1528\n[0/10][1350/1583]\tLoss_D: 1.1567\tLoss_G: 4.7218\tD(x): 0.8719\tD(G(z)): 0.6211 / 0.0108\n[0/10][1400/1583]\tLoss_D: 1.0325\tLoss_G: 1.8227\tD(x): 0.5953\tD(G(z)): 0.3781 / 0.1722\n[0/10][1450/1583]\tLoss_D: 1.3953\tLoss_G: 3.9441\tD(x): 0.8348\tD(G(z)): 0.6787 / 0.0291\n[0/10][1500/1583]\tLoss_D: 1.2802\tLoss_G: 1.0428\tD(x): 0.3722\tD(G(z)): 0.1800 / 0.3761\n[0/10][1550/1583]\tLoss_D: 1.1205\tLoss_G: 2.7306\tD(x): 0.8443\tD(G(z)): 0.5925 / 0.0734\n[1/10][0/1583]\tLoss_D: 0.7464\tLoss_G: 1.9306\tD(x): 0.6062\tD(G(z)): 0.1852 / 0.1715\n[1/10][50/1583]\tLoss_D: 1.9872\tLoss_G: 0.5773\tD(x): 0.1724\tD(G(z)): 0.0711 / 0.5770\n[1/10][100/1583]\tLoss_D: 0.9533\tLoss_G: 2.4943\tD(x): 0.6715\tD(G(z)): 0.4013 / 0.0940\n[1/10][150/1583]\tLoss_D: 1.1588\tLoss_G: 2.7016\tD(x): 0.7271\tD(G(z)): 0.5479 / 0.0741\n[1/10][200/1583]\tLoss_D: 1.0760\tLoss_G: 1.6180\tD(x): 0.4438\tD(G(z)): 0.1657 / 0.2152\n[1/10][250/1583]\tLoss_D: 0.7080\tLoss_G: 2.5390\tD(x): 0.7622\tD(G(z)): 0.3344 / 0.0880\n[1/10][300/1583]\tLoss_D: 1.0707\tLoss_G: 4.0185\tD(x): 0.8743\tD(G(z)): 0.5900 / 0.0214\n[1/10][350/1583]\tLoss_D: 0.9802\tLoss_G: 1.1715\tD(x): 0.4252\tD(G(z)): 0.0606 / 0.3411\n[1/10][400/1583]\tLoss_D: 0.5714\tLoss_G: 1.6487\tD(x): 0.6800\tD(G(z)): 0.1501 / 0.2093\n[1/10][450/1583]\tLoss_D: 1.6702\tLoss_G: 4.4839\tD(x): 0.9348\tD(G(z)): 0.7772 / 0.0133\n[1/10][500/1583]\tLoss_D: 0.8980\tLoss_G: 1.2171\tD(x): 0.4867\tD(G(z)): 0.1298 / 0.3243\n[1/10][550/1583]\tLoss_D: 3.1459\tLoss_G: 1.6532\tD(x): 0.0528\tD(G(z)): 0.0114 / 0.2161\n[1/10][600/1583]\tLoss_D: 1.1561\tLoss_G: 0.5261\tD(x): 0.3996\tD(G(z)): 0.1414 / 0.6028\n[1/10][650/1583]\tLoss_D: 0.6256\tLoss_G: 2.1039\tD(x): 0.6824\tD(G(z)): 0.1921 / 0.1386\n[1/10][700/1583]\tLoss_D: 0.9113\tLoss_G: 1.6695\tD(x): 0.5929\tD(G(z)): 0.2866 / 0.2027\n[1/10][750/1583]\tLoss_D: 0.8794\tLoss_G: 2.0048\tD(x): 0.4830\tD(G(z)): 0.0993 / 0.1575\n[1/10][800/1583]\tLoss_D: 0.3421\tLoss_G: 1.9588\tD(x): 0.8197\tD(G(z)): 0.1244 / 0.1545\n[1/10][850/1583]\tLoss_D: 2.4288\tLoss_G: 2.0714\tD(x): 0.1108\tD(G(z)): 0.0186 / 0.1436\n[1/10][900/1583]\tLoss_D: 0.4965\tLoss_G: 1.7525\tD(x): 0.7925\tD(G(z)): 0.2213 / 0.1868\n[1/10][950/1583]\tLoss_D: 0.7090\tLoss_G: 2.3948\tD(x): 0.6842\tD(G(z)): 0.2567 / 0.1000\n[1/10][1000/1583]\tLoss_D: 3.0662\tLoss_G: 3.6219\tD(x): 0.0557\tD(G(z)): 0.0052 / 0.0326\n[1/10][1050/1583]\tLoss_D: 1.7626\tLoss_G: 4.9307\tD(x): 0.8975\tD(G(z)): 0.7891 / 0.0086\n[1/10][1100/1583]\tLoss_D: 0.5424\tLoss_G: 1.9474\tD(x): 0.7133\tD(G(z)): 0.1718 / 0.1616\n[1/10][1150/1583]\tLoss_D: 1.7070\tLoss_G: 2.5992\tD(x): 0.9845\tD(G(z)): 0.7855 / 0.0875\n[1/10][1200/1583]\tLoss_D: 0.5788\tLoss_G: 3.1580\tD(x): 0.8638\tD(G(z)): 0.3384 / 0.0484\n[1/10][1250/1583]\tLoss_D: 0.2699\tLoss_G: 2.6347\tD(x): 0.8833\tD(G(z)): 0.1317 / 0.0787\n[1/10][1300/1583]\tLoss_D: 0.7216\tLoss_G: 1.9602\tD(x): 0.8451\tD(G(z)): 0.4080 / 0.1532\n[1/10][1350/1583]\tLoss_D: 0.2556\tLoss_G: 2.3703\tD(x): 0.8520\tD(G(z)): 0.0860 / 0.1051\n[1/10][1400/1583]\tLoss_D: 1.4434\tLoss_G: 5.3177\tD(x): 0.9705\tD(G(z)): 0.7375 / 0.0067\n[1/10][1450/1583]\tLoss_D: 2.8161\tLoss_G: 3.5339\tD(x): 0.0726\tD(G(z)): 0.0048 / 0.0336\n[1/10][1500/1583]\tLoss_D: 2.2796\tLoss_G: 7.2438\tD(x): 0.9925\tD(G(z)): 0.8825 / 0.0009\n[1/10][1550/1583]\tLoss_D: 0.1691\tLoss_G: 3.1219\tD(x): 0.9718\tD(G(z)): 0.1290 / 0.0498\n[2/10][0/1583]\tLoss_D: 0.3609\tLoss_G: 3.2974\tD(x): 0.9563\tD(G(z)): 0.2641 / 0.0430\n[2/10][50/1583]\tLoss_D: 0.2395\tLoss_G: 2.8062\tD(x): 0.9557\tD(G(z)): 0.1731 / 0.0672\n[2/10][100/1583]\tLoss_D: 0.1930\tLoss_G: 3.1359\tD(x): 0.9662\tD(G(z)): 0.1417 / 0.0503\n[2/10][150/1583]\tLoss_D: 1.0435\tLoss_G: 2.5664\tD(x): 0.8175\tD(G(z)): 0.5399 / 0.0898\n[2/10][200/1583]\tLoss_D: 1.1003\tLoss_G: 2.3807\tD(x): 0.4466\tD(G(z)): 0.2078 / 0.1015\n[2/10][250/1583]\tLoss_D: 0.6263\tLoss_G: 3.2964\tD(x): 0.5697\tD(G(z)): 0.0250 / 0.0430\n[2/10][300/1583]\tLoss_D: 0.4309\tLoss_G: 2.9201\tD(x): 0.9265\tD(G(z)): 0.2867 / 0.0623\n[2/10][350/1583]\tLoss_D: 0.4268\tLoss_G: 2.7601\tD(x): 0.6937\tD(G(z)): 0.0385 / 0.0734\n[2/10][400/1583]\tLoss_D: 0.7256\tLoss_G: 1.7288\tD(x): 0.6623\tD(G(z)): 0.2396 / 0.1999\n[2/10][450/1583]\tLoss_D: 0.5779\tLoss_G: 4.5685\tD(x): 0.9715\tD(G(z)): 0.4075 / 0.0126\n[2/10][500/1583]\tLoss_D: 0.5675\tLoss_G: 2.9153\tD(x): 0.9114\tD(G(z)): 0.3569 / 0.0623\n[2/10][550/1583]\tLoss_D: 0.0308\tLoss_G: 5.6352\tD(x): 0.9733\tD(G(z)): 0.0034 / 0.0042\n[2/10][600/1583]\tLoss_D: 0.0485\tLoss_G: 3.7003\tD(x): 0.9673\tD(G(z)): 0.0147 / 0.0297\n[2/10][650/1583]\tLoss_D: 0.2172\tLoss_G: 3.1599\tD(x): 0.9669\tD(G(z)): 0.1625 / 0.0492\n[2/10][700/1583]\tLoss_D: 0.0188\tLoss_G: 5.7828\tD(x): 0.9895\tD(G(z)): 0.0082 / 0.0042\n[2/10][750/1583]\tLoss_D: 0.0061\tLoss_G: 6.2972\tD(x): 0.9968\tD(G(z)): 0.0029 / 0.0027\n[2/10][800/1583]\tLoss_D: 0.0312\tLoss_G: 4.6475\tD(x): 0.9976\tD(G(z)): 0.0283 / 0.0123\n[2/10][850/1583]\tLoss_D: 0.0125\tLoss_G: 5.2163\tD(x): 0.9954\tD(G(z)): 0.0079 / 0.0066\n[2/10][900/1583]\tLoss_D: 0.0348\tLoss_G: 5.1966\tD(x): 0.9936\tD(G(z)): 0.0280 / 0.0058\n[2/10][950/1583]\tLoss_D: 0.0052\tLoss_G: 6.2336\tD(x): 0.9975\tD(G(z)): 0.0027 / 0.0020\n[2/10][1000/1583]\tLoss_D: 0.0044\tLoss_G: 6.3149\tD(x): 0.9977\tD(G(z)): 0.0021 / 0.0019\n[2/10][1050/1583]\tLoss_D: 0.0021\tLoss_G: 7.8804\tD(x): 0.9985\tD(G(z)): 0.0006 / 0.0006\n[2/10][1100/1583]\tLoss_D: 0.0027\tLoss_G: 9.3323\tD(x): 0.9974\tD(G(z)): 0.0001 / 0.0001\n[2/10][1150/1583]\tLoss_D: 0.0068\tLoss_G: 12.1606\tD(x): 0.9932\tD(G(z)): 0.0000 / 0.0000\n[2/10][1200/1583]\tLoss_D: 0.0054\tLoss_G: 9.0685\tD(x): 0.9948\tD(G(z)): 0.0001 / 0.0001\n[2/10][1250/1583]\tLoss_D: 0.0128\tLoss_G: 14.8489\tD(x): 0.9875\tD(G(z)): 0.0000 / 0.0000\n[2/10][1300/1583]\tLoss_D: 0.0009\tLoss_G: 12.6263\tD(x): 0.9991\tD(G(z)): 0.0000 / 0.0000\n[2/10][1350/1583]\tLoss_D: 0.0058\tLoss_G: 12.6843\tD(x): 0.9942\tD(G(z)): 0.0000 / 0.0000\n[2/10][1400/1583]\tLoss_D: 0.0045\tLoss_G: 6.3357\tD(x): 0.9982\tD(G(z)): 0.0028 / 0.0018\n[2/10][1450/1583]\tLoss_D: 0.0026\tLoss_G: 8.4308\tD(x): 0.9996\tD(G(z)): 0.0022 / 0.0003\n[2/10][1500/1583]\tLoss_D: 0.3646\tLoss_G: 6.6377\tD(x): 0.9785\tD(G(z)): 0.2774 / 0.0017\n[2/10][1550/1583]\tLoss_D: 0.0789\tLoss_G: 5.3022\tD(x): 0.9914\tD(G(z)): 0.0658 / 0.0069\n[3/10][0/1583]\tLoss_D: 0.0221\tLoss_G: 6.8401\tD(x): 0.9829\tD(G(z)): 0.0045 / 0.0012\n[3/10][50/1583]\tLoss_D: 1.4111\tLoss_G: 1.8582\tD(x): 0.5401\tD(G(z)): 0.0425 / 0.3321\n[3/10][100/1583]\tLoss_D: 0.1743\tLoss_G: 4.3603\tD(x): 0.9370\tD(G(z)): 0.0988 / 0.0168\n[3/10][150/1583]\tLoss_D: 0.0783\tLoss_G: 4.8226\tD(x): 0.9979\tD(G(z)): 0.0687 / 0.0148\n[3/10][200/1583]\tLoss_D: 0.2658\tLoss_G: 3.6669\tD(x): 0.8280\tD(G(z)): 0.0461 / 0.0286\n[3/10][250/1583]\tLoss_D: 0.1495\tLoss_G: 4.7028\tD(x): 0.9031\tD(G(z)): 0.0316 / 0.0148\n[3/10][300/1583]\tLoss_D: 0.0954\tLoss_G: 4.1437\tD(x): 0.9428\tD(G(z)): 0.0326 / 0.0202\n[3/10][350/1583]\tLoss_D: 0.2795\tLoss_G: 4.1889\tD(x): 0.8027\tD(G(z)): 0.0235 / 0.0233\n[3/10][400/1583]\tLoss_D: 0.5932\tLoss_G: 10.5014\tD(x): 0.9826\tD(G(z)): 0.4055 / 0.0000\n[3/10][450/1583]\tLoss_D: 0.1584\tLoss_G: 3.0915\tD(x): 0.9091\tD(G(z)): 0.0521 / 0.0596\n[3/10][500/1583]\tLoss_D: 0.1932\tLoss_G: 4.4340\tD(x): 0.9255\tD(G(z)): 0.1026 / 0.0158\n[3/10][550/1583]\tLoss_D: 0.0938\tLoss_G: 2.8780\tD(x): 0.9784\tD(G(z)): 0.0667 / 0.0809\n[3/10][600/1583]\tLoss_D: 0.3032\tLoss_G: 3.6359\tD(x): 0.7930\tD(G(z)): 0.0253 / 0.0372\n[3/10][650/1583]\tLoss_D: 0.2565\tLoss_G: 5.8886\tD(x): 0.9403\tD(G(z)): 0.1602 / 0.0039\n[3/10][700/1583]\tLoss_D: 0.0899\tLoss_G: 4.2851\tD(x): 0.9634\tD(G(z)): 0.0484 / 0.0211\n[3/10][750/1583]\tLoss_D: 0.3167\tLoss_G: 6.8599\tD(x): 0.9505\tD(G(z)): 0.2145 / 0.0016\n[3/10][800/1583]\tLoss_D: 0.3264\tLoss_G: 4.6900\tD(x): 0.7610\tD(G(z)): 0.0050 / 0.0130\n[3/10][850/1583]\tLoss_D: 0.1787\tLoss_G: 4.3583\tD(x): 0.9734\tD(G(z)): 0.1359 / 0.0177\n[3/10][900/1583]\tLoss_D: 0.1679\tLoss_G: 4.5079\tD(x): 0.9905\tD(G(z)): 0.1379 / 0.0174\n[3/10][950/1583]\tLoss_D: 1.2970\tLoss_G: 3.9278\tD(x): 0.3576\tD(G(z)): 0.0009 / 0.0338\n[3/10][1000/1583]\tLoss_D: 0.0557\tLoss_G: 4.3296\tD(x): 0.9810\tD(G(z)): 0.0352 / 0.0185\n[3/10][1050/1583]\tLoss_D: 0.1836\tLoss_G: 3.5692\tD(x): 0.9115\tD(G(z)): 0.0774 / 0.0421\n[3/10][1100/1583]\tLoss_D: 0.1421\tLoss_G: 3.7749\tD(x): 0.9962\tD(G(z)): 0.1172 / 0.0400\n[3/10][1150/1583]\tLoss_D: 0.2096\tLoss_G: 3.8123\tD(x): 0.8305\tD(G(z)): 0.0076 / 0.0351\n[3/10][1200/1583]\tLoss_D: 0.3573\tLoss_G: 2.5320\tD(x): 0.8226\tD(G(z)): 0.1168 / 0.1037\n[3/10][1250/1583]\tLoss_D: 0.2020\tLoss_G: 3.2137\tD(x): 0.9381\tD(G(z)): 0.1190 / 0.0540\n[3/10][1300/1583]\tLoss_D: 0.1817\tLoss_G: 4.6632\tD(x): 0.8632\tD(G(z)): 0.0133 / 0.0172\n[3/10][1350/1583]\tLoss_D: 0.3727\tLoss_G: 5.7776\tD(x): 0.7359\tD(G(z)): 0.0039 / 0.0044\n[3/10][1400/1583]\tLoss_D: 0.4800\tLoss_G: 6.4792\tD(x): 0.9697\tD(G(z)): 0.3246 / 0.0024\n[3/10][1450/1583]\tLoss_D: 0.0797\tLoss_G: 3.8753\tD(x): 0.9578\tD(G(z)): 0.0321 / 0.0302\n[3/10][1500/1583]\tLoss_D: 0.7538\tLoss_G: 8.2256\tD(x): 0.9884\tD(G(z)): 0.4787 / 0.0004\n[3/10][1550/1583]\tLoss_D: 0.3177\tLoss_G: 3.1371\tD(x): 0.9592\tD(G(z)): 0.2194 / 0.0636\n[4/10][0/1583]\tLoss_D: 0.2815\tLoss_G: 3.7628\tD(x): 0.8347\tD(G(z)): 0.0691 / 0.0361\n[4/10][50/1583]\tLoss_D: 3.9746\tLoss_G: 4.4333\tD(x): 0.0410\tD(G(z)): 0.0003 / 0.0357\n[4/10][100/1583]\tLoss_D: 0.4233\tLoss_G: 3.1120\tD(x): 0.7841\tD(G(z)): 0.1323 / 0.0596\n[4/10][150/1583]\tLoss_D: 0.4997\tLoss_G: 5.7728\tD(x): 0.9963\tD(G(z)): 0.3493 / 0.0055\n[4/10][200/1583]\tLoss_D: 0.2419\tLoss_G: 3.7129\tD(x): 0.8208\tD(G(z)): 0.0187 / 0.0435\n[4/10][250/1583]\tLoss_D: 0.0286\tLoss_G: 5.5852\tD(x): 0.9834\tD(G(z)): 0.0114 / 0.0067\n[4/10][300/1583]\tLoss_D: 0.1327\tLoss_G: 3.6687\tD(x): 0.9639\tD(G(z)): 0.0847 / 0.0374\n[4/10][350/1583]\tLoss_D: 0.1762\tLoss_G: 3.9439\tD(x): 0.8656\tD(G(z)): 0.0155 / 0.0359\n[4/10][400/1583]\tLoss_D: 0.1074\tLoss_G: 4.6106\tD(x): 0.9832\tD(G(z)): 0.0790 / 0.0181\n[4/10][450/1583]\tLoss_D: 0.1773\tLoss_G: 4.3583\tD(x): 0.9234\tD(G(z)): 0.0833 / 0.0205\n[4/10][500/1583]\tLoss_D: 1.2522\tLoss_G: 10.4561\tD(x): 0.9987\tD(G(z)): 0.6458 / 0.0001\n[4/10][550/1583]\tLoss_D: 0.0984\tLoss_G: 4.7881\tD(x): 0.9247\tD(G(z)): 0.0159 / 0.0119\n[4/10][600/1583]\tLoss_D: 0.0775\tLoss_G: 3.7564\tD(x): 0.9628\tD(G(z)): 0.0369 / 0.0347\n[4/10][650/1583]\tLoss_D: 0.3294\tLoss_G: 6.1482\tD(x): 0.9487\tD(G(z)): 0.2221 / 0.0035\n[4/10][700/1583]\tLoss_D: 0.8337\tLoss_G: 8.5673\tD(x): 0.9958\tD(G(z)): 0.5051 / 0.0003\n[4/10][750/1583]\tLoss_D: 0.1206\tLoss_G: 4.1042\tD(x): 0.9600\tD(G(z)): 0.0733 / 0.0229\n[4/10][800/1583]\tLoss_D: 0.1729\tLoss_G: 4.2715\tD(x): 0.9568\tD(G(z)): 0.1139 / 0.0201\n[4/10][850/1583]\tLoss_D: 0.0212\tLoss_G: 5.6445\tD(x): 0.9837\tD(G(z)): 0.0046 / 0.0058\n[4/10][900/1583]\tLoss_D: 1.4797\tLoss_G: 8.3064\tD(x): 0.9901\tD(G(z)): 0.7188 / 0.0007\n[4/10][950/1583]\tLoss_D: 0.4270\tLoss_G: 5.8900\tD(x): 0.9337\tD(G(z)): 0.2692 / 0.0043\n[4/10][1000/1583]\tLoss_D: 0.0634\tLoss_G: 1.8620\tD(x): 0.9775\tD(G(z)): 0.0383 / 0.2060\n[4/10][1050/1583]\tLoss_D: 0.5189\tLoss_G: 1.4340\tD(x): 0.7284\tD(G(z)): 0.1039 / 0.3372\n[4/10][1100/1583]\tLoss_D: 0.2002\tLoss_G: 4.1373\tD(x): 0.9169\tD(G(z)): 0.0953 / 0.0272\n[4/10][1150/1583]\tLoss_D: 0.1853\tLoss_G: 4.2457\tD(x): 0.8667\tD(G(z)): 0.0293 / 0.0202\n[4/10][1200/1583]\tLoss_D: 0.6401\tLoss_G: 7.9021\tD(x): 0.9437\tD(G(z)): 0.4086 / 0.0006\n[4/10][1250/1583]\tLoss_D: 0.8199\tLoss_G: 6.8932\tD(x): 0.9791\tD(G(z)): 0.4911 / 0.0018\n[4/10][1300/1583]\tLoss_D: 0.1220\tLoss_G: 6.7354\tD(x): 0.8933\tD(G(z)): 0.0019 / 0.0018\n[4/10][1350/1583]\tLoss_D: 0.0516\tLoss_G: 4.0087\tD(x): 0.9875\tD(G(z)): 0.0374 / 0.0253\n[4/10][1400/1583]\tLoss_D: 0.1439\tLoss_G: 3.2635\tD(x): 0.9798\tD(G(z)): 0.1097 / 0.0535\n[4/10][1450/1583]\tLoss_D: 0.0880\tLoss_G: 3.7822\tD(x): 0.9901\tD(G(z)): 0.0723 / 0.0332\n[4/10][1500/1583]\tLoss_D: 0.2492\tLoss_G: 3.7739\tD(x): 0.8148\tD(G(z)): 0.0246 / 0.0390\n[4/10][1550/1583]\tLoss_D: 0.2916\tLoss_G: 3.6760\tD(x): 0.9956\tD(G(z)): 0.2320 / 0.0332\n[5/10][0/1583]\tLoss_D: 0.2232\tLoss_G: 6.5431\tD(x): 0.9942\tD(G(z)): 0.1778 / 0.0021\n[5/10][50/1583]\tLoss_D: 0.2143\tLoss_G: 3.7932\tD(x): 0.9115\tD(G(z)): 0.1085 / 0.0290\n[5/10][100/1583]\tLoss_D: 0.0578\tLoss_G: 4.1956\tD(x): 0.9698\tD(G(z)): 0.0254 / 0.0241\n[5/10][150/1583]\tLoss_D: 0.0682\tLoss_G: 5.4652\tD(x): 0.9531\tD(G(z)): 0.0139 / 0.0067\n[5/10][200/1583]\tLoss_D: 0.4885\tLoss_G: 6.5586\tD(x): 0.9793\tD(G(z)): 0.3492 / 0.0020\n[5/10][250/1583]\tLoss_D: 0.0933\tLoss_G: 3.2060\tD(x): 0.9602\tD(G(z)): 0.0485 / 0.0614\n[5/10][300/1583]\tLoss_D: 0.1724\tLoss_G: 4.0449\tD(x): 0.9156\tD(G(z)): 0.0664 / 0.0347\n[5/10][350/1583]\tLoss_D: 0.0410\tLoss_G: 5.2099\tD(x): 0.9683\tD(G(z)): 0.0079 / 0.0109\n[5/10][400/1583]\tLoss_D: 0.1657\tLoss_G: 4.9061\tD(x): 0.9942\tD(G(z)): 0.1394 / 0.0102\n[5/10][450/1583]\tLoss_D: 0.1890\tLoss_G: 3.6535\tD(x): 0.8993\tD(G(z)): 0.0698 / 0.0376\n[5/10][500/1583]\tLoss_D: 0.1530\tLoss_G: 4.9710\tD(x): 0.8960\tD(G(z)): 0.0342 / 0.0110\n[5/10][550/1583]\tLoss_D: 0.2224\tLoss_G: 4.2772\tD(x): 0.8275\tD(G(z)): 0.0069 / 0.0208\n[5/10][600/1583]\tLoss_D: 0.1239\tLoss_G: 6.4034\tD(x): 0.8947\tD(G(z)): 0.0037 / 0.0030\n[5/10][650/1583]\tLoss_D: 0.0696\tLoss_G: 6.9327\tD(x): 0.9362\tD(G(z)): 0.0010 / 0.0014\n[5/10][700/1583]\tLoss_D: 0.3365\tLoss_G: 4.2364\tD(x): 0.7636\tD(G(z)): 0.0225 / 0.0259\n[5/10][750/1583]\tLoss_D: 0.0768\tLoss_G: 5.0843\tD(x): 0.9344\tD(G(z)): 0.0057 / 0.0086\n[5/10][800/1583]\tLoss_D: 0.9689\tLoss_G: 7.5169\tD(x): 0.9847\tD(G(z)): 0.5446 / 0.0010\n[5/10][850/1583]\tLoss_D: 0.1005\tLoss_G: 3.8598\tD(x): 0.9847\tD(G(z)): 0.0785 / 0.0360\n[5/10][900/1583]\tLoss_D: 0.4047\tLoss_G: 5.3074\tD(x): 0.9906\tD(G(z)): 0.2929 / 0.0079\n[5/10][950/1583]\tLoss_D: 0.8929\tLoss_G: 6.4407\tD(x): 0.4932\tD(G(z)): 0.0023 / 0.0033\n[5/10][1000/1583]\tLoss_D: 0.1216\tLoss_G: 3.3760\tD(x): 0.9388\tD(G(z)): 0.0522 / 0.0488\n[5/10][1050/1583]\tLoss_D: 1.3047\tLoss_G: 3.0597\tD(x): 0.3330\tD(G(z)): 0.0072 / 0.0864\n[5/10][1100/1583]\tLoss_D: 0.1406\tLoss_G: 4.1494\tD(x): 0.9173\tD(G(z)): 0.0476 / 0.0212\n[5/10][1150/1583]\tLoss_D: 0.1382\tLoss_G: 3.8347\tD(x): 0.8949\tD(G(z)): 0.0202 / 0.0337\n[5/10][1200/1583]\tLoss_D: 0.1232\tLoss_G: 5.0778\tD(x): 0.8984\tD(G(z)): 0.0061 / 0.0098\n[5/10][1250/1583]\tLoss_D: 0.1259\tLoss_G: 4.7316\tD(x): 0.9747\tD(G(z)): 0.0879 / 0.0163\n[5/10][1300/1583]\tLoss_D: 0.0500\tLoss_G: 4.5991\tD(x): 0.9655\tD(G(z)): 0.0130 / 0.0184\n[5/10][1350/1583]\tLoss_D: 0.1549\tLoss_G: 5.1578\tD(x): 0.9956\tD(G(z)): 0.1310 / 0.0087\n[5/10][1400/1583]\tLoss_D: 0.0305\tLoss_G: 5.4274\tD(x): 0.9836\tD(G(z)): 0.0131 / 0.0072\n[5/10][1450/1583]\tLoss_D: 0.1006\tLoss_G: 5.2798\tD(x): 0.9190\tD(G(z)): 0.0097 / 0.0077\n[5/10][1500/1583]\tLoss_D: 0.6064\tLoss_G: 3.1939\tD(x): 0.6534\tD(G(z)): 0.0700 / 0.0703\n[5/10][1550/1583]\tLoss_D: 0.0649\tLoss_G: 3.5160\tD(x): 0.9733\tD(G(z)): 0.0360 / 0.0465\n[6/10][0/1583]\tLoss_D: 0.2605\tLoss_G: 3.7127\tD(x): 0.9396\tD(G(z)): 0.1653 / 0.0372\n[6/10][50/1583]\tLoss_D: 0.1619\tLoss_G: 4.5104\tD(x): 0.9816\tD(G(z)): 0.1278 / 0.0145\n[6/10][100/1583]\tLoss_D: 0.1640\tLoss_G: 4.0016\tD(x): 0.9774\tD(G(z)): 0.1268 / 0.0232\n[6/10][150/1583]\tLoss_D: 0.0480\tLoss_G: 4.8283\tD(x): 0.9693\tD(G(z)): 0.0156 / 0.0126\n[6/10][200/1583]\tLoss_D: 0.0904\tLoss_G: 4.2154\tD(x): 0.9785\tD(G(z)): 0.0643 / 0.0231\n[6/10][250/1583]\tLoss_D: 0.0291\tLoss_G: 4.6496\tD(x): 0.9881\tD(G(z)): 0.0166 / 0.0176\n[6/10][300/1583]\tLoss_D: 0.2112\tLoss_G: 4.4934\tD(x): 0.9916\tD(G(z)): 0.1662 / 0.0212\n[6/10][350/1583]\tLoss_D: 0.1579\tLoss_G: 3.3070\tD(x): 0.9157\tD(G(z)): 0.0580 / 0.0511\n[6/10][400/1583]\tLoss_D: 0.0887\tLoss_G: 4.9782\tD(x): 0.9519\tD(G(z)): 0.0350 / 0.0107\n[6/10][450/1583]\tLoss_D: 0.2867\tLoss_G: 3.4052\tD(x): 0.8632\tD(G(z)): 0.1097 / 0.0471\n[6/10][500/1583]\tLoss_D: 0.3297\tLoss_G: 2.6491\tD(x): 0.8621\tD(G(z)): 0.1475 / 0.1075\n[6/10][550/1583]\tLoss_D: 0.3668\tLoss_G: 4.8099\tD(x): 0.7268\tD(G(z)): 0.0048 / 0.0140\n[6/10][600/1583]\tLoss_D: 0.3157\tLoss_G: 3.8677\tD(x): 0.9811\tD(G(z)): 0.2364 / 0.0288\n[6/10][650/1583]\tLoss_D: 0.8217\tLoss_G: 10.2138\tD(x): 0.9832\tD(G(z)): 0.4880 / 0.0001\n[6/10][700/1583]\tLoss_D: 0.0378\tLoss_G: 3.2261\tD(x): 0.9808\tD(G(z)): 0.0178 / 0.0693\n[6/10][750/1583]\tLoss_D: 0.1301\tLoss_G: 6.1202\tD(x): 0.8847\tD(G(z)): 0.0012 / 0.0045\n[6/10][800/1583]\tLoss_D: 0.0734\tLoss_G: 4.1059\tD(x): 0.9613\tD(G(z)): 0.0312 / 0.0272\n[6/10][850/1583]\tLoss_D: 0.1736\tLoss_G: 4.1690\tD(x): 0.8948\tD(G(z)): 0.0490 / 0.0332\n[6/10][900/1583]\tLoss_D: 0.0315\tLoss_G: 6.6711\tD(x): 0.9716\tD(G(z)): 0.0020 / 0.0020\n[6/10][950/1583]\tLoss_D: 0.2350\tLoss_G: 4.5630\tD(x): 0.9943\tD(G(z)): 0.1922 / 0.0158\n[6/10][1000/1583]\tLoss_D: 0.1572\tLoss_G: 4.4337\tD(x): 0.9851\tD(G(z)): 0.1253 / 0.0185\n[6/10][1050/1583]\tLoss_D: 0.4580\tLoss_G: 4.9070\tD(x): 0.9860\tD(G(z)): 0.3090 / 0.0139\n[6/10][1100/1583]\tLoss_D: 0.1906\tLoss_G: 4.0830\tD(x): 0.9883\tD(G(z)): 0.1526 / 0.0246\n[6/10][1150/1583]\tLoss_D: 0.0506\tLoss_G: 4.2205\tD(x): 0.9660\tD(G(z)): 0.0147 / 0.0212\n[6/10][1200/1583]\tLoss_D: 0.3476\tLoss_G: 5.8818\tD(x): 0.8705\tD(G(z)): 0.1603 / 0.0051\n[6/10][1250/1583]\tLoss_D: 0.1119\tLoss_G: 3.9688\tD(x): 0.9317\tD(G(z)): 0.0357 / 0.0260\n[6/10][1300/1583]\tLoss_D: 1.2150\tLoss_G: 2.4590\tD(x): 0.3836\tD(G(z)): 0.0037 / 0.1284\n[6/10][1350/1583]\tLoss_D: 0.0631\tLoss_G: 4.4710\tD(x): 0.9875\tD(G(z)): 0.0480 / 0.0167\n[6/10][1400/1583]\tLoss_D: 0.0656\tLoss_G: 4.1537\tD(x): 0.9938\tD(G(z)): 0.0558 / 0.0256\n[6/10][1450/1583]\tLoss_D: 1.3199\tLoss_G: 6.5414\tD(x): 0.9845\tD(G(z)): 0.6528 / 0.0040\n[6/10][1500/1583]\tLoss_D: 0.3850\tLoss_G: 6.4080\tD(x): 0.9951\tD(G(z)): 0.2718 / 0.0028\n[6/10][1550/1583]\tLoss_D: 0.1085\tLoss_G: 7.8280\tD(x): 0.9059\tD(G(z)): 0.0033 / 0.0017\n[7/10][0/1583]\tLoss_D: 0.0958\tLoss_G: 4.3412\tD(x): 0.9573\tD(G(z)): 0.0482 / 0.0205\n[7/10][50/1583]\tLoss_D: 0.1177\tLoss_G: 5.5053\tD(x): 0.9522\tD(G(z)): 0.0520 / 0.0068\n[7/10][100/1583]\tLoss_D: 0.0908\tLoss_G: 4.8281\tD(x): 0.9589\tD(G(z)): 0.0443 / 0.0129\n[7/10][150/1583]\tLoss_D: 0.0497\tLoss_G: 7.4804\tD(x): 0.9538\tD(G(z)): 0.0005 / 0.0011\n[7/10][200/1583]\tLoss_D: 0.0248\tLoss_G: 4.2554\tD(x): 0.9952\tD(G(z)): 0.0195 / 0.0221\n[7/10][250/1583]\tLoss_D: 0.4428\tLoss_G: 6.7760\tD(x): 0.9875\tD(G(z)): 0.3227 / 0.0023\n[7/10][300/1583]\tLoss_D: 0.1061\tLoss_G: 5.2456\tD(x): 0.9932\tD(G(z)): 0.0899 / 0.0085\n[7/10][350/1583]\tLoss_D: 0.0900\tLoss_G: 4.1679\tD(x): 0.9536\tD(G(z)): 0.0375 / 0.0248\n[7/10][400/1583]\tLoss_D: 0.5160\tLoss_G: 3.3720\tD(x): 0.8203\tD(G(z)): 0.2345 / 0.0610\n[7/10][450/1583]\tLoss_D: 0.2382\tLoss_G: 3.5098\tD(x): 0.9868\tD(G(z)): 0.1847 / 0.0463\n[7/10][500/1583]\tLoss_D: 0.1288\tLoss_G: 4.6089\tD(x): 0.9013\tD(G(z)): 0.0136 / 0.0169\n[7/10][550/1583]\tLoss_D: 6.1310\tLoss_G: 2.5595\tD(x): 0.9965\tD(G(z)): 0.9368 / 0.2016\n[7/10][600/1583]\tLoss_D: 0.3201\tLoss_G: 3.5604\tD(x): 0.9509\tD(G(z)): 0.2077 / 0.0503\n[7/10][650/1583]\tLoss_D: 0.1220\tLoss_G: 4.5269\tD(x): 0.9159\tD(G(z)): 0.0177 / 0.0198\n[7/10][700/1583]\tLoss_D: 0.1242\tLoss_G: 4.5492\tD(x): 0.8961\tD(G(z)): 0.0045 / 0.0170\n[7/10][750/1583]\tLoss_D: 0.7109\tLoss_G: 3.9467\tD(x): 0.9811\tD(G(z)): 0.4387 / 0.0333\n[7/10][800/1583]\tLoss_D: 0.1877\tLoss_G: 3.8328\tD(x): 0.9123\tD(G(z)): 0.0780 / 0.0370\n[7/10][850/1583]\tLoss_D: 0.3274\tLoss_G: 2.9219\tD(x): 0.8131\tD(G(z)): 0.0781 / 0.0846\n[7/10][900/1583]\tLoss_D: 0.0544\tLoss_G: 4.6588\tD(x): 0.9892\tD(G(z)): 0.0414 / 0.0174\n[7/10][950/1583]\tLoss_D: 0.7421\tLoss_G: 1.9860\tD(x): 0.5615\tD(G(z)): 0.0075 / 0.1939\n[7/10][1000/1583]\tLoss_D: 0.0919\tLoss_G: 4.4154\tD(x): 0.9969\tD(G(z)): 0.0803 / 0.0205\n[7/10][1050/1583]\tLoss_D: 0.0725\tLoss_G: 5.1910\tD(x): 0.9466\tD(G(z)): 0.0129 / 0.0125\n[7/10][1100/1583]\tLoss_D: 0.0952\tLoss_G: 4.3001\tD(x): 0.9832\tD(G(z)): 0.0720 / 0.0195\n[7/10][1150/1583]\tLoss_D: 0.0303\tLoss_G: 4.4507\tD(x): 0.9766\tD(G(z)): 0.0050 / 0.0207\n[7/10][1200/1583]\tLoss_D: 0.2600\tLoss_G: 4.7910\tD(x): 0.9917\tD(G(z)): 0.2048 / 0.0122\n[7/10][1250/1583]\tLoss_D: 0.2882\tLoss_G: 5.7575\tD(x): 0.9691\tD(G(z)): 0.2007 / 0.0061\n[7/10][1300/1583]\tLoss_D: 0.0630\tLoss_G: 5.2018\tD(x): 0.9496\tD(G(z)): 0.0090 / 0.0090\n[7/10][1350/1583]\tLoss_D: 0.2318\tLoss_G: 4.7240\tD(x): 0.9333\tD(G(z)): 0.1367 / 0.0165\n[7/10][1400/1583]\tLoss_D: 0.2439\tLoss_G: 4.5969\tD(x): 0.9530\tD(G(z)): 0.1570 / 0.0170\n[7/10][1450/1583]\tLoss_D: 0.2333\tLoss_G: 2.8400\tD(x): 0.8613\tD(G(z)): 0.0626 / 0.0790\n[7/10][1500/1583]\tLoss_D: 0.3601\tLoss_G: 4.2316\tD(x): 0.7474\tD(G(z)): 0.0095 / 0.0235\n[7/10][1550/1583]\tLoss_D: 0.1433\tLoss_G: 5.2365\tD(x): 0.9625\tD(G(z)): 0.0937 / 0.0091\n[8/10][0/1583]\tLoss_D: 2.4570\tLoss_G: 3.8449\tD(x): 0.1531\tD(G(z)): 0.0034 / 0.0541\n[8/10][50/1583]\tLoss_D: 0.3675\tLoss_G: 2.8589\tD(x): 0.7907\tD(G(z)): 0.0768 / 0.1013\n[8/10][100/1583]\tLoss_D: 0.5272\tLoss_G: 4.3079\tD(x): 0.9788\tD(G(z)): 0.3442 / 0.0238\n[8/10][150/1583]\tLoss_D: 0.6882\tLoss_G: 3.4271\tD(x): 0.9030\tD(G(z)): 0.3680 / 0.0604\n[8/10][200/1583]\tLoss_D: 2.8699\tLoss_G: 1.8291\tD(x): 0.1037\tD(G(z)): 0.0005 / 0.2994\n[8/10][250/1583]\tLoss_D: 0.0843\tLoss_G: 4.0352\tD(x): 0.9704\tD(G(z)): 0.0505 / 0.0289\n[8/10][300/1583]\tLoss_D: 0.6668\tLoss_G: 6.9575\tD(x): 0.9719\tD(G(z)): 0.3759 / 0.0035\n[8/10][350/1583]\tLoss_D: 0.2347\tLoss_G: 5.4009\tD(x): 0.8509\tD(G(z)): 0.0344 / 0.0093\n[8/10][400/1583]\tLoss_D: 0.7720\tLoss_G: 3.6257\tD(x): 0.7607\tD(G(z)): 0.3265 / 0.0546\n[8/10][450/1583]\tLoss_D: 0.6595\tLoss_G: 6.7962\tD(x): 0.9586\tD(G(z)): 0.4136 / 0.0018\n[8/10][500/1583]\tLoss_D: 0.0921\tLoss_G: 4.9441\tD(x): 0.9252\tD(G(z)): 0.0090 / 0.0125\n[8/10][550/1583]\tLoss_D: 1.5903\tLoss_G: 1.1796\tD(x): 0.2771\tD(G(z)): 0.0013 / 0.4039\n[8/10][600/1583]\tLoss_D: 0.0529\tLoss_G: 4.3027\tD(x): 0.9897\tD(G(z)): 0.0407 / 0.0211\n[8/10][650/1583]\tLoss_D: 0.5498\tLoss_G: 6.5201\tD(x): 0.9651\tD(G(z)): 0.3484 / 0.0025\n[8/10][700/1583]\tLoss_D: 0.3315\tLoss_G: 3.4536\tD(x): 0.8904\tD(G(z)): 0.1707 / 0.0494\n[8/10][750/1583]\tLoss_D: 0.4548\tLoss_G: 3.0908\tD(x): 0.9635\tD(G(z)): 0.2994 / 0.0840\n[8/10][800/1583]\tLoss_D: 0.1665\tLoss_G: 4.5681\tD(x): 0.9858\tD(G(z)): 0.1353 / 0.0147\n[8/10][850/1583]\tLoss_D: 0.2898\tLoss_G: 6.2740\tD(x): 0.9824\tD(G(z)): 0.2249 / 0.0028\n[8/10][900/1583]\tLoss_D: 0.3700\tLoss_G: 5.6310\tD(x): 0.7281\tD(G(z)): 0.0039 / 0.0069\n[8/10][950/1583]\tLoss_D: 0.5876\tLoss_G: 4.4553\tD(x): 0.9482\tD(G(z)): 0.3615 / 0.0219\n[8/10][1000/1583]\tLoss_D: 0.2530\tLoss_G: 3.2839\tD(x): 0.8526\tD(G(z)): 0.0625 / 0.0673\n[8/10][1050/1583]\tLoss_D: 0.2302\tLoss_G: 5.0087\tD(x): 0.9690\tD(G(z)): 0.1665 / 0.0118\n[8/10][1100/1583]\tLoss_D: 0.0644\tLoss_G: 3.2721\tD(x): 0.9809\tD(G(z)): 0.0409 / 0.0624\n[8/10][1150/1583]\tLoss_D: 0.3527\tLoss_G: 3.3452\tD(x): 0.8624\tD(G(z)): 0.1508 / 0.0613\n[8/10][1200/1583]\tLoss_D: 0.2462\tLoss_G: 1.9225\tD(x): 0.8677\tD(G(z)): 0.0713 / 0.2157\n[8/10][1250/1583]\tLoss_D: 1.1298\tLoss_G: 4.0842\tD(x): 0.4064\tD(G(z)): 0.0010 / 0.0285\n[8/10][1300/1583]\tLoss_D: 0.7385\tLoss_G: 7.4073\tD(x): 0.9931\tD(G(z)): 0.4382 / 0.0014\n[8/10][1350/1583]\tLoss_D: 0.0365\tLoss_G: 3.9131\tD(x): 0.9907\tD(G(z)): 0.0261 / 0.0339\n[8/10][1400/1583]\tLoss_D: 0.0605\tLoss_G: 4.7669\tD(x): 0.9935\tD(G(z)): 0.0511 / 0.0140\n[8/10][1450/1583]\tLoss_D: 0.0808\tLoss_G: 4.7457\tD(x): 0.9463\tD(G(z)): 0.0221 / 0.0167\n[8/10][1500/1583]\tLoss_D: 0.0422\tLoss_G: 8.1173\tD(x): 0.9604\tD(G(z)): 0.0007 / 0.0009\n[8/10][1550/1583]\tLoss_D: 0.3718\tLoss_G: 4.2554\tD(x): 0.8489\tD(G(z)): 0.1574 / 0.0283\n[9/10][0/1583]\tLoss_D: 0.3626\tLoss_G: 3.3703\tD(x): 0.7936\tD(G(z)): 0.0921 / 0.0500\n[9/10][50/1583]\tLoss_D: 0.2351\tLoss_G: 3.0021\tD(x): 0.8394\tD(G(z)): 0.0391 / 0.0805\n[9/10][100/1583]\tLoss_D: 0.8303\tLoss_G: 7.3406\tD(x): 0.9867\tD(G(z)): 0.4806 / 0.0014\n[9/10][150/1583]\tLoss_D: 0.4731\tLoss_G: 4.5171\tD(x): 0.6941\tD(G(z)): 0.0120 / 0.0232\n[9/10][200/1583]\tLoss_D: 1.0686\tLoss_G: 2.0994\tD(x): 0.4266\tD(G(z)): 0.0057 / 0.2032\n[9/10][250/1583]\tLoss_D: 0.4494\tLoss_G: 3.0254\tD(x): 0.8252\tD(G(z)): 0.1873 / 0.0882\n[9/10][300/1583]\tLoss_D: 0.3021\tLoss_G: 4.4330\tD(x): 0.9440\tD(G(z)): 0.1996 / 0.0184\n[9/10][350/1583]\tLoss_D: 0.3693\tLoss_G: 3.0542\tD(x): 0.7466\tD(G(z)): 0.0187 / 0.0742\n[9/10][400/1583]\tLoss_D: 0.1385\tLoss_G: 4.8019\tD(x): 0.9010\tD(G(z)): 0.0208 / 0.0182\n[9/10][450/1583]\tLoss_D: 0.2766\tLoss_G: 2.6711\tD(x): 0.8400\tD(G(z)): 0.0720 / 0.1081\n[9/10][500/1583]\tLoss_D: 1.1097\tLoss_G: 3.0379\tD(x): 0.4202\tD(G(z)): 0.0019 / 0.0874\n[9/10][550/1583]\tLoss_D: 0.6868\tLoss_G: 2.4659\tD(x): 0.6054\tD(G(z)): 0.0226 / 0.1499\n[9/10][600/1583]\tLoss_D: 0.3293\tLoss_G: 3.4430\tD(x): 0.8895\tD(G(z)): 0.1655 / 0.0501\n[9/10][650/1583]\tLoss_D: 2.9044\tLoss_G: 0.4706\tD(x): 0.1110\tD(G(z)): 0.0013 / 0.6699\n[9/10][700/1583]\tLoss_D: 0.2508\tLoss_G: 5.2170\tD(x): 0.9953\tD(G(z)): 0.1980 / 0.0096\n[9/10][750/1583]\tLoss_D: 0.0550\tLoss_G: 3.1139\tD(x): 0.9817\tD(G(z)): 0.0344 / 0.0727\n[9/10][800/1583]\tLoss_D: 0.2037\tLoss_G: 5.3293\tD(x): 0.9730\tD(G(z)): 0.1409 / 0.0104\n[9/10][850/1583]\tLoss_D: 0.2112\tLoss_G: 3.3515\tD(x): 0.9157\tD(G(z)): 0.1038 / 0.0557\n[9/10][900/1583]\tLoss_D: 0.7535\tLoss_G: 2.7496\tD(x): 0.6622\tD(G(z)): 0.1338 / 0.1142\n[9/10][950/1583]\tLoss_D: 0.1388\tLoss_G: 3.7103\tD(x): 0.9557\tD(G(z)): 0.0828 / 0.0431\n[9/10][1000/1583]\tLoss_D: 0.2499\tLoss_G: 3.7634\tD(x): 0.9274\tD(G(z)): 0.1386 / 0.0416\n[9/10][1050/1583]\tLoss_D: 0.5021\tLoss_G: 5.7147\tD(x): 0.9849\tD(G(z)): 0.3200 / 0.0070\n[9/10][1100/1583]\tLoss_D: 0.5199\tLoss_G: 5.8632\tD(x): 0.6661\tD(G(z)): 0.0037 / 0.0121\n[9/10][1150/1583]\tLoss_D: 0.2256\tLoss_G: 4.3842\tD(x): 0.9778\tD(G(z)): 0.1720 / 0.0184\n[9/10][1200/1583]\tLoss_D: 0.5194\tLoss_G: 1.9354\tD(x): 0.6688\tD(G(z)): 0.0380 / 0.2065\n[9/10][1250/1583]\tLoss_D: 0.4267\tLoss_G: 3.6725\tD(x): 0.6964\tD(G(z)): 0.0047 / 0.0408\n[9/10][1300/1583]\tLoss_D: 0.1564\tLoss_G: 6.2245\tD(x): 0.8711\tD(G(z)): 0.0036 / 0.0042\n[9/10][1350/1583]\tLoss_D: 0.5540\tLoss_G: 12.3099\tD(x): 0.9887\tD(G(z)): 0.3564 / 0.0000\n[9/10][1400/1583]\tLoss_D: 0.2375\tLoss_G: 4.8241\tD(x): 0.9532\tD(G(z)): 0.1532 / 0.0139\n[9/10][1450/1583]\tLoss_D: 0.3130\tLoss_G: 3.9461\tD(x): 0.9447\tD(G(z)): 0.2084 / 0.0300\n[9/10][1500/1583]\tLoss_D: 0.2859\tLoss_G: 4.3700\tD(x): 0.9453\tD(G(z)): 0.1884 / 0.0193\n[9/10][1550/1583]\tLoss_D: 0.1260\tLoss_G: 5.1380\tD(x): 0.9794\tD(G(z)): 0.0933 / 0.0108\n</pre> In\u00a0[11]: Copied! <pre>torch.save(netG.state_dict(), \"model.pth\")\nnetG.load_state_dict(torch.load(\"model.pth\"))\n</pre> torch.save(netG.state_dict(), \"model.pth\") netG.load_state_dict(torch.load(\"model.pth\")) Out[11]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[12]: Copied! <pre>fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n</pre> fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)  HTML(ani.to_jshtml()) Out[12]: Once Loop Reflect In\u00a0[13]: Copied! <pre>def objective_function(complements):\n    lesion_dict = defaultdict(list)\n    for x, y in complements:\n        lesion_dict[x].append(y)\n    return netG(fixed_noise, lesion_dict).detach().cpu().numpy()\n</pre> def objective_function(complements):     lesion_dict = defaultdict(list)     for x, y in complements:         lesion_dict[x].append(y)     return netG(fixed_noise, lesion_dict).detach().cpu().numpy() In\u00a0[14]: Copied! <pre>elements = []\nfor i in range(ngf*16):\n    elements.append((0, i))\n\nfor i in range(1, 5):\n    for j in range(0, ngf * 2**(4-i)):\n        elements.append((i, j))\n\nfor i in range(3):\n    elements.append((5, i))\nlen(elements)\n</pre> elements = [] for i in range(ngf*16):     elements.append((0, i))  for i in range(1, 5):     for j in range(0, ngf * 2**(4-i)):         elements.append((i, j))  for i in range(3):     elements.append((5, i)) len(elements) Out[14]: <pre>1987</pre> In\u00a0[15]: Copied! <pre>netG.eval()\nshapley_modes, _, _ = msa.interface(\n    elements=elements,\n    n_permutations=1000, # might want to increase it for better results\n    objective_function=objective_function, #only the first batch to save time. But the batch size is 1024 i.e ~100 images per class\n    n_parallel_games=1, #parallelized over all CPU cores\n    rng=RNG)\n</pre> netG.eval() shapley_modes, _, _ = msa.interface(     elements=elements,     n_permutations=1000, # might want to increase it for better results     objective_function=objective_function, #only the first batch to save time. But the batch size is 1024 i.e ~100 images per class     n_parallel_games=1, #parallelized over all CPU cores     rng=RNG) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1987/1987 [00:40&lt;00:00, 49.30it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [12:18:55&lt;00:00, 44.34s/it] \n</pre> In\u00a0[16]: Copied! <pre>shapley_modes.to_pickle(\"save.pkl\")\n</pre> shapley_modes.to_pickle(\"save.pkl\") In\u00a0[33]: Copied! <pre>import pickle as pkl\n</pre> import pickle as pkl In\u00a0[34]: Copied! <pre>with open('save.pkl', 'rb') as fp:\n    shapley_modes = pkl.load(fp)\n</pre> with open('save.pkl', 'rb') as fp:     shapley_modes = pkl.load(fp) In\u00a0[35]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_total_contributions()), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_total_contributions()), padding=2, normalize=True), (1, 2, 0))) Out[35]: <pre>&lt;matplotlib.image.AxesImage at 0x7f45657d8f70&gt;</pre> In\u00a0[266]: Copied! <pre>def get_layer_contributions(layer):\n    layer_contrib = []\n\n    for x, y in elements:\n        if x == layer:\n            layer_contrib.append(torch.Tensor(shapley_modes.get_shapley_mode((x, y))))\n    return layer_contrib\n</pre> def get_layer_contributions(layer):     layer_contrib = []      for x, y in elements:         if x == layer:             layer_contrib.append(torch.Tensor(shapley_modes.get_shapley_mode((x, y))))     return layer_contrib In\u00a0[270]: Copied! <pre>get_layer_contributions(0)[0].shape\n</pre> get_layer_contributions(0)[0].shape Out[270]: <pre>torch.Size([32, 3, 128, 128])</pre> In\u00a0[283]: Copied! <pre>layer = 5\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n</pre> layer = 5  fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)  HTML(ani.to_jshtml()) Out[283]: Once Loop Reflect In\u00a0[282]: Copied! <pre>layer = 4\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n</pre> layer = 4  fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)  HTML(ani.to_jshtml()) Out[282]: Once Loop Reflect In\u00a0[284]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(sum(get_layer_contributions(0))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(sum(get_layer_contributions(0))), padding=2, normalize=True), (1, 2, 0))) Out[284]: <pre>&lt;matplotlib.image.AxesImage at 0x7f45241ef310&gt;</pre> In\u00a0[290]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(torch.Tensor(shapley_modes.get_total_contributions()) - sum(get_layer_contributions(4))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(torch.Tensor(shapley_modes.get_total_contributions()) - sum(get_layer_contributions(4))), padding=2, normalize=True), (1, 2, 0))) Out[290]: <pre>&lt;matplotlib.image.AxesImage at 0x7f406da233d0&gt;</pre> In\u00a0[262]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((0, 2))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((0, 2))), padding=2, normalize=True), (1, 2, 0))) Out[262]: <pre>&lt;matplotlib.image.AxesImage at 0x7f45348ee2e0&gt;</pre> In\u00a0[46]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((1, 20))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((1, 20))), padding=2, normalize=True), (1, 2, 0))) Out[46]: <pre>&lt;matplotlib.image.AxesImage at 0x7f45653161c0&gt;</pre> In\u00a0[20]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((2, 5))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((2, 5))), padding=2, normalize=True), (1, 2, 0))) Out[20]: <pre>&lt;matplotlib.image.AxesImage at 0x7f46306df880&gt;</pre> In\u00a0[47]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((3, 10))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((3, 10))), padding=2, normalize=True), (1, 2, 0))) Out[47]: <pre>&lt;matplotlib.image.AxesImage at 0x7f45652f24f0&gt;</pre> In\u00a0[50]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((4, 20))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode((4, 20))), padding=2, normalize=True), (1, 2, 0))) Out[50]: <pre>&lt;matplotlib.image.AxesImage at 0x7f45651aa2b0&gt;</pre> In\u00a0[264]: Copied! <pre>\n</pre> In\u00a0[265]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(sum(get_layer_contributions(0))), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(sum(get_layer_contributions(0))), padding=2, normalize=True), (1, 2, 0))) Out[265]: <pre>&lt;matplotlib.image.AxesImage at 0x7f453483d280&gt;</pre> In\u00a0[254]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(1), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(1), padding=2, normalize=True), (1, 2, 0))) Out[254]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4534cea700&gt;</pre> In\u00a0[256]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(2), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(2), padding=2, normalize=True), (1, 2, 0))) Out[256]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4534c33790&gt;</pre> In\u00a0[257]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(3), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(3), padding=2, normalize=True), (1, 2, 0))) Out[257]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4534ba5af0&gt;</pre> In\u00a0[258]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(4), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(4), padding=2, normalize=True), (1, 2, 0))) Out[258]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4534b21220&gt;</pre> In\u00a0[259]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(5), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(get_layer_contributions(5), padding=2, normalize=True), (1, 2, 0))) Out[259]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4534a81820&gt;</pre> In\u00a0[23]: Copied! <pre>data  = pd.read_csv(\"save.csv\")\n</pre> data  = pd.read_csv(\"save.csv\") In\u00a0[24]: Copied! <pre>from msapy.datastructures import ShapleyModeND\nshapley_modes = ShapleyModeND(data, (64, 3, 64, 64))\n</pre> from msapy.datastructures import ShapleyModeND shapley_modes = ShapleyModeND(data, (64, 3, 64, 64)) In\u00a0[25]: Copied! <pre>shapley_modes.head()\n</pre> shapley_modes.head() Out[25]: Unnamed: 0 (0, 0) (0, 1) (0, 2) (0, 3) (0, 4) (0, 5) (0, 6) (0, 7) (0, 8) ... (2, 54) (2, 55) (2, 56) (2, 57) (2, 58) (2, 59) (2, 60) (2, 61) (2, 62) (2, 63) 0 0 -0.008011 0.000507 -0.003670 0.004912 0.002389 0.028007 -0.006156 0.000504 0.001395 ... 0.000294 0.032455 -0.056266 -0.071625 0.034472 0.065164 0.000229 -0.020261 -0.014153 0.020508 1 1 -0.006666 0.002961 -0.001881 0.004971 0.003963 0.032097 -0.005553 0.003863 0.004251 ... 0.048232 0.021015 0.007530 -0.137956 -0.042188 0.058649 -0.017935 -0.019456 -0.058080 0.050915 2 2 -0.005590 0.004933 -0.002259 0.003790 0.003247 0.036187 -0.006335 0.005841 0.003215 ... -0.019988 0.027050 -0.074247 -0.100684 -0.053767 0.085536 0.002603 0.001242 -0.060125 0.034323 3 3 -0.004106 0.005408 -0.001384 0.003698 0.005717 0.028947 -0.004903 0.007108 0.003967 ... 0.052053 0.014999 0.009317 -0.154490 0.003706 0.063747 -0.011577 -0.004890 -0.090471 0.052416 4 4 -0.004776 0.005859 -0.002446 0.004773 0.005917 0.036999 -0.004351 0.007374 0.004908 ... -0.024068 0.033097 -0.071527 -0.070253 0.026837 0.074985 -0.002468 0.000647 0.006541 0.017303 <p>5 rows \u00d7 449 columns</p> In\u00a0[26]: Copied! <pre>shapley_modes.drop(columns=\"Unnamed: 0\", inplace=True)\n</pre> shapley_modes.drop(columns=\"Unnamed: 0\", inplace=True) In\u00a0[32]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode(\"(1, 10)\")), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_shapley_mode(\"(1, 10)\")), padding=2, normalize=True), (1, 2, 0))) Out[32]: <pre>&lt;matplotlib.image.AxesImage at 0x7f456c00c430&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[28]: Copied! <pre>plt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_total_contributions()), padding=2, normalize=True), (1, 2, 0)))\n</pre> plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(shapley_modes.get_total_contributions()), padding=2, normalize=True), (1, 2, 0))) Out[28]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4564759f10&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/Shapley%20GAN.html#work-in-progress","title":"Work In Progress\u00b6","text":""},{"location":"examples/brain%20modes.html","title":"Capturing contributions in different 'brain modes'","text":"In\u00a0[1]: Copied! <pre># Uncomment the line below if you don't have them.\n# !pip install networkx matplotlib seaborn\n</pre> # Uncomment the line below if you don't have them. # !pip install networkx matplotlib seaborn In\u00a0[2]: Copied! <pre># Imports\nimport matplotlib.pyplot as plt\n# ---------\nfrom msapy import msa, utils as ut\n# ---------\nimport matplotlib as mpl\nimport seaborn as sns\nmpl.rcParams['pdf.fonttype'] = 42\nmpl.rcParams['font.size'] = 8\n# ---------\nCM = 1 / 2.54\nSEED = 111\nFIGPATH = \"figures/bm/\"\n</pre> # Imports import matplotlib.pyplot as plt # --------- from msapy import msa, utils as ut # --------- import matplotlib as mpl import seaborn as sns mpl.rcParams['pdf.fonttype'] = 42 mpl.rcParams['font.size'] = 8 # --------- CM = 1 / 2.54 SEED = 111 FIGPATH = \"figures/bm/\" <pre>/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version &gt;=1.18.5 and &lt;1.25.0 is required for this version of SciPy (detected version 1.26.3\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n</pre> <p>Although the original paper talks about two regions, here we go with some dummy regions and work with one to three regions. Keep it simple but more spicy! The regions of interest will be mostly 'a' and 'b' but once 'c' too.</p> In\u00a0[3]: Copied! <pre>regions = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n</pre> regions = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] In\u00a0[4]: Copied! <pre>def unicity(complements):\n    # Region 'a' is the only cause of the function.\n\n    if set('a').issubset(complements):\n        return 0  # simulates a total behavioral deficit.\n    else:\n        return 100  # otherwise, we're cool.\n\n\ndef equivalence(complements):\n    # Function relies on one **or** the other region so lesioning either will produce a deficit.\n\n    if set('a').issubset(complements) or set('b').issubset(complements):\n        return 0\n    else:\n        return 100\n\n\ndef association(complements):\n    # If **both** regions are damaged at the same time, there will be a problem.\n\n    if set(['a', 'b']).issubset(complements):\n        return 0\n    else:\n        return 100\n\n\ndef summation(complements):\n    # Lesioning both will have a **bigger impact** compared to lesioning one, and it get's worst with three.\n\n    if set(['a', 'b', 'c']).issubset(complements):\n        return 30  # lesions add up and make larger impact on the performance.\n    elif set(['a', 'b']).issubset(complements):\n        return 60\n    elif set('a').issubset(complements):\n        return 90\n    else:\n        return 100\n\n\ndef mutual_inhibition(complements):\n    # Lesioning one region produces deficit while lesioning another repairs it.\n\n    if set('a').issubset(complements) and set('b').issubset(complements):\n        return 100  # Paradoxically, two lesions = full performance\n\n    elif set('a').issubset(complements):\n        return 0  # lesioning either one is a bad idea\n    elif set('b').issubset(complements):\n        return 0\n\n    else:\n        return 100\n</pre> def unicity(complements):     # Region 'a' is the only cause of the function.      if set('a').issubset(complements):         return 0  # simulates a total behavioral deficit.     else:         return 100  # otherwise, we're cool.   def equivalence(complements):     # Function relies on one **or** the other region so lesioning either will produce a deficit.      if set('a').issubset(complements) or set('b').issubset(complements):         return 0     else:         return 100   def association(complements):     # If **both** regions are damaged at the same time, there will be a problem.      if set(['a', 'b']).issubset(complements):         return 0     else:         return 100   def summation(complements):     # Lesioning both will have a **bigger impact** compared to lesioning one, and it get's worst with three.      if set(['a', 'b', 'c']).issubset(complements):         return 30  # lesions add up and make larger impact on the performance.     elif set(['a', 'b']).issubset(complements):         return 60     elif set('a').issubset(complements):         return 90     else:         return 100   def mutual_inhibition(complements):     # Lesioning one region produces deficit while lesioning another repairs it.      if set('a').issubset(complements) and set('b').issubset(complements):         return 100  # Paradoxically, two lesions = full performance      elif set('a').issubset(complements):         return 0  # lesioning either one is a bad idea     elif set('b').issubset(complements):         return 0      else:         return 100 <p>Now let's loop through them, simulate, and calculate Shapley values of each element in these brain modes.</p> In\u00a0[5]: Copied! <pre>modes = [unicity, equivalence, association, summation, mutual_inhibition]\n\nshapley_tables = dict.fromkeys([keys.__name__ for keys in modes])\n\nfor mode in modes:\n    shapley_tables[mode.__name__] = msa.interface(elements=regions,\n                                                  n_permutations=1_000,\n                                                  objective_function=mode,\n                                                  n_parallel_games=1,\n                                                  random_seed=SEED)\n</pre> modes = [unicity, equivalence, association, summation, mutual_inhibition]  shapley_tables = dict.fromkeys([keys.__name__ for keys in modes])  for mode in modes:     shapley_tables[mode.__name__] = msa.interface(elements=regions,                                                   n_permutations=1_000,                                                   objective_function=mode,                                                   n_parallel_games=1,                                                   random_seed=SEED) <p>And plotting stuff:</p> In\u00a0[6]: Copied! <pre>fig, axes = plt.subplots(1, 5, figsize=(12*CM, 5*CM), dpi=150)\nfig.suptitle(\"Regions' contributions w.r.t different Brain Modes\")\n\nfor i, mode in enumerate(shapley_tables.keys()):\n\n    axes[i].set_title(f'{mode}', fontsize=8)\n    dataset = ut.sorter(shapley_tables[mode])\n    sns.barplot(data=dataset, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'})\n\nfig.tight_layout()\nplt.savefig(f\"{FIGPATH}brain_modes.pdf\", dpi=300, bbox_inches='tight')\n</pre>  fig, axes = plt.subplots(1, 5, figsize=(12*CM, 5*CM), dpi=150) fig.suptitle(\"Regions' contributions w.r.t different Brain Modes\")  for i, mode in enumerate(shapley_tables.keys()):      axes[i].set_title(f'{mode}', fontsize=8)     dataset = ut.sorter(shapley_tables[mode])     sns.barplot(data=dataset, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'})  fig.tight_layout() plt.savefig(f\"{FIGPATH}brain_modes.pdf\", dpi=300, bbox_inches='tight') <p>The interesting points for me are the negative Shapley value of region 'a' and 'b' in the mutual inhibition mode, and the indistinguishable contributions in equivalence and association modes.</p>"},{"location":"examples/brain%20modes.html#capturing-contributions-in-different-brain-modes","title":"Capturing contributions in different 'brain modes'\u00b6","text":"<p>In this notebook, I will simulate the modes described here:</p> <ul> <li>Toba, Monica N., Olivier Godefroy, R. Jarrett Rushmore, Melissa Zavaglia, Redwan Maatoug, Claus C. Hilgetag, and Antoni Valero-Cabr\u00e9. 2019. \u201cRevisiting \u2018brain Modes\u2019 in a New Computational Era: Approaches for the Characterization of Brain-Behavioural Associations.\u201d Brain: A Journal of Neurology, November. https://doi.org/10.1093/brain/awz343.</li> </ul> <p>The idea, as described in the above paper comes from an earlier work by Godfroy and colleagues in which they \"conceptualized the potential of four elementary typologies of brain-behaviour relationships named \u2018brain modes\u2019 (unicity, equivalence, association, summation) as building blocks able to describe the association between intact or lesionedbrain regions and cognitive processes or neurological deficits.\"</p> <p>In this work, Toba and co-workers revisted the idea and added a fifth mode called \"mutual inhibition\". It's a very nice concept for categorizing brain-behavior relationships so I thought I give MSA a try and see if I can capture these interactions.</p>"},{"location":"examples/brain%20modes.html#modes","title":"Modes:\u00b6","text":"<p>Let's look at the modes first:</p> <p>Each mode represents a type of interaction between two (or more) regions that causes some deficit in a cognitive/behavioral domain. Let's look at the parts describing these modes in the paper: (bolded parts are by me)</p> <ul> <li><p>Unicity: Unicity, could depict the functional contributions of isolated nodes, which are hardly present in the highly and intricately connected mammalian nervous systems. Thus, this mode has been theoretically hypothesized but remains to be documented clinically.</p> </li> <li><p>Equivalence: The equivalence brain mode has been documented theoretically and also clinically. Indeed, in the original paper describing brain modes, single lesions localized at two different levels along the cortico-spinal tract were characterized as equally responsible for motor weakness.</p> </li> <li><p>Association: The association brain mode has been identified theoretically but remains to be better documented clinically, as it requires rare-to-find patients with selective lesions damaging multiple regions within the same network. This mode was originally illustrated in patients with unilateral lenticulostriate lesions(Godefroy et al., 1992) showing executive function impairment only when, additionally, they suffered an associated cortical infarct.</p> </li> <li><p>Summation: The summation mode has been documented both theoretically and clinically. For example, in language impairments, non-fluent aphasia was associated with lesions of putamen and surrounding structures while mutism was associated with large lesion of the three frontal gyri and putamen.</p> </li> <li><p>Mutual Inhibition: [...]multivariate CART approaches originally used for their characterization failed to identify \u2018paradoxical lesion cancellation\u2019 effects,[...] This phenomenon described the paradoxical improvement of performance deficits caused by a circumscribed lesion thanks to a reversible or permanent suppression of activity in a second brain area interacting with the former.</p> </li> </ul> <p>So let's code them:</p>"},{"location":"examples/minimal.html","title":"Minimal examples of how to use MSA","text":"<p>MSA stands for \"Multiperturbation Shapley value Analysis\" and as the name suggests, it's comprised of \"multiple perturbations\" and \"Shapley value\". Fundamentally, it uses a dataset of multi-element perturbation to estimate Shapley values of each element with respect to a global function. I'll refer you to these papers for technical and conceptual details:</p> <ul> <li><p>Keinan, Alon, Claus C. Hilgetag, Isaac Meilijson, and Eytan Ruppin. 2004. \u201cCausal Localization of Neural Function: The Shapley Value Method.\u201d Neurocomputing 58-60 (June): 215\u201322.</p> </li> <li><p>Keinan, Alon, Ben Sandbank, Claus C. Hilgetag, Isaac Meilijson, and Eytan Ruppin. 2004. \u201cFair Attribution of Functional Contribution in Artificial and Biological Networks.\u201d Neural Computation 16 (9): 1887\u20131915.</p> </li> <li><p>K\u00f6tter, Rolf. 2007. \u201cShapley Ratings in Brain Networks.\u201d Frontiers in Neuroinformatics 1 (NOV): 1\u20139.</p> </li> <li><p>Toba, Monica N., Melissa Zavaglia, Caroline Malherbe, Tristan Moreau, Federica Rastelli, Anna Kaglik, Romain Valabr\u00e8gue, Pascale Pradat-Diehl, Claus C. Hilgetag, and Antoni Valero-Cabr\u00e9. 2020. \u201cGame Theoretical Mapping of White Matter Contributions to Visuospatial Attention in Stroke Patients with Hemineglect.\u201d Human Brain Mapping, no. February: 1\u201325.</p> </li> <li><p>Zavaglia, Melissa, and Claus C. Hilgetag. 2016. \u201cCausal Functional Contributions and Interactions in the Attention Network of the Brain: An Objective Multi-Perturbation Analysis.\u201d Brain Structure &amp; Function 221 (5): 2553\u201368.</p> </li> </ul> <p>The toolbox is designed to handle a large variety of systems. All it needs is a list of elements, let's say node labels, node indices, tuples (connections between nodes for examples), but I didn't really test things that are not networks! For example, if you're interested in explainable machine learning there's already a very versatile toolbox called SHAP that calculates the Shapley values of model's input features. MSA is traditionally used as a brain-mapping tool (I mean, just look at those papers) and I'm also a neuroscientist so there's a bias towards networks and lesion-mapping conceptualization. These said, let's see how MSA works using some small networks.</p> In\u00a0[1]: Copied! <pre># Uncomment the line below if you don't have them.\n# !pip install networkx matplotlib seaborn\n</pre> # Uncomment the line below if you don't have them. # !pip install networkx matplotlib seaborn In\u00a0[2]: Copied! <pre># Imports n stuff\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport numpy as np\n#---------\nfrom msapy import msa, utils as ut\n#---------\nmpl.rcParams['pdf.fonttype'] = 42\nmpl.rcParams['font.size'] = 10\nCM = 1 / 2.54\nSEED = 111\nFIGPATH = \"figures/minimal/\"\n</pre> # Imports n stuff import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns import networkx as nx import numpy as np #--------- from msapy import msa, utils as ut #--------- mpl.rcParams['pdf.fonttype'] = 42 mpl.rcParams['font.size'] = 10 CM = 1 / 2.54 SEED = 111 FIGPATH = \"figures/minimal/\" <pre>/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version &gt;=1.18.5 and &lt;1.25.0 is required for this version of SciPy (detected version 1.26.3\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n</pre> <p>As mentioned, all MSA needs is a list of elements, that's not quite true tho it needs a game. Players (elements) should play the game and well, the game can be anything and in a way, that's the beauty of it. Here, I will define a game called \"ge\" that stands for \"average global efficiency\" (yeah it should be \"age\" but that would be weird!), which is a graph metric. See this page for more detail.</p> <p>So you need to define a game, a function that encapsulates the procedure you care about while the system is being lesioned. For example, if you're using MSA in the classic sense of brain mapping, the game function is where you perform the \"cognitive experiment\". Keep in mind a few things:</p> <ol> <li><p>Take care of the edge-cases. MSA goes through all the possible ways you can lesion your system so if you're interested in the contribution (shapley value) of each node to information flow in a network, then in some of these combinations of lesions the network might end up with just a bunch of isolated nodes. Like define conditions, if len(x) &lt; y return 0.</p> </li> <li><p>Copy your stuff before feeding it to the toolbox. Otherwise (depending on how you're lesioning the system) it might leak to the next step and messes with everything.</p> </li> <li><p>The result should be \"one value\", a global measure that describes the system with one value.</p> </li> </ol> In\u00a0[3]: Copied! <pre>def ge(complements, graph):\n    if len(complements) &lt; 0:\n        # grand coalition is in, nothing to lesion so work with the intact network.\n        return float(nx.global_efficiency(graph))\n    \n    elif len(complements) == len(graph):\n        # everything is gone, why calling nx.global_efficiency?\n        return 0.0\n\n    else:\n        # lesion the system, calculate global efficiency\n        lesioned = graph.copy()\n        lesioned.remove_nodes_from(complements)\n        return float(nx.global_efficiency(lesioned))\n</pre> def ge(complements, graph):     if len(complements) &lt; 0:         # grand coalition is in, nothing to lesion so work with the intact network.         return float(nx.global_efficiency(graph))          elif len(complements) == len(graph):         # everything is gone, why calling nx.global_efficiency?         return 0.0      else:         # lesion the system, calculate global efficiency         lesioned = graph.copy()         lesioned.remove_nodes_from(complements)         return float(nx.global_efficiency(lesioned)) <p>Next, I'll make a graph with an intuitive topology, i.e., a balanced tree. Intuitively, the farther we go from the root, the smaller shapley values we will have. It's possible that the outer branch ends with negative shapley values since removing them shortenes the overal paths and improves global efficiency.</p> In\u00a0[4]: Copied! <pre>G = nx.balanced_tree(3,3)\nfig,ax = plt.subplots()\nfig.set_dpi(150)\nfig.set_size_inches((12*CM,12*CM))\n\nax = nx.draw_networkx(G,with_labels=True,node_color='#FFE48D',font_size=8)\nplt.savefig(f\"{FIGPATH}balanced_tree.pdf\",dpi=300,bbox_inches='tight')\n</pre> G = nx.balanced_tree(3,3) fig,ax = plt.subplots() fig.set_dpi(150) fig.set_size_inches((12*CM,12*CM))  ax = nx.draw_networkx(G,with_labels=True,node_color='#FFE48D',font_size=8) plt.savefig(f\"{FIGPATH}balanced_tree.pdf\",dpi=300,bbox_inches='tight') In\u00a0[5]: Copied! <pre>node_perms = msa.make_permutation_space(elements=list(G.nodes), n_permutations=1_000,random_seed=SEED)\nprint(np.shape(node_perms))\nprint(f'Nodes: {list(G.nodes)}\\n')\nprint(f'Permuted nodes: {node_perms[0]}')\n</pre> node_perms = msa.make_permutation_space(elements=list(G.nodes), n_permutations=1_000,random_seed=SEED) print(np.shape(node_perms)) print(f'Nodes: {list(G.nodes)}\\n') print(f'Permuted nodes: {node_perms[0]}') <pre>(1000, 40)\nNodes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n\nPermuted nodes: (21, 16, 18, 22, 15, 24, 38, 7, 29, 33, 11, 25, 19, 14, 37, 6, 27, 10, 0, 17, 5, 9, 32, 13, 39, 28, 1, 3, 36, 34, 26, 31, 20, 8, 4, 23, 12, 30, 2, 35)\n</pre> In\u00a0[6]: Copied! <pre>node_combs_template = msa.make_combination_space(permutation_space=node_perms)\nnode_compl_template = msa.make_complement_space(combination_space=node_combs_template, elements=list(G.nodes))\nprint(f'Number of lesion combinations: {len(node_combs_template)}')\n</pre> node_combs_template = msa.make_combination_space(permutation_space=node_perms) node_compl_template = msa.make_complement_space(combination_space=node_combs_template, elements=list(G.nodes)) print(f'Number of lesion combinations: {len(node_combs_template)}')  <pre>Number of lesion combinations: 36088\n</pre> <p>Here we use the parallelized_take_contributions to actually play games and fill the values. We then use this multi-site perturbation dataset to calculate shapley values and sort them.</p> In\u00a0[7]: Copied! <pre>global_eff,_ = ut.parallelized_take_contributions(complement_space=node_compl_template,\n                                                  combination_space=node_combs_template,\n                                                  objective_function=ge,\n                                                  objective_function_params={'graph': G})\n</pre> global_eff,_ = ut.parallelized_take_contributions(complement_space=node_compl_template,                                                   combination_space=node_combs_template,                                                   objective_function=ge,                                                   objective_function_params={'graph': G})        0.01% [5/36088 00:00&lt;00:29]      In\u00a0[8]: Copied! <pre>global_eff_shapley = msa.get_shapley_table(contributions=global_eff, permutation_space=node_perms)\nglobal_eff_shapley = ut.sorter(global_eff_shapley)\nglobal_eff_shapley.head()\n</pre> global_eff_shapley = msa.get_shapley_table(contributions=global_eff, permutation_space=node_perms) global_eff_shapley = ut.sorter(global_eff_shapley) global_eff_shapley.head() Out[8]: 37 22 27 26 35 19 21 39 17 18 ... 7 12 4 9 6 5 2 0 1 3 0 -0.008062 -0.005808 0.001170 0.007937 -0.008140 0.006794 0.001562 -0.019444 -0.010490 -0.013636 ... 0.000631 0.030357 0.026218 0.000000 0.000000 0.043039 0.112287 0.004040 0.022421 0.000000 1 -0.000074 -0.001991 -0.001687 0.000252 -0.005491 -0.022073 -0.166667 0.015152 -0.001532 -0.066667 ... -0.023810 0.000000 0.026345 0.000000 0.037694 0.011905 0.063440 0.159722 0.004579 1.000000 2 -0.014828 -0.004919 0.000709 0.000050 -0.012646 0.001495 0.005898 -0.022533 0.000900 0.006291 ... -0.047619 0.036062 0.036170 -0.066667 0.027778 0.066667 0.105047 0.000000 1.000000 0.017593 3 0.000916 0.000000 -0.001076 -0.005931 -0.000713 0.003239 0.004179 0.001333 -0.005848 0.000000 ... 0.119048 -0.019841 -0.013889 0.005700 0.000000 0.020759 0.010975 0.143057 0.012573 0.030157 4 -0.000721 -0.000696 -0.001950 -0.010490 -0.001991 0.001197 0.007597 -0.000378 0.000000 0.000000 ... 0.015544 0.016739 0.021936 0.000000 0.027778 0.038660 0.077008 0.053453 0.019192 0.018672 <p>5 rows \u00d7 40 columns</p> In\u00a0[9]: Copied! <pre>intact_global_eff = nx.global_efficiency(G)\nd_global_eff = ut.distribution_of_processing(shapley_vector=global_eff_shapley.mean())\n</pre> intact_global_eff = nx.global_efficiency(G) d_global_eff = ut.distribution_of_processing(shapley_vector=global_eff_shapley.mean())  In\u00a0[10]: Copied! <pre>fig,ax = plt.subplots()\nsns.barplot(data=global_eff_shapley, ax=ax, errorbar=('ci', 95), orient=  \"v\", err_kws={'color': 'k'})\nfig.set_dpi(150)\nfig.set_size_inches((21*CM,5*CM))\nplt.text(0.5, 0.04,f'Intact global efficiency: {intact_global_eff:.2f}')\nplt.text(0.5, 0.03,f'Distribution of process: {d_global_eff:.4f}')\nplt.xticks(fontsize=8)\nplt.title(\"Contribution of each node to the Global efficiency\")\nplt.savefig(f\"{FIGPATH}global_efficiency.pdf\",dpi=300,bbox_inches='tight')\n</pre> fig,ax = plt.subplots() sns.barplot(data=global_eff_shapley, ax=ax, errorbar=('ci', 95), orient=  \"v\", err_kws={'color': 'k'}) fig.set_dpi(150) fig.set_size_inches((21*CM,5*CM)) plt.text(0.5, 0.04,f'Intact global efficiency: {intact_global_eff:.2f}') plt.text(0.5, 0.03,f'Distribution of process: {d_global_eff:.4f}') plt.xticks(fontsize=8) plt.title(\"Contribution of each node to the Global efficiency\") plt.savefig(f\"{FIGPATH}global_efficiency.pdf\",dpi=300,bbox_inches='tight') <p>Voila! Minimal example. p.s: Sum of all the shapley values will add up to the value you get if you run the analysis on the intact network (grand coalition). It's a nice sanity check. Here:</p> In\u00a0[11]: Copied! <pre>print(intact_global_eff-global_eff_shapley.mean().sum())\n</pre> print(intact_global_eff-global_eff_shapley.mean().sum()) <pre>-1.6653345369377348e-16\n</pre> <p>Of course this might not be the case if there are stochasticity in the results. But in that case too, it should not be very far. Anyways, here's the same graph but each node is now colored by its contribution to the global efficiency.</p> In\u00a0[12]: Copied! <pre>fig,ax = plt.subplots()\nfig.set_dpi(150)\nfig.set_size_inches((12*CM,12*CM))\nax = nx.draw_networkx(G,with_labels=True,font_size=8)\nplt.savefig(f\"{FIGPATH}balanced_tree_colorcoded.pdf\",dpi=300,bbox_inches='tight')\n</pre> fig,ax = plt.subplots() fig.set_dpi(150) fig.set_size_inches((12*CM,12*CM)) ax = nx.draw_networkx(G,with_labels=True,font_size=8) plt.savefig(f\"{FIGPATH}balanced_tree_colorcoded.pdf\",dpi=300,bbox_inches='tight')"},{"location":"examples/minimal.html#minimal-examples-of-how-to-use-msa","title":"Minimal examples of how to use MSA\u00b6","text":""},{"location":"examples/minimal.html#background","title":"Background\u00b6","text":""},{"location":"examples/minimal.html#defining-the-network-and-the-game","title":"Defining the network and the game\u00b6","text":""},{"location":"examples/minimal.html#msa-in-action","title":"MSA in action\u00b6","text":"<p>Briefly, we'll estimate shapley values by first permuting the elements N times, then producing an instruction for which combinations to lesion.</p>"},{"location":"examples/mnist_torch.html","title":"Mnist torch","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># Imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n# ---------\nfrom msapy import msa, utils as ut\n# ---------\nfrom functools import partial\nfrom typing import Union, Optional, List\n\nCM = 1 / 2.54\nSEED = 42\nRNG = np.random.default_rng(SEED)\nFIGPATH = \"figures/mnist/\"\n</pre> # Imports import matplotlib.pyplot as plt import numpy as np import seaborn as sns # --------- from msapy import msa, utils as ut # --------- from functools import partial from typing import Union, Optional, List  CM = 1 / 2.54 SEED = 42 RNG = np.random.default_rng(SEED) FIGPATH = \"figures/mnist/\" In\u00a0[3]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\n\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n</pre> import torch import torch.nn as nn import torch.optim as optim import torchvision.transforms as T  from torch.utils.data import DataLoader, TensorDataset from torchvision import datasets from sklearn.metrics import accuracy_score, confusion_matrix In\u00a0[4]: Copied! <pre># Device configuration\ndevice = torch.device('cpu')\ndevice\n</pre> # Device configuration device = torch.device('cpu') device Out[4]: <pre>device(type='cpu')</pre> In\u00a0[5]: Copied! <pre>mnist_transforms = T.Lambda(lambda x: torch.flatten(x, 1)/255)\n\ntrain_data = datasets.MNIST(\n    root = 'data',\n    train = True,                         \n    transform = mnist_transforms, \n    download = True,            \n)\n\ntest_data = datasets.MNIST(\n    root = 'data', \n    train = False, \n    transform = mnist_transforms\n)\n\ntrain_data = TensorDataset(mnist_transforms(train_data.data.to(device)), train_data.targets.to(device))\ntest_data = TensorDataset(mnist_transforms(test_data.data.to(device)), test_data.targets.to(device))\n</pre> mnist_transforms = T.Lambda(lambda x: torch.flatten(x, 1)/255)  train_data = datasets.MNIST(     root = 'data',     train = True,                              transform = mnist_transforms,      download = True,             )  test_data = datasets.MNIST(     root = 'data',      train = False,      transform = mnist_transforms )  train_data = TensorDataset(mnist_transforms(train_data.data.to(device)), train_data.targets.to(device)) test_data = TensorDataset(mnist_transforms(test_data.data.to(device)), test_data.targets.to(device)) In\u00a0[6]: Copied! <pre>trainloader = DataLoader(train_data, \n                        batch_size=64, \n                        shuffle=True)\n    \ntestloader = DataLoader(test_data, \n                        batch_size=1024)\n</pre> trainloader = DataLoader(train_data,                          batch_size=64,                          shuffle=True)      testloader = DataLoader(test_data,                          batch_size=1024) In\u00a0[7]: Copied! <pre>class MNISTNet(nn.Module):\n    def __init__(self):\n        super(MNISTNet, self).__init__()\n        self.layer1 = nn.Linear(28*28, 32)\n        self.layer2 = nn.Sequential(nn.LeakyReLU(),\n                                    nn.Linear(32, 10))\n\n    def forward(self, x: torch.Tensor, lesion_idx: Optional[Union[int, List[int]]] = None) -&gt; torch.Tensor:\n        \"\"\"forward function to calculate the scores for each class\n\n        Args:\n            x (torch.Tensor): data of shape [batch_size, 28*28]\n            lesion_idx (Optional[Union[int,List[int]]], optional): the neuron that we want to lesion in the hidden layer1. Defaults to None i.e. no lesioning performed.\n\n        Returns:\n            torch.Tensor: scores for each class\n        \"\"\"\n        out = self.layer1(x)\n\n        if lesion_idx:\n            out[:, lesion_idx] = 0  # set the value to 0 for the lesioned neuron\n\n        return self.layer2(out)\n\n\nmodel = MNISTNet().to(device)\n</pre> class MNISTNet(nn.Module):     def __init__(self):         super(MNISTNet, self).__init__()         self.layer1 = nn.Linear(28*28, 32)         self.layer2 = nn.Sequential(nn.LeakyReLU(),                                     nn.Linear(32, 10))      def forward(self, x: torch.Tensor, lesion_idx: Optional[Union[int, List[int]]] = None) -&gt; torch.Tensor:         \"\"\"forward function to calculate the scores for each class          Args:             x (torch.Tensor): data of shape [batch_size, 28*28]             lesion_idx (Optional[Union[int,List[int]]], optional): the neuron that we want to lesion in the hidden layer1. Defaults to None i.e. no lesioning performed.          Returns:             torch.Tensor: scores for each class         \"\"\"         out = self.layer1(x)          if lesion_idx:             out[:, lesion_idx] = 0  # set the value to 0 for the lesioned neuron          return self.layer2(out)   model = MNISTNet().to(device)  In\u00a0[8]: Copied! <pre>loss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = 0.01)\n</pre> loss_func = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr = 0.01) In\u00a0[9]: Copied! <pre>for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 400 == 399:    # print every 200 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 400:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n</pre> for epoch in range(2):  # loop over the dataset multiple times      running_loss = 0.0     for i, data in enumerate(trainloader, 0):         # get the inputs; data is a list of [inputs, labels]         inputs, labels = data         # zero the parameter gradients         optimizer.zero_grad()          # forward + backward + optimize         outputs = model(inputs)         loss = loss_func(outputs, labels)         loss.backward()         optimizer.step()          # print statistics         running_loss += loss.item()         if i % 400 == 399:    # print every 200 mini-batches             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 400:.3f}')             running_loss = 0.0  print('Finished Training')  <pre>/home/sdixit/miniconda3/envs/msapy/lib/python3.8/site-packages/torch/autograd/__init__.py:173: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484657607/work/c10/cuda/CUDAFunctions.cpp:109.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n</pre> <pre>[1,   400] loss: 0.343\n[1,   800] loss: 0.215\n[2,   400] loss: 0.157\n[2,   800] loss: 0.160\nFinished Training\n</pre> In\u00a0[10]: Copied! <pre>@torch.no_grad()\ndef evaluate_model(lesion_idx: Optional[Union[int, List[int]]] = None, num_batches: int = -1, score_fn=accuracy_score):\n    \"\"\"return the accuracy of the model on test dataset\n\n    Args:\n        lesion_idx (Optional[Union[int,List[int]]], optional): the neuron that we want to lesion in the hidden layer1. Defaults to None i.e. no lesioning performed.\n        num_batches (int, optional): the number of batches we want to test our model. Defaults to -1 i.e. all data\n\n    Returns:\n        float: test accuracy\n    \"\"\"\n\n    targets = []\n    preds = []\n    for i, data in enumerate(testloader):\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = model(images, lesion_idx)\n        # the class with the highest score is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        preds.append(predicted)\n        targets.append(labels)\n\n        if i == (num_batches-1):\n            break\n\n    return score_fn(torch.concat(preds).cpu(), torch.concat(targets).cpu())\n\ndef accuracy_each_class(targets, preds):\n    matrix = confusion_matrix(targets, preds)\n    acc = []\n    for i, val in enumerate(matrix.diagonal()/(matrix.sum(axis=1)+ 1e-6)):\n        acc.append(val)\n\n    return np.array(acc)\n</pre> @torch.no_grad() def evaluate_model(lesion_idx: Optional[Union[int, List[int]]] = None, num_batches: int = -1, score_fn=accuracy_score):     \"\"\"return the accuracy of the model on test dataset      Args:         lesion_idx (Optional[Union[int,List[int]]], optional): the neuron that we want to lesion in the hidden layer1. Defaults to None i.e. no lesioning performed.         num_batches (int, optional): the number of batches we want to test our model. Defaults to -1 i.e. all data      Returns:         float: test accuracy     \"\"\"      targets = []     preds = []     for i, data in enumerate(testloader):         images, labels = data         # calculate outputs by running images through the network         outputs = model(images, lesion_idx)         # the class with the highest score is what we choose as prediction         _, predicted = torch.max(outputs.data, 1)         preds.append(predicted)         targets.append(labels)          if i == (num_batches-1):             break      return score_fn(torch.concat(preds).cpu(), torch.concat(targets).cpu())  def accuracy_each_class(targets, preds):     matrix = confusion_matrix(targets, preds)     acc = []     for i, val in enumerate(matrix.diagonal()/(matrix.sum(axis=1)+ 1e-6)):         acc.append(val)      return np.array(acc)      In\u00a0[11]: Copied! <pre>print(f\"the accuracy of the model on the first batch without leasoning is: {evaluate_model(num_batches=1)}\")\n</pre> print(f\"the accuracy of the model on the first batch without leasoning is: {evaluate_model(num_batches=1)}\") <pre>the accuracy of the model on the first batch without leasoning is: 0.9580078125\n</pre> In\u00a0[12]: Copied! <pre>ground_truth_elements = list(range(32)) #Indices for the neurons in the hidden layer\nprint(f'total number of possible lesions: {2**len(ground_truth_elements)}')\n</pre> ground_truth_elements = list(range(32)) #Indices for the neurons in the hidden layer print(f'total number of possible lesions: {2**len(ground_truth_elements)}') <pre>total number of possible lesions: 4294967296\n</pre> <p>Now we perform the MSA to calculate the contributions for each neuron</p> <p>Note: You might want to change the device to cpu while running MSA if you have to load the data into the GPU at every iteration. This could become a bottleneck. If your data is already on the gpu than it's not a problem</p> In\u00a0[13]: Copied! <pre>shapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1000, # might want to increase it for better results\n    objective_function=partial(evaluate_model, score_fn=accuracy_score, num_batches=1), #only the first batch to save time. But the batch size is 1024 i.e ~100 images per class\n    rng=RNG)\n</pre> shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1000, # might want to increase it for better results     objective_function=partial(evaluate_model, score_fn=accuracy_score, num_batches=1), #only the first batch to save time. But the batch size is 1024 i.e ~100 images per class     rng=RNG) In\u00a0[14]: Copied! <pre>shapley_table.plot_shapley_ranks(150, xlabel=\"Shapley values\", ylabel=\"Elements\",\n                                 title=\"Shapley values for neurons in hidden layer\", savepath=f\"{FIGPATH}Shapley.pdf\")\n</pre> shapley_table.plot_shapley_ranks(150, xlabel=\"Shapley values\", ylabel=\"Elements\",                                  title=\"Shapley values for neurons in hidden layer\", savepath=f\"{FIGPATH}Shapley.pdf\") <p>We can also analyse the contributions of each neuron w.r.t each class by changing the objective function to return a dictionary of accuracies instead of the average accuracy for the whole dataset. Notice that the objective function also returns a dictionary of shapley tables in case the objective function returns a dictionary.</p> In\u00a0[15]: Copied! <pre>shapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1000, # might want to increase it for better results\n    objective_function=partial(evaluate_model, score_fn=accuracy_each_class, num_batches=1), #only the first batch to save time. But the batch size is 1024 i.e ~100 images per class\n    n_parallel_games=-1, #parallelized over all CPU cores\n    rng=RNG)\n</pre> shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1000, # might want to increase it for better results     objective_function=partial(evaluate_model, score_fn=accuracy_each_class, num_batches=1), #only the first batch to save time. But the batch size is 1024 i.e ~100 images per class     n_parallel_games=-1, #parallelized over all CPU cores     rng=RNG) In\u00a0[24]: Copied! <pre>shapley_table_digit0 = shapley_table.iloc[0]\n\nshapley_table_digit0.plot(kind='barh', figsize=(15,8), xlabel=\"Shapley values\", ylabel=\"Elements\",\n                                 title=\"Shapley values for neurons in hidden layer\")\n</pre> shapley_table_digit0 = shapley_table.iloc[0]  shapley_table_digit0.plot(kind='barh', figsize=(15,8), xlabel=\"Shapley values\", ylabel=\"Elements\",                                  title=\"Shapley values for neurons in hidden layer\") Out[24]: <pre>&lt;AxesSubplot:title={'center':'Shapley values for neurons in hidden layer'}, ylabel='Shapley values'&gt;</pre> In\u00a0[25]: Copied! <pre>shapley_table_digit8 = shapley_table.iloc[8]\n\nshapley_table_digit8.plot(kind='barh', figsize=(15,8), xlabel=\"Shapley values\", ylabel=\"Elements\",\n                                 title=\"Shapley values for neurons in hidden layer\")\n</pre> shapley_table_digit8 = shapley_table.iloc[8]  shapley_table_digit8.plot(kind='barh', figsize=(15,8), xlabel=\"Shapley values\", ylabel=\"Elements\",                                  title=\"Shapley values for neurons in hidden layer\") Out[25]: <pre>&lt;AxesSubplot:title={'center':'Shapley values for neurons in hidden layer'}, ylabel='Shapley values'&gt;</pre>"},{"location":"examples/mnist_torch.html#msa-on-pytorch-network-trained-on-mnist","title":"MSA on PyTorch Network trained on MNIST\u00b6","text":"<p>This is an example of how we can perform \"Multiperturbation Shapley value Analysis\" on a PyTorch neural network. We train a three layer [input, hidden, output] network with 32 neurons in the hidden layer. We use MSA to analyse the contribution of each neuron in the hidden layer in accurately predicting the classes.</p>"},{"location":"examples/mnist_torch.html#loading-data","title":"Loading Data\u00b6","text":""},{"location":"examples/mnist_torch.html#model-definition","title":"Model Definition\u00b6","text":""},{"location":"examples/mnist_torch.html#training-the-model","title":"Training the Model\u00b6","text":""},{"location":"examples/mnist_torch.html#msa","title":"MSA\u00b6","text":""},{"location":"examples/on%20ground-truth%20models.html","title":"Working with groundtruth models","text":"In\u00a0[1]: Copied! <pre># Uncomment the line below if you don't have them.\n# !pip install networkx matplotlib seaborn\n</pre> # Uncomment the line below if you don't have them. # !pip install networkx matplotlib seaborn In\u00a0[2]: Copied! <pre># Imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n# ---------\nfrom msapy import msa, utils as ut\n# ---------\nimport matplotlib as mpl\n\nmpl.rcParams['pdf.fonttype'] = 42\nmpl.rcParams['font.size'] = 8\nCM = 1 / 2.54\nSEED = 111\nRNG = np.random.default_rng(SEED)\nFIGPATH = \"figures/gt/\"\n</pre> # Imports import matplotlib.pyplot as plt import numpy as np import seaborn as sns # --------- from msapy import msa, utils as ut # --------- import matplotlib as mpl  mpl.rcParams['pdf.fonttype'] = 42 mpl.rcParams['font.size'] = 8 CM = 1 / 2.54 SEED = 111 RNG = np.random.default_rng(SEED) FIGPATH = \"figures/gt/\" <pre>/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version &gt;=1.18.5 and &lt;1.25.0 is required for this version of SciPy (detected version 1.26.3\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n</pre> In\u00a0[3]: Copied! <pre>ground_truth_elements = ['a','b','c','d','e','f','g','h'] # our brain regions, for example!\nprint(f'total number of possible lesions: {2**len(ground_truth_elements)}')\n</pre> ground_truth_elements = ['a','b','c','d','e','f','g','h'] # our brain regions, for example! print(f'total number of possible lesions: {2**len(ground_truth_elements)}') <pre>total number of possible lesions: 256\n</pre> <p>As you can see, the number of lesions are not \"that large\" so we can exhauste the combination space.</p> In\u00a0[4]: Copied! <pre>ground_truth_cause = ['a'] # the cognitive function is the product of just one element 'a'\n</pre> ground_truth_cause = ['a'] # the cognitive function is the product of just one element 'a' In\u00a0[5]: Copied! <pre>def gt(complements, causes):\n    # default score, on average, is 100.\n    score = RNG.normal(loc=100, scale=10)\n    \n    # checking if the regions of interest are being lesioned.\n    if len(causes) != 0 and set(causes).issubset(complements):\n        \n        # lesioning ends with a reduction of 50 points.\n        return score - 50 \n    else:\n        return score\n</pre> def gt(complements, causes):     # default score, on average, is 100.     score = RNG.normal(loc=100, scale=10)          # checking if the regions of interest are being lesioned.     if len(causes) != 0 and set(causes).issubset(complements):                  # lesioning ends with a reduction of 50 points.         return score - 50      else:         return score In\u00a0[6]: Copied! <pre>shapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1000,\n    objective_function=gt,\n    n_parallel_games=-1, #parallelized over all CPU cores\n    objective_function_params={'causes': ground_truth_cause},\n    rng=RNG)\n</pre> shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1000,     objective_function=gt,     n_parallel_games=-1, #parallelized over all CPU cores     objective_function_params={'causes': ground_truth_cause},     rng=RNG)        6.18% [61/987 00:00&lt;00:05]             0.00% [0/8 00:00&lt;?]      <p>To quantify how much the function is distributed in our system, we can use this nifty function called \"distribution_of_processing\" in the utils module. You can read more about it here:</p> <ul> <li>Aharonov, R., Segev, L., Meilijson, I., &amp; Ruppin, E. 2003. Localization of function via lesion analysis. Neural Computation.</li> <li>Saggie-Wexler, Keren, Alon Keinan, and Eytan Ruppin. 2006. Neural Processing of Counting in Evolved Spiking and McCulloch-Pitts Agents. Artificial Life.</li> </ul> <p>But the basic idea is to see how many units are involved in producing the function, one way or another, so negative Shapley values (interpreted as hinderace) still counts. If the value is around zero, it means a localized function, which we expect here, and if it's around one, it suggests a distributed process. So far I couldn't get anything really around zero and I assume it's because of the noise in the datasets. I think it needs some rescaling but generally it's a nice metric and still, you can compare two values to see which process is more distributed and how much.</p> In\u00a0[7]: Copied! <pre>d = ut.distribution_of_processing(shapley_vector=shapley_table.mean())\nprint(f'D index is: {d}')\n</pre> d = ut.distribution_of_processing(shapley_vector=shapley_table.mean()) print(f'D index is: {d}') <pre>D index is: 0.19889855206048468\n</pre> In\u00a0[8]: Copied! <pre>shapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)\n\nfig,ax = plt.subplots()\nsns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'})\nfig.set_dpi(154)\nfig.set_size_inches((4*CM,6*CM))\nplt.xlabel('Shapley values')\nplt.ylabel('Elements')\nplt.title('Shapley values of a ground-truth dataset\\nwith only 1 critical element')\nplt.savefig(f\"{FIGPATH}1critical.pdf\",dpi=300,bbox_inches='tight')\n</pre> shapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)  fig,ax = plt.subplots() sns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'}) fig.set_dpi(154) fig.set_size_inches((4*CM,6*CM)) plt.xlabel('Shapley values') plt.ylabel('Elements') plt.title('Shapley values of a ground-truth dataset\\nwith only 1 critical element') plt.savefig(f\"{FIGPATH}1critical.pdf\",dpi=300,bbox_inches='tight') <p>Here's an interesting thing, pay attention to the Shapley value, it's around 50, which is the same number we mentioned for the performance deficit.</p> In\u00a0[9]: Copied! <pre>ground_truth_cause = ['a','b','c']\n\n# The same call here:\nshapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1_000,\n    objective_function=gt,\n    n_parallel_games=-1, #parallelized over all CPU cores\n    objective_function_params={'causes': ground_truth_cause},\n    rng=RNG)\n</pre> ground_truth_cause = ['a','b','c']  # The same call here: shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1_000,     objective_function=gt,     n_parallel_games=-1, #parallelized over all CPU cores     objective_function_params={'causes': ground_truth_cause},     rng=RNG)        18.03% [178/987 00:01&lt;00:04]             50.00% [4/8 00:00&lt;00:00]      In\u00a0[10]: Copied! <pre># The same pipeline here:\nshapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)\n\nfig,ax = plt.subplots()\nsns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'})\nfig.set_dpi(154)\nfig.set_size_inches((4*CM,6*CM))\n\nplt.xlabel('Shapley values')\nplt.ylabel('Elements')\nplt.title('Shapley values of a ground-truth dataset\\nwith three critical elements')\nplt.savefig(f\"{FIGPATH}3critical.pdf\",dpi=300,bbox_inches='tight')\n\n# And the D index\nd = ut.distribution_of_processing(shapley_vector=shapley_table.mean())\nprint(f'D intex is: {d}')\n</pre> # The same pipeline here: shapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)  fig,ax = plt.subplots() sns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'}) fig.set_dpi(154) fig.set_size_inches((4*CM,6*CM))  plt.xlabel('Shapley values') plt.ylabel('Elements') plt.title('Shapley values of a ground-truth dataset\\nwith three critical elements') plt.savefig(f\"{FIGPATH}3critical.pdf\",dpi=300,bbox_inches='tight')  # And the D index d = ut.distribution_of_processing(shapley_vector=shapley_table.mean()) print(f'D intex is: {d}') <pre>D intex is: 0.5432645447465022\n</pre> <p>Again, the interesting point is Shapley values roughly correspond to 50/3, which is pretty neat.</p> In\u00a0[11]: Copied! <pre>ground_truth_cause = ['a','b','c','d','e','f','g'] #only 'h' is out.\n\n# The same call here:\nshapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1_000,\n    objective_function=gt,\n    n_parallel_games=-1, #parallelized over all CPU cores\n    objective_function_params={'causes': ground_truth_cause},\n    rng=RNG)\n</pre> ground_truth_cause = ['a','b','c','d','e','f','g'] #only 'h' is out.  # The same call here: shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1_000,     objective_function=gt,     n_parallel_games=-1, #parallelized over all CPU cores     objective_function_params={'causes': ground_truth_cause},     rng=RNG)        44.43% [439/988 00:02&lt;00:02]             25.00% [2/8 00:00&lt;00:00]      In\u00a0[12]: Copied! <pre># The same pipeline here:\nshapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)\n\nfig,ax = plt.subplots()\nsns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'})\nfig.set_dpi(154)\nfig.set_size_inches((4*CM,6*CM))\nplt.xlabel('Shapley values')\nplt.ylabel('Elements')\nplt.title('Shapley values of a ground-truth dataset\\nwith only one non-critical element')\nplt.savefig(f\"{FIGPATH}1noncritical.pdf\",dpi=300,bbox_inches='tight')\n\n# And the D index\nd = ut.distribution_of_processing(shapley_vector=shapley_table.mean())\nprint(f'D intex is: {d}')\n</pre> # The same pipeline here: shapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)  fig,ax = plt.subplots() sns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'}) fig.set_dpi(154) fig.set_size_inches((4*CM,6*CM)) plt.xlabel('Shapley values') plt.ylabel('Elements') plt.title('Shapley values of a ground-truth dataset\\nwith only one non-critical element') plt.savefig(f\"{FIGPATH}1noncritical.pdf\",dpi=300,bbox_inches='tight')  # And the D index d = ut.distribution_of_processing(shapley_vector=shapley_table.mean()) print(f'D intex is: {d}') <pre>D intex is: 0.8630240119335526\n</pre> <p>You see, still it captured the critical elements, but the importance is not equally distributed. I believe it's due to two things:</p> <ol> <li>Noise in the performance.</li> <li>Noise in the estimation.</li> </ol> <p>Let's get rid of the first and see if it makes a difference.</p> In\u00a0[13]: Copied! <pre>def gt_noisless(complements, causes):\n    # default score, will be 100 sharp!\n    score = 100\n    \n    # checking if the regions of interest are being lesioned.\n    if len(causes) != 0 and set(causes).issubset(complements):\n        \n        # lesioning ends with a reduction of 50 points.\n        return score - 50 \n    else:\n        return score\n</pre> def gt_noisless(complements, causes):     # default score, will be 100 sharp!     score = 100          # checking if the regions of interest are being lesioned.     if len(causes) != 0 and set(causes).issubset(complements):                  # lesioning ends with a reduction of 50 points.         return score - 50      else:         return score In\u00a0[14]: Copied! <pre>ground_truth_cause = ['a','b','c','d','e','f','g'] #only 'h' is out.\n\n# The same call here:\nshapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1_000,\n    objective_function=gt_noisless,\n    n_parallel_games=-1, #parallelized over all CPU cores\n    objective_function_params={'causes': ground_truth_cause},\n    rng=RNG)\n</pre> ground_truth_cause = ['a','b','c','d','e','f','g'] #only 'h' is out.  # The same call here: shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1_000,     objective_function=gt_noisless,     n_parallel_games=-1, #parallelized over all CPU cores     objective_function_params={'causes': ground_truth_cause},     rng=RNG)        14.89% [147/987 00:00&lt;00:04]             50.00% [4/8 00:00&lt;00:00]      In\u00a0[15]: Copied! <pre># The same pipeline here:\nshapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)\n\nfig,ax = plt.subplots()\nsns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'})\nfig.set_dpi(154)\nfig.set_size_inches((4*CM,6*CM))\nplt.xlabel('Shapley values')\nplt.ylabel('Elements')\nplt.title('Shapley values of a ground-truth dataset with only one non-critical element')\nplt.savefig(f\"{FIGPATH}1noncritical_noiseless.pdf\",dpi=300,bbox_inches='tight')\n\n# And the D index\nd = ut.distribution_of_processing(shapley_vector=shapley_table.mean())\nprint(f'D intex is: {d}')\n</pre> # The same pipeline here: shapley_table = ut.sorter(shapley_table) # sorting based on the average contribution (Shapley values)  fig,ax = plt.subplots() sns.barplot(data=shapley_table, ax=ax, errorbar=('ci', 95), orient=  \"h\", err_kws={'color': 'k'}) fig.set_dpi(154) fig.set_size_inches((4*CM,6*CM)) plt.xlabel('Shapley values') plt.ylabel('Elements') plt.title('Shapley values of a ground-truth dataset with only one non-critical element') plt.savefig(f\"{FIGPATH}1noncritical_noiseless.pdf\",dpi=300,bbox_inches='tight')  # And the D index d = ut.distribution_of_processing(shapley_vector=shapley_table.mean()) print(f'D intex is: {d}') <pre>D intex is: 0.8551934592118597\n</pre> <p>Much better! So here's the moral, in case you missed it: The noisier the data, the less accurate Shapley values we will be. What is nice tho is the fact that even with noisy data, the ranking still makes sense. Now let's see if it scales to a large system. We'll use the same logic but on a system with 500 elements. Let's see if we can localize \"the\" element first.</p> <p>WARNING: This step needs about 40GB of RAM! I need to fix this later probably with dask arrays or something else. let me know if you have a solution. Also, this step shows that in principle, a large number of elements doesn't make the calculation unstable (but look below for a caveat), it makes the process computationally prohibitive. I mean, my task takes a few miliseconds but imagine a task that costs a second or two!</p> In\u00a0[16]: Copied! <pre>ground_truth_elements = list(range(500))\nlen(ground_truth_elements)\n</pre> ground_truth_elements = list(range(500)) len(ground_truth_elements) Out[16]: <pre>500</pre> In\u00a0[17]: Copied! <pre>ground_truth_cause = [100] # element number 100 is the sole producer of the function\n</pre> ground_truth_cause = [100] # element number 100 is the sole producer of the function In\u00a0[18]: Copied! <pre># Everything else is the same, again.\nshapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1_000,\n    objective_function=gt,\n    n_parallel_games=1, # somehow all over one core is faster!\n    objective_function_params={'causes': ground_truth_cause},\n    rng=RNG)\n</pre> # Everything else is the same, again. shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1_000,     objective_function=gt,     n_parallel_games=1, # somehow all over one core is faster!     objective_function_params={'causes': ground_truth_cause},     rng=RNG) <p>This time we don't sort by importance but we still need to sort in ascending order to plot, in the dataframe everything is in its place but the positions are shuffled in the process or calculating Shapley values so I might later add the <code>shapley_table.sort_index(axis=1)</code> to the package itself but for now, we'll be doing it here. We'll use scatter and line plots since barplots of this size will be very ugly.</p> In\u00a0[19]: Copied! <pre>shapley_table = shapley_table.sort_index(axis=1)\n\n# these are my custom color palette btw but feel free to use it. It's colorblind friendly.\ncolor = sns.blend_palette(['#006685', '#3FA5C4', '#FFE48D', '#E84653', '#BF003F'],\n                          len(ground_truth_elements),\n                          as_cmap= True) \n\nplt.figure(figsize=(21*CM,5*CM),dpi = 154)\nplt.plot(np.arange(len(ground_truth_elements)),\n         shapley_table.mean(),\n         c='k',linewidth=1,alpha=0.1)\n\nplt.scatter(x = np.arange(len(ground_truth_elements)),\n            y = shapley_table.mean(),\n            c = shapley_table.mean(),\n            s = shapley_table.mean().abs()*10,\n            cmap = color)\n\nplt.axhline(linewidth=1, color='#BF003F')\nplt.xlabel('Elements (Indices)')\nplt.ylabel('Shapley values')\nplt.title('Shapley values of a ground-truth dataset with 1 critical elements')\nplt.savefig(f\"{FIGPATH}scaled1critical.pdf\",dpi=300,bbox_inches='tight')\n\n\n# And the D index\nd = ut.distribution_of_processing(shapley_vector=shapley_table.mean())\nprint(f'D intex is: {d}')\n</pre> shapley_table = shapley_table.sort_index(axis=1)  # these are my custom color palette btw but feel free to use it. It's colorblind friendly. color = sns.blend_palette(['#006685', '#3FA5C4', '#FFE48D', '#E84653', '#BF003F'],                           len(ground_truth_elements),                           as_cmap= True)   plt.figure(figsize=(21*CM,5*CM),dpi = 154) plt.plot(np.arange(len(ground_truth_elements)),          shapley_table.mean(),          c='k',linewidth=1,alpha=0.1)  plt.scatter(x = np.arange(len(ground_truth_elements)),             y = shapley_table.mean(),             c = shapley_table.mean(),             s = shapley_table.mean().abs()*10,             cmap = color)  plt.axhline(linewidth=1, color='#BF003F') plt.xlabel('Elements (Indices)') plt.ylabel('Shapley values') plt.title('Shapley values of a ground-truth dataset with 1 critical elements') plt.savefig(f\"{FIGPATH}scaled1critical.pdf\",dpi=300,bbox_inches='tight')   # And the D index d = ut.distribution_of_processing(shapley_vector=shapley_table.mean()) print(f'D intex is: {d}') <pre>D intex is: 0.7791750779196088\n</pre> <p>Works well, what doesn't work well is the D index! For sure it needs some adjustments to be more robust to the noise. But let's see if we can capture a function that is distributed over 100 nodes.</p> In\u00a0[20]: Copied! <pre>ground_truth_cause = list(range(100,200))\n\nshapley_table = msa.interface(\n    elements=ground_truth_elements,\n    n_permutations=1_000,\n    objective_function=gt,\n    n_parallel_games=1, \n    objective_function_params={'causes': ground_truth_cause},\n    rng=RNG)\n\nshapley_table = shapley_table.sort_index(axis=1)\n</pre> ground_truth_cause = list(range(100,200))  shapley_table = msa.interface(     elements=ground_truth_elements,     n_permutations=1_000,     objective_function=gt,     n_parallel_games=1,      objective_function_params={'causes': ground_truth_cause},     rng=RNG)  shapley_table = shapley_table.sort_index(axis=1) In\u00a0[21]: Copied! <pre>plt.figure(figsize=(21*CM,5*CM),dpi = 154)\n\nplt.plot(np.arange(len(ground_truth_elements)),\n         shapley_table.mean(),c='k',linewidth=1,alpha=0.1)\n\nplt.scatter(x = np.arange(len(ground_truth_elements)),\n            y = shapley_table.mean(),\n            c = shapley_table.mean(),\n            s = shapley_table.mean().abs()*20,\n            cmap = color)\n\nplt.axhline(linewidth=1, color='#BF003F')\nplt.xlabel('Elements (Indices)')\nplt.ylabel('Shapley values')\nplt.title('Shapley values of a ground-truth dataset with 100 critical elements')\nplt.savefig(f\"{FIGPATH}scaled100critical.pdf\",dpi=300,bbox_inches='tight')\n\n# And the D index\nd = ut.distribution_of_processing(shapley_vector=shapley_table.mean())\nprint(f'D intex is: {d}')\n</pre> plt.figure(figsize=(21*CM,5*CM),dpi = 154)  plt.plot(np.arange(len(ground_truth_elements)),          shapley_table.mean(),c='k',linewidth=1,alpha=0.1)  plt.scatter(x = np.arange(len(ground_truth_elements)),             y = shapley_table.mean(),             c = shapley_table.mean(),             s = shapley_table.mean().abs()*20,             cmap = color)  plt.axhline(linewidth=1, color='#BF003F') plt.xlabel('Elements (Indices)') plt.ylabel('Shapley values') plt.title('Shapley values of a ground-truth dataset with 100 critical elements') plt.savefig(f\"{FIGPATH}scaled100critical.pdf\",dpi=300,bbox_inches='tight')  # And the D index d = ut.distribution_of_processing(shapley_vector=shapley_table.mean()) print(f'D intex is: {d}') <pre>D intex is: 0.9456323166894177\n</pre> <p>As you can see, it didn't workout well and it actually makes sense, again remember the noise and remember that you're trying to distribute 50 points across 100 elements so although there's a noticable bump, the result is much more noisier. Let's try eliminating the performance noise.</p> In\u00a0[22]: Copied! <pre>shapley_table = msa.interface(\n   elements=ground_truth_elements,\n   n_permutations=1_000,\n   objective_function=gt_noisless,\n   n_parallel_games=1, # somehow all over one core is faster!\n   objective_function_params={'causes': ground_truth_cause})\n\nshapley_table = shapley_table.sort_index(axis=1)\n</pre> shapley_table = msa.interface(    elements=ground_truth_elements,    n_permutations=1_000,    objective_function=gt_noisless,    n_parallel_games=1, # somehow all over one core is faster!    objective_function_params={'causes': ground_truth_cause})  shapley_table = shapley_table.sort_index(axis=1) In\u00a0[23]: Copied! <pre>plt.figure(figsize=(21*CM,5*CM),dpi = 154)\n\ncolor = sns.blend_palette(['#FFE48D', '#E84653', '#BF003F'], # no negative Shapley values, no cold colors\n                          len(ground_truth_elements),\n                          as_cmap= True) \n\nplt.plot(np.arange(len(ground_truth_elements)),\n         shapley_table.mean(),c='k',linewidth=1,alpha=0.1)\n\nplt.scatter(x = np.arange(len(ground_truth_elements)),\n            y = shapley_table.mean(),\n            c = shapley_table.mean(),\n            s = shapley_table.mean().abs()*20,\n            cmap = color)\n\nplt.axhline(linewidth=1, color='#BF003F')\nplt.xlabel('Elements (Indices)')\nplt.ylabel('Shapley values')\nplt.title('Shapley values of a ground-truth dataset with 100 critical elements')\nplt.savefig(f\"{FIGPATH}scaled100critical_noiseless.pdf\",dpi=300,bbox_inches='tight')\n\n# And the D index\nd = ut.distribution_of_processing(shapley_vector=shapley_table.mean())\nprint(f'D intex is: {d}')\n</pre> plt.figure(figsize=(21*CM,5*CM),dpi = 154)  color = sns.blend_palette(['#FFE48D', '#E84653', '#BF003F'], # no negative Shapley values, no cold colors                           len(ground_truth_elements),                           as_cmap= True)   plt.plot(np.arange(len(ground_truth_elements)),          shapley_table.mean(),c='k',linewidth=1,alpha=0.1)  plt.scatter(x = np.arange(len(ground_truth_elements)),             y = shapley_table.mean(),             c = shapley_table.mean(),             s = shapley_table.mean().abs()*20,             cmap = color)  plt.axhline(linewidth=1, color='#BF003F') plt.xlabel('Elements (Indices)') plt.ylabel('Shapley values') plt.title('Shapley values of a ground-truth dataset with 100 critical elements') plt.savefig(f\"{FIGPATH}scaled100critical_noiseless.pdf\",dpi=300,bbox_inches='tight')  # And the D index d = ut.distribution_of_processing(shapley_vector=shapley_table.mean()) print(f'D intex is: {d}') <pre>D intex is: 0.9057781579939023\n</pre> <p>Interestingly, this number changes but follows a normal distribution. Look: Here I'll just use 20 elements and keep reproducing permutations and lesion combinations. Then I'll plot the histogram.</p> In\u00a0[24]: Copied! <pre>elements = list(range(20))\n\nlenths = []\nfor i in range(1000):\n    permutations = msa.make_permutation_space(elements=elements,n_permutations=1000)\n    combinations = msa.make_combination_space(permutation_space=permutations)\n    lenths.append(len(combinations))\n</pre> elements = list(range(20))  lenths = [] for i in range(1000):     permutations = msa.make_permutation_space(elements=elements,n_permutations=1000)     combinations = msa.make_combination_space(permutation_space=permutations)     lenths.append(len(combinations)) In\u00a0[25]: Copied! <pre>plt.figure(figsize=(8*CM,8*CM),dpi=154)\nsns.histplot(lenths,bins=20,color = \"#FFE48D\")\nplt.xticks(rotation=90)\nplt.xlabel('Number of coalitions '\n           '\\n(N permutations = 1000)')\nplt.title(f'Generated coalitions\\n(from {2**20} possible coalitions)')\nplt.savefig(f\"{FIGPATH}distribution_of_coalitions.pdf\",dpi=300,bbox_inches='tight')\n</pre> plt.figure(figsize=(8*CM,8*CM),dpi=154) sns.histplot(lenths,bins=20,color = \"#FFE48D\") plt.xticks(rotation=90) plt.xlabel('Number of coalitions '            '\\n(N permutations = 1000)') plt.title(f'Generated coalitions\\n(from {2**20} possible coalitions)') plt.savefig(f\"{FIGPATH}distribution_of_coalitions.pdf\",dpi=300,bbox_inches='tight')"},{"location":"examples/on%20ground-truth%20models.html#working-with-groundtruth-models","title":"Working with groundtruth models\u00b6","text":"<p>In this notebook, I'll quickly generate a simple function that represents a cognitive task. Then I check if MSA can find the important units, and how well it performs with more distributed processes and larger number of elements.</p>"},{"location":"examples/on%20ground-truth%20models.html#a-very-localized-function","title":"A very localized function\u00b6","text":"<p>The task is, well, basically just a random generator! The idea is to draw a number from a normal distribution around some value, let's say 100 with standard deviation of 10. Then condition on lesioning some arbitrary element, there will be a deficit. For simulating a very localized \"cognitive function\" we can pass one element as the only cause. Passing more elements means this cognitive function is distributed across these elements so naturally we expect those to have the highest contributions while the others settle around zero.</p>"},{"location":"examples/on%20ground-truth%20models.html#a-more-distributed-function","title":"A more distributed function\u00b6","text":"<p>Here, we say the cognitive function relies on let's say 3 regions, if \"all of these regions\" are perturbed then the performance will drop by 50 points. The function we made stays the same, only the causes need an adjustment.</p>"},{"location":"examples/on%20ground-truth%20models.html#a-totally-distributed-function","title":"A totally distributed function\u00b6","text":"<p>Yeah why not! let's say all except one unit is involved. Things will get tricky here and you'll see why!</p>"},{"location":"examples/on%20ground-truth%20models.html#scaling-up-to-500-elements","title":"Scaling up to 500 elements\u00b6","text":""},{"location":"examples/shapley%20analysis.html","title":"Work In Progress","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># Imports\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nimport seaborn as sns\n# ---------\nfrom msapy import msa, utils as ut\n# ---------\nfrom functools import partial\nfrom typing import Union, Optional, List\nfrom itertools import product\nfrom collections import defaultdict, Counter\n\nimport bct\nfrom netneurotools import cluster\nimport pandas as pd\n\nfrom IPython.display import HTML\n\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nCM = 1 / 2.54\nSEED = 42\nRNG = np.random.default_rng(SEED)\n</pre> # Imports import matplotlib.pyplot as plt import matplotlib.animation as animation import numpy as np import seaborn as sns # --------- from msapy import msa, utils as ut # --------- from functools import partial from typing import Union, Optional, List from itertools import product from collections import defaultdict, Counter  import bct from netneurotools import cluster import pandas as pd  from IPython.display import HTML  from sklearn.manifold import TSNE import seaborn as sns  CM = 1 / 2.54 SEED = 42 RNG = np.random.default_rng(SEED) <pre>/home/shrey/miniconda3/envs/msa/lib/python3.9/site-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n</pre> In\u00a0[3]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\nimport torchvision.datasets as dset\nimport torchvision.utils as vutils\nimport torchvision.models as models\n\n\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, AffinityPropagation\ntorch.manual_seed(SEED)\n</pre> import torch import torch.nn as nn import torch.optim as optim import torchvision.transforms as T import torchvision.datasets as dset import torchvision.utils as vutils import torchvision.models as models   from torch.utils.data import DataLoader, TensorDataset from torchvision import datasets from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.decomposition import PCA from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, AffinityPropagation torch.manual_seed(SEED) Out[3]: <pre>&lt;torch._C.Generator at 0x7fccec701730&gt;</pre> In\u00a0[4]: Copied! <pre>image_size = 128\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n</pre> image_size = 128  # Number of channels in the training images. For color images this is 3 nc = 3  # Size of z latent vector (i.e. size of generator input) nz = 100  # Size of feature maps in generator ngf = 64  # Size of feature maps in discriminator ndf = 64 In\u00a0[5]: Copied! <pre># Generator Code\nclass ConvTranspose2dLayer(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size = 4, stride = 1, padding = 0, output_padding = 0, groups: int = 1, bias: bool = True):\n        super(ConvTranspose2dLayer, self).__init__()\n        self.seq = nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n    \n    def forward(self, input):\n        return self.seq(input)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        first_layer = ConvTranspose2dLayer(nz, ngf * 16, 4, 1, 0, bias=False)\n        middle_layers = []\n        for i in range(4, 0, -1):\n            middle_layers.append(ConvTranspose2dLayer(ngf * 2**(i), ngf * 2**(i-1), 4, 2, 1, bias=False))\n        last_layer = nn.Sequential(nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False), nn.Sigmoid())\n        self.model = nn.Sequential(first_layer, *middle_layers, last_layer)\n\n    def forward(self, input, lesion_dict = None):\n        lesion_dict = lesion_dict if lesion_dict else defaultdict(list)\n        x = input\n        for i, layer in enumerate(self.model):\n            x = layer(x)\n            if i in lesion_dict:\n                x[:, lesion_dict[i]] = 0\n        \n        return x\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n</pre> # Generator Code class ConvTranspose2dLayer(nn.Module):     def __init__(self, in_channels: int, out_channels: int, kernel_size = 4, stride = 1, padding = 0, output_padding = 0, groups: int = 1, bias: bool = True):         super(ConvTranspose2dLayer, self).__init__()         self.seq = nn.Sequential(             nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias),             nn.BatchNorm2d(out_channels),             nn.ReLU(True),         )          def forward(self, input):         return self.seq(input)  class Generator(nn.Module):     def __init__(self):         super(Generator, self).__init__()          first_layer = ConvTranspose2dLayer(nz, ngf * 16, 4, 1, 0, bias=False)         middle_layers = []         for i in range(4, 0, -1):             middle_layers.append(ConvTranspose2dLayer(ngf * 2**(i), ngf * 2**(i-1), 4, 2, 1, bias=False))         last_layer = nn.Sequential(nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False), nn.Sigmoid())         self.model = nn.Sequential(first_layer, *middle_layers, last_layer)      def forward(self, input, lesion_dict = None):         lesion_dict = lesion_dict if lesion_dict else defaultdict(list)         x = input         for i, layer in enumerate(self.model):             x = layer(x)             if i in lesion_dict:                 x[:, lesion_dict[i]] = 0                  return x   class Discriminator(nn.Module):     def __init__(self):         super(Discriminator, self).__init__()         self.main = nn.Sequential(             # input is (nc) x 64 x 64             nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),             nn.LeakyReLU(0.2, inplace=True),                          nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf) x 32 x 32             nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf * 2),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf*2) x 16 x 16             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf * 4),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf*4) x 8 x 8             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),             nn.BatchNorm2d(ndf * 8),             nn.LeakyReLU(0.2, inplace=True),             # state size. (ndf*8) x 4 x 4             nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),             nn.Sigmoid()         )      def forward(self, input):         return self.main(input) In\u00a0[6]: Copied! <pre>netG = Generator()\nnetG.load_state_dict(torch.load(\"model_GAN_Face.pth\", map_location=torch.device('cpu')))\nnetG.eval();\n</pre> netG = Generator() netG.load_state_dict(torch.load(\"model_GAN_Face.pth\", map_location=torch.device('cpu'))) netG.eval(); In\u00a0[7]: Copied! <pre>fixed_noise = torch.load(\"fixed_noise_face_gan.t\")\n</pre> fixed_noise = torch.load(\"fixed_noise_face_gan.t\") In\u00a0[8]: Copied! <pre>import pickle as pkl\n\nwith open('save_face.pkl', 'rb') as fp:\n    shapley_modes = pkl.load(fp)\n</pre> import pickle as pkl  with open('save_face.pkl', 'rb') as fp:     shapley_modes = pkl.load(fp) In\u00a0[9]: Copied! <pre>shapley_modes.values.reshape((32, 3, 128, 128, -1))[0].transpose((3, 1, 2, 0)).shape\n</pre> shapley_modes.values.reshape((32, 3, 128, 128, -1))[0].transpose((3, 1, 2, 0)).shape Out[9]: <pre>(1987, 128, 128, 3)</pre> In\u00a0[10]: Copied! <pre>lesion_dict = defaultdict(list)\nfor x, y in shapley_modes.columns:\n    lesion_dict[x].append(y)\n</pre> lesion_dict = defaultdict(list) for x, y in shapley_modes.columns:     lesion_dict[x].append(y) In\u00a0[49]: Copied! <pre>corelation_matrix = pd.DataFrame(shapley_modes[[x for x in shapley_modes.columns if x[0]==layer]]).corr()\n</pre> corelation_matrix = pd.DataFrame(shapley_modes[[x for x in shapley_modes.columns if x[0]==layer]]).corr() In\u00a0[50]: Copied! <pre>nonegative = np.abs(corelation_matrix.values.copy())\n#nonegative = np.maximum(0, corelation_matrix.values.copy())\n\n#ci, Q = bct.community_louvain(nonegative, gamma=1.5)\nci = [bct.community_louvain(nonegative, gamma=1.5, seed=n)[0] for n in range(100)]\n\nconsensus = cluster.find_consensus(np.column_stack(ci), seed=1234)\nnum_ci = len(np.unique(consensus))\ny = np.bincount(consensus)\nii = np.nonzero(y)[0]\nlist(zip(ii,y[ii]))\n</pre> nonegative = np.abs(corelation_matrix.values.copy()) #nonegative = np.maximum(0, corelation_matrix.values.copy())  #ci, Q = bct.community_louvain(nonegative, gamma=1.5) ci = [bct.community_louvain(nonegative, gamma=1.5, seed=n)[0] for n in range(100)]  consensus = cluster.find_consensus(np.column_stack(ci), seed=1234) num_ci = len(np.unique(consensus)) y = np.bincount(consensus) ii = np.nonzero(y)[0] list(zip(ii,y[ii]))  Out[50]: <pre>[(1, 14),\n (2, 7),\n (3, 14),\n (4, 14),\n (5, 12),\n (6, 12),\n (7, 18),\n (8, 6),\n (9, 6),\n (10, 13),\n (11, 5),\n (12, 7)]</pre> In\u00a0[36]: Copied! <pre>out = netG(fixed_noise.to(\"cpu\"), {})\nplt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0)))\n</pre> out = netG(fixed_noise.to(\"cpu\"), {}) plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0))) Out[36]: <pre>&lt;matplotlib.image.AxesImage at 0x7f6748303d60&gt;</pre> In\u00a0[61]: Copied! <pre>out = netG(fixed_noise.to(\"cpu\"), {layer: np.where((consensus==11))})\nplt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0)))\n</pre> out = netG(fixed_noise.to(\"cpu\"), {layer: np.where((consensus==11))}) plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0))) Out[61]: <pre>&lt;matplotlib.image.AxesImage at 0x7f6728242880&gt;</pre> In\u00a0[11]: Copied! <pre>def get_layer_contributions(layer):\n    layer_contrib = []\n    min_val = shapley_modes.values.min()\n\n    for x, y in shapley_modes.columns:\n        if x == layer:\n            layer_contrib.append(torch.Tensor(shapley_modes.get_shapley_mode((x, y))) - min_val)\n    return layer_contrib\n</pre> def get_layer_contributions(layer):     layer_contrib = []     min_val = shapley_modes.values.min()      for x, y in shapley_modes.columns:         if x == layer:             layer_contrib.append(torch.Tensor(shapley_modes.get_shapley_mode((x, y))) - min_val)     return layer_contrib In\u00a0[25]: Copied! <pre>layer = 5\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n\nani.save('animation.gif', writer='imagemagick', fps=1)\n</pre> layer = 5  fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)  HTML(ani.to_jshtml())  ani.save('animation.gif', writer='imagemagick', fps=1) <pre>MovieWriter imagemagick unavailable; using Pillow instead.\n</pre> In\u00a0[26]: Copied! <pre>layer = 4\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n\nani.save('animation.gif', writer='imagemagick', fps=1)\n</pre> layer = 4  fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)  HTML(ani.to_jshtml())  ani.save('animation.gif', writer='imagemagick', fps=1) <pre>Animation size has reached 21075928 bytes, exceeding the limit of 20971520.0. If you're sure you want a larger animation embedded, set the animation.embed_limit rc parameter to a larger value (in MB). This and further frames will be dropped.\nMovieWriter imagemagick unavailable; using Pillow instead.\n</pre> In\u00a0[103]: Copied! <pre>layer = 2\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\nani.save('animation.gif', writer='imagemagick', fps=1)\n</pre> layer = 2  fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)  HTML(ani.to_jshtml()) ani.save('animation.gif', writer='imagemagick', fps=1) <pre>Animation size has reached 20977750 bytes, exceeding the limit of 20971520.0. If you're sure you want a larger animation embedded, set the animation.embed_limit rc parameter to a larger value (in MB). This and further frames will be dropped.\nMovieWriter imagemagick unavailable; using Pillow instead.\n</pre> In\u00a0[27]: Copied! <pre>layer = 0\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\nani.save('animation.gif', writer='imagemagick', fps=3)\n</pre> layer = 0  fig = plt.figure(figsize=(8,8)) plt.axis(\"off\") ims = [[plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(i), padding=2, normalize=True),(1,2,0)), animated=True)] for i in get_layer_contributions(layer)] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)  HTML(ani.to_jshtml()) ani.save('animation.gif', writer='imagemagick', fps=3) <pre>Animation size has reached 21098213 bytes, exceeding the limit of 20971520.0. If you're sure you want a larger animation embedded, set the animation.embed_limit rc parameter to a larger value (in MB). This and further frames will be dropped.\nMovieWriter imagemagick unavailable; using Pillow instead.\n</pre> In\u00a0[27]: Copied! <pre>face_id = 22\n</pre> face_id = 22 In\u00a0[73]: Copied! <pre>from scipy.spatial.distance import pdist, cosine\nfrom image_similarity_measures.quality_metrics import psnr, fsim, uiq\n\nimport dask\nfrom dask.diagnostics import ProgressBar\nimport dask.array as da\nfrom tqdm import tqdm\n</pre> from scipy.spatial.distance import pdist, cosine from image_similarity_measures.quality_metrics import psnr, fsim, uiq  import dask from dask.diagnostics import ProgressBar import dask.array as da from tqdm import tqdm In\u00a0[70]: Copied! <pre>def dist_func(u, v):\n    return uiq(u.reshape((128, 128, 3)), v.reshape((128, 128, 3)))\n</pre> def dist_func(u, v):     return uiq(u.reshape((128, 128, 3)), v.reshape((128, 128, 3))) In\u00a0[74]: Copied! <pre>face_contrib = shapley_modes.values.reshape((32, 3, 128, 128, -1))[face_id].copy()\nfor i in range(face_contrib.shape[-1]):\n    high = face_contrib[..., i].max((1 , 2), keepdims = True)\n    low = face_contrib[..., i].min((1 , 2), keepdims = True)\n    face_contrib[..., i] = (face_contrib[..., i] - low) / (high - low)\nface_contrib = face_contrib.reshape((3*128*128, -1))\n\nmatrix = np.zeros((face_contrib.shape[1], face_contrib.shape[1]))\nfor i in tqdm(range(matrix.shape[0] - 1)):\n    for j in range(i+1, matrix.shape[0]):\n        matrix[i, j] = cosine(face_contrib[:, i], face_contrib[:, j])\n</pre> face_contrib = shapley_modes.values.reshape((32, 3, 128, 128, -1))[face_id].copy() for i in range(face_contrib.shape[-1]):     high = face_contrib[..., i].max((1 , 2), keepdims = True)     low = face_contrib[..., i].min((1 , 2), keepdims = True)     face_contrib[..., i] = (face_contrib[..., i] - low) / (high - low) face_contrib = face_contrib.reshape((3*128*128, -1))  matrix = np.zeros((face_contrib.shape[1], face_contrib.shape[1])) for i in tqdm(range(matrix.shape[0] - 1)):     for j in range(i+1, matrix.shape[0]):         matrix[i, j] = cosine(face_contrib[:, i], face_contrib[:, j]) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1986/1986 [07:13&lt;00:00,  4.58it/s] \n</pre> In\u00a0[14]: Copied! <pre>tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=500)\nz = tsne.fit_transform(face_contrib.T) \n\ndf = pd.DataFrame()\ndf[\"comp-1\"] = z[:,0]\ndf[\"comp-2\"] = z[:,1]\n\nsns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue = [a[0] for a in shapley_modes.columns],\n                palette=sns.color_palette(\"hls\", 6),\n                data=df).set(title=\"Shapley Modes T-SNE Single Face\")\n</pre> tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=500) z = tsne.fit_transform(face_contrib.T)   df = pd.DataFrame() df[\"comp-1\"] = z[:,0] df[\"comp-2\"] = z[:,1]  sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue = [a[0] for a in shapley_modes.columns],                 palette=sns.color_palette(\"hls\", 6),                 data=df).set(title=\"Shapley Modes T-SNE Single Face\")  <pre>[t-SNE] Computing 1501 nearest neighbors...\n[t-SNE] Indexed 1987 samples in 0.226s...\n[t-SNE] Computed neighbors for 1987 samples in 1.542s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 1987\n[t-SNE] Computed conditional probabilities for sample 1987 / 1987\n[t-SNE] Mean sigma: 0.074252\n[t-SNE] KL divergence after 50 iterations with early exaggeration: 40.397686\n[t-SNE] KL divergence after 1000 iterations: 0.578948\n</pre> Out[14]: <pre>[Text(0.5, 1.0, 'Shapley Modes T-SNE Single Face')]</pre> In\u00a0[13]: Copied! <pre>shapley_modes_copy = shapley_modes.values.copy()\n\nfor face_id in range(32):\n    face_contrib = shapley_modes_copy.reshape((32, 3, 128, 128, -1))[face_id]\n    for i in range(face_contrib.shape[-1]):\n        high = face_contrib[..., i].max((1 , 2), keepdims = True)\n        low = face_contrib[..., i].min((1 , 2), keepdims = True)\n        face_contrib[..., i] = (face_contrib[..., i] - low) / (high - low)\n    #face_contrib = face_contrib.reshape((3*128*128, -1))\n    shapley_modes_copy.reshape((32, 3, 128, 128, -1))[face_id] = face_contrib\n\nprint(shapley_modes_copy.max(), shapley_modes_copy.min())\n\ntsne = TSNE(n_components=2, verbose=1, random_state=123,  perplexity=500)\nz = tsne.fit_transform(shapley_modes_copy.T) \n\ndf = pd.DataFrame()\ndf[\"comp-1\"] = z[:,0]\ndf[\"comp-2\"] = z[:,1]\n\nsns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue = [a[0] for a in shapley_modes.columns],\n                palette=sns.color_palette(\"hls\", 6),\n                data=df).set(title=\"Shapley Modes T-SNE\")\n</pre> shapley_modes_copy = shapley_modes.values.copy()  for face_id in range(32):     face_contrib = shapley_modes_copy.reshape((32, 3, 128, 128, -1))[face_id]     for i in range(face_contrib.shape[-1]):         high = face_contrib[..., i].max((1 , 2), keepdims = True)         low = face_contrib[..., i].min((1 , 2), keepdims = True)         face_contrib[..., i] = (face_contrib[..., i] - low) / (high - low)     #face_contrib = face_contrib.reshape((3*128*128, -1))     shapley_modes_copy.reshape((32, 3, 128, 128, -1))[face_id] = face_contrib  print(shapley_modes_copy.max(), shapley_modes_copy.min())  tsne = TSNE(n_components=2, verbose=1, random_state=123,  perplexity=500) z = tsne.fit_transform(shapley_modes_copy.T)   df = pd.DataFrame() df[\"comp-1\"] = z[:,0] df[\"comp-2\"] = z[:,1]  sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue = [a[0] for a in shapley_modes.columns],                 palette=sns.color_palette(\"hls\", 6),                 data=df).set(title=\"Shapley Modes T-SNE\")  <pre>1.0 0.0\n[t-SNE] Computing 1501 nearest neighbors...\n[t-SNE] Indexed 1987 samples in 18.744s...\n[t-SNE] Computed neighbors for 1987 samples in 44.086s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 1987\n[t-SNE] Computed conditional probabilities for sample 1987 / 1987\n[t-SNE] Mean sigma: 96.310672\n[t-SNE] KL divergence after 50 iterations with early exaggeration: 44.036026\n[t-SNE] KL divergence after 650 iterations: 0.429707\n</pre> Out[13]: <pre>[Text(0.5, 1.0, 'Shapley Modes T-SNE')]</pre> In\u00a0[15]: Copied! <pre>corelation_matrix = np.corrcoef(face_contrib.T)\n</pre> corelation_matrix = np.corrcoef(face_contrib.T) In\u00a0[77]: Copied! <pre>nonegative = np.abs(matrix.copy())\n#nonegative = np.maximum(0, corelation_matrix.copy())\n#nonegative = corelation_matrix.copy() + 1\n\n#ci, Q = bct.community_louvain(nonegative, gamma=1.5)\nci = [bct.community_louvain(nonegative, gamma=1, seed=n)[0] for n in range(100)]\n\nconsensus = cluster.find_consensus(np.column_stack(ci), seed=1234)\nnum_ci = len(np.unique(consensus))\ny = np.bincount(consensus)\nii = np.nonzero(y)[0]\nlist(zip(ii,y[ii]))\n</pre> nonegative = np.abs(matrix.copy()) #nonegative = np.maximum(0, corelation_matrix.copy()) #nonegative = corelation_matrix.copy() + 1  #ci, Q = bct.community_louvain(nonegative, gamma=1.5) ci = [bct.community_louvain(nonegative, gamma=1, seed=n)[0] for n in range(100)]  consensus = cluster.find_consensus(np.column_stack(ci), seed=1234) num_ci = len(np.unique(consensus)) y = np.bincount(consensus) ii = np.nonzero(y)[0] list(zip(ii,y[ii]))  Out[77]: <pre>[(1, 1), (2, 1986)]</pre> In\u00a0[32]: Copied! <pre>out_unlesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), {}).detach().squeeze()\nfigure, axis = plt.subplots(1, 3)\nfigure.set_dpi(200)\naxis[0].imshow(np.transpose(out_unlesioned.cpu(),(1,2,0)))\n\nlesion_dict = defaultdict(list)\nfor layer, neuron in shapley_modes.columns[consensus==15]:\n    lesion_dict[layer].append(neuron)\n\nout_lesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), lesion_dict).detach().squeeze()\naxis[1].imshow(np.transpose(out_lesioned.cpu(),(1,2,0)))\n\nout_diff = out_unlesioned - out_lesioned\naxis[2].imshow(np.transpose(out_diff.cpu(),(1,2,0)))\n</pre> out_unlesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), {}).detach().squeeze() figure, axis = plt.subplots(1, 3) figure.set_dpi(200) axis[0].imshow(np.transpose(out_unlesioned.cpu(),(1,2,0)))  lesion_dict = defaultdict(list) for layer, neuron in shapley_modes.columns[consensus==15]:     lesion_dict[layer].append(neuron)  out_lesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), lesion_dict).detach().squeeze() axis[1].imshow(np.transpose(out_lesioned.cpu(),(1,2,0)))  out_diff = out_unlesioned - out_lesioned axis[2].imshow(np.transpose(out_diff.cpu(),(1,2,0))) <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> Out[32]: <pre>&lt;matplotlib.image.AxesImage at 0x7f6748390100&gt;</pre> In\u00a0[28]: Copied! <pre>lesion_dict = defaultdict(list)\nfor layer, neuron in shapley_modes.columns[consensus==11]:\n    lesion_dict[layer].append(neuron)\n\nout = netG(fixed_noise.to(\"cpu\"), lesion_dict)\nplt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0)))\n</pre> lesion_dict = defaultdict(list) for layer, neuron in shapley_modes.columns[consensus==11]:     lesion_dict[layer].append(neuron)  out = netG(fixed_noise.to(\"cpu\"), lesion_dict) plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0))) Out[28]: <pre>&lt;matplotlib.image.AxesImage at 0x7f676c36f100&gt;</pre> In\u00a0[114]: Copied! <pre>out = netG(fixed_noise.to(\"cpu\"), {})\nplt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0)))\n</pre> out = netG(fixed_noise.to(\"cpu\"), {}) plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0))) Out[114]: <pre>&lt;matplotlib.image.AxesImage at 0x7feb3d6439d0&gt;</pre> In\u00a0[110]: Copied! <pre>face_id = 19\nface_contrib = shapley_modes.values.reshape((32, 3, 128, 128, -1))[face_id].reshape(3*128*128, -1)\ncorelation_matrix = np.corrcoef(face_contrib.T)\nnonegative = np.abs(corelation_matrix.copy())\n#nonegative = np.maximum(0, corelation_matrix.copy())\n\n#ci, Q = bct.community_louvain(nonegative, gamma=1.5)\nci = [bct.community_louvain(nonegative, gamma=1.5, seed=n)[0] for n in range(100)]\n\nconsensus = cluster.find_consensus(np.column_stack(ci), seed=1234)\nnum_ci = len(np.unique(consensus))\ny = np.bincount(consensus)\nii = np.nonzero(y)[0]\nlist(zip(ii,y[ii]))\n</pre> face_id = 19 face_contrib = shapley_modes.values.reshape((32, 3, 128, 128, -1))[face_id].reshape(3*128*128, -1) corelation_matrix = np.corrcoef(face_contrib.T) nonegative = np.abs(corelation_matrix.copy()) #nonegative = np.maximum(0, corelation_matrix.copy())  #ci, Q = bct.community_louvain(nonegative, gamma=1.5) ci = [bct.community_louvain(nonegative, gamma=1.5, seed=n)[0] for n in range(100)]  consensus = cluster.find_consensus(np.column_stack(ci), seed=1234) num_ci = len(np.unique(consensus)) y = np.bincount(consensus) ii = np.nonzero(y)[0] list(zip(ii,y[ii]))  Out[110]: <pre>[(1, 119),\n (2, 179),\n (3, 207),\n (4, 338),\n (5, 313),\n (6, 224),\n (7, 136),\n (8, 92),\n (9, 379)]</pre> In\u00a0[133]: Copied! <pre>out_unlesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), {}).detach()[0]\nfigure, axis = plt.subplots(1, 3)\nfigure.set_dpi(200)\naxis[0].imshow(np.transpose(out_unlesioned.cpu(),(1,2,0)))\n\nlesion_dict = defaultdict(list)\nfor layer, neuron in shapley_modes.columns[consensus==9]:\n    lesion_dict[layer].append(neuron)\n\nout_lesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), lesion_dict).detach().squeeze()\naxis[1].imshow(np.transpose(out_lesioned.cpu(),(1,2,0)))\n\nout_diff = out_unlesioned - out_lesioned\naxis[2].imshow(np.transpose(out_diff.cpu(),(1,2,0)))\n</pre> out_unlesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), {}).detach()[0] figure, axis = plt.subplots(1, 3) figure.set_dpi(200) axis[0].imshow(np.transpose(out_unlesioned.cpu(),(1,2,0)))  lesion_dict = defaultdict(list) for layer, neuron in shapley_modes.columns[consensus==9]:     lesion_dict[layer].append(neuron)  out_lesioned = netG(fixed_noise[face_id][None].to(\"cpu\"), lesion_dict).detach().squeeze() axis[1].imshow(np.transpose(out_lesioned.cpu(),(1,2,0)))  out_diff = out_unlesioned - out_lesioned axis[2].imshow(np.transpose(out_diff.cpu(),(1,2,0))) Out[133]: <pre>&lt;matplotlib.image.AxesImage at 0x7fec6986afd0&gt;</pre> In\u00a0[132]: Copied! <pre>out = netG(fixed_noise.to(\"cpu\"), lesion_dict)\nplt.figure(dpi=200)\nplt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0)))\n</pre> out = netG(fixed_noise.to(\"cpu\"), lesion_dict) plt.figure(dpi=200) plt.imshow(np.transpose(vutils.make_grid(out.cpu(), padding=2),(1,2,0))) Out[132]: <pre>&lt;matplotlib.image.AxesImage at 0x7fec69912820&gt;</pre>"},{"location":"examples/shapley%20analysis.html#work-in-progress","title":"Work In Progress\u00b6","text":""},{"location":"examples/shapley%20analysis.html#imports","title":"Imports\u00b6","text":""},{"location":"examples/shapley%20analysis.html#details-in-faces","title":"Details in Faces\u00b6","text":""}]}